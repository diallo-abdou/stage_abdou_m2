---
title: "Internship progress"
author: "Abdourahmane Diallo"
date: '`r Sys.Date()`'
format: 
  revealjs
multiplex: true
#smaller: true
scrollable: true
#theme: sky
editor: visual
number-sections: true
toc: FALSE
#toc-expand: false
#toc_float: 'yes'
code_download: 'yes'
slide-number: true
margin: 0.1
#center: true
code-fold: true
width: 1300
height: 700
toc_depth: 1
execute: 
  cache: true
---

# Setting {.unnumbered}

```{r setup, include=FALSE,fig.align='center',message=FALSE,warning=FALSE,message=FALSE,echo=TRUE}
# rm(list=ls()) # Properly clear workspace
# source("function_abdou.R")
# knitr::opts_chunk$set(echo = TRUE)

```

## Packages {.unnumbered}

```{r packages,echo=TRUE}
  library(tidyverse)
  library(glme)
  library(lsmeans)
  library(agricolae)
  library(RVAideMemoire)
  library(corrplot)
  library(emmeans)
  library(lme4)
  library(multcomp)
  library(MASS)
  library(R2WinBUGS)
  library(arm)
  library(performance)
  library(AER)
  library(AICcmodavg)
  library(MuMIn)
  library(ade4)
  library(Hmisc)
  library(labdsv)
  library(vegan)
  library(cowplot)
  library(ggpubr)
  library(rstatix)
  library(patchwork)
  library(multcompView)
  library(ggsignif)
  library(grid)
  library(FactoMineR)
  library(factoextra)
  library(explore)
  library(ggrepel)
  library(naniar)
  library(outliers)
  library(leaps)
  library(fastDummies)
  library(caret) # pour l'entrainement des models
  library(mgcv)
  library(ggeffects)
  library(gratia)
  library(GGally) # pour ggpair
  library(caTools)
  library(rpart)
  library(rpart.plot)
  library(openxlsx)
  library(readxl)
  library(leaflet) # pour la carto
  library(quarto)
  library(raster)
  library(knitr)
  library(kableExtra)
  library(stringr)
  library(plotly)
  library(PerformanceAnalytics)
  library(usdm)
  library(vcd) # pour la distribution des var reponse
  library(prospectr)# pour split data avec kenSton()
  library(glmnet)
  library(randomForest)
  library(doParallel)
  library(gbm)
  library(kernlab)
  library(e1071)
  library(ggforce)
  library(keras)
  library(tensorflow)
  library(neuralnet)
  library(parallel)
  library(iml) # pour l'interpretabilité des models https://cran.r-project.org/web/packages/iml/vignettes/intro.html
  library(e1071)
  library(stats)
  library(Boruta) # importance des predicteurs
  library(bestNormalize)


```

## Functions {.unnumbered}

```{r fonction, echo=TRUE}

## Identification des NA dans un df -----------------------------------------------
taux_completion<-
  function(df, afficher_zero_percent = FALSE, seuil, trie=FALSE) {
    # Calcule du pourcentage de NA dans le dataframe
    pourcentage_total <-
      round(sum(is.na(df)) / (nrow(df) * ncol(df)) * 100, 1)
    
    # Calcule du pourcentage de NA par colonne
    pourcentage_colonnes <- round(colMeans(is.na(df)) * 100, 1)
    
    # Creation d'un dataframe résultat avec deux colonnes
    result <-
      data.frame(
        Variables = names(df),
        CR = pourcentage_colonnes,
        row.names = NULL
      )
    
    if (afficher_zero_percent) {
      result <- result[result$CR == 0, ]
      result$CR = 100 -result$CR
    } else {
      result <- result[result$CR > 0, ]
      result$CR = 100 -result$CR
      
    }
    
    result <- rbind(result, c("Total", pourcentage_total))
    #result <- rbind(result, c("Total", paste0(pourcentage_total, "")))
    
    result <- result[, c("Variables", "CR")]
    result$CR = as.numeric(result$CR)
    result$CR = round(result$CR,1)
    if (trie){
      result = result %>% arrange(desc(CR))
    }
    result$CR = paste0(result$CR,"%")
    
    return(result)
  }
# Converssion des colonne en num ou factor-----------------------------------------------
conv_col <- function (data, columns_to_convert, to_types) {
  if (to_types == "numeric") {
    # Conversion des colonnes en numeric
    for (col in columns_to_convert) {
      data[, col] <- as.numeric(data[, col])
    }
  } else {
    # Conversion des colonnes en facteurs
    for (col in columns_to_convert) {
      data[, col] <- as.factor(data[, col])
    }
  }
  return(data)
}
#data_converted <- conv_col(data, names(data [, c(1, 3)]), "factor")

# exploration graphiques des variables numeriques -----------------------------------------------
explo_num <- function(nom_col, titre, df = bdd, ligne_col = c(2, 2),mini = min(df[[nom_col]]), maxi=max(df[[nom_col]]) ) {
  par(mfrow = ligne_col)
  
  df[complete.cases(df[[nom_col]]), ]
  df <- df %>%filter(!is.na(df[[nom_col]]))
  df[[nom_col]] = as.numeric(df[[nom_col]])
  # Boxplot
  boxplot(df[[nom_col]], col = 'blue', ylab = titre, ylim = c(mini, maxi))
  # Cleveland plot
  dotchart(df[[nom_col]], pch = 16, col = 'blue', xlab = titre)
  # Histogram
  hist(df[[nom_col]], col = 'blue', xlab = titre, main = "")
  # Quantile-Quantile plot
  qqnorm(df[[nom_col]], pch = 16, col = 'blue', xlab = '')
  qqline(df[[nom_col]], col = 'red') 
}

# Extraction des predictors + moyennes -----------------------------------------------

extraction <- function(nom_col, tif_file_path, df = bdd, conv = 1) {
  #df <- df %>%filter(!is.na(gps_x) & !is.na(gps_y))
  raster_data <- raster(tif_file_path)
  
  # Création d'un dataframe pour stocker les valeurs extraites
  df_interne <- data.frame(gps_x = df$gps_x, gps_y = df$gps_y)
  proj4Str <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"
  # Transformer les coordonnées GPS en système de coordonnées du raster
  gps_coords_sp <- SpatialPoints(df_interne, proj4string = CRS(proj4Str))
  gps_coords_proj <- spTransform(gps_coords_sp, crs(raster_data))
  
  # Extraction des valeurs du raster 
  values <- raster::extract(raster_data, gps_coords_proj)
  
  # Ajout des valeurs extraites comme nouvelles colonnes a df
  #df_save = data.frame()
  #df_save[[nom_col]] <- values / conv
  
  df[[nom_col]] <- values / conv
  
  return(df)
}

# la moyenne des predictores -----------------------------------------------
moyenne_val_extrct <- function(nom_col, vec_col, df=bdd) {
  df[[nom_col]] <- rowMeans(as.matrix(df[, vec_col, drop = FALSE]), na.rm = TRUE)
  df[[nom_col]] = round(df[[nom_col]],1)
  return(as.data.frame(df))
}


# tests de corrélation avec un seuil -----------------------------------------------
cor_function_seuil <- function(data, seuil,affiche=FALSE) {
  # Création d'un vecteur pour stocker les paires de variables corrélées
  variables_corr <- c()
  
  # Boucle pour tester la corrélation entre chaque paire de variables
  for (i in 1:(ncol(data) - 1)) {
    for (j in (i + 1):ncol(data)) {
      # Calcul de la corrélation entre les variables i et j
      cor_value <- stats::cor(data[, i], data[, j], use = "na.or.complete")
      
      # Stockage du résultat dans le vecteur si supérieur au seuil
      if (cor_value >= seuil | cor_value <= -seuil) {
        if(affiche){
        cat(
          "***",
          colnames(data)[i],
          "  __est correlee a__  ",
          colnames(data)[j],
          "avec un R =",
          cor_value,
          "\n \n \n"
        )
      }
        
        variables_corr <-
          c(variables_corr, colnames(data)[i], colnames(data)[j])
      }
    }
  }
  
  return(variables_corr)
}


# tests de valeurs aberant -----------------------------------------------
test_grub <- function(data, variable, direction = "maxi") {
  
  if (direction == "maxi") { 
    repeat {
      # Effectuer le test de Grubbs
      test_aberrant <- grubbs.test(data[[variable]], opposite = FALSE)
      
      # Obtenir la p-valeur du test
      p.value <- test_aberrant$p.value
      # Si la p-valeur est inférieure au seuil de 0.05, on supprime la valeur aberrante
      if (p.value < 0.05) {
        max_value <- max(data[[variable]],na.rm=TRUE)
        data <- subset(data, data[[variable]] != max_value | is.na(data[[variable]]))
      } else {
        # S'il n'y a plus de valeurs aberrantes, sortir de la boucle
        break
      }
    }
  }
  
  
  if (direction == "mini") { 
    repeat {
      test_aberrant <- grubbs.test(data[[variable]], opposite = TRUE)
      # Obtenir la p-valeur du test
      p.value <- test_aberrant$p.value
      # Si la p-valeur est inférieure au seuil de 0.05, on supprime la valeur aberrante
      if (p.value < 0.05) {
        min_value <- min(data[[variable]],na.rm=TRUE)
        data <- subset(data, data[[variable]] != min_value | is.na(data[[variable]]))
      } else {
        # S'il n'y a plus de valeurs aberrantes, sortir de la boucle
        break
      }
    }
  }
  
  
  return(data)
}




# boxplote -----------------------------------------------
plot_boxplot <-function(donnee,
           x_col,y_col,x_label,y_label,title,legend_title,
           couleurs,
           affiche_point = TRUE,
           ymin = min(donnee[[y_col]]),
           ymax = 1.2 * max(donnee[[y_col]])) {
    
  graphe <-ggplot(donnee,
             aes_string(
               x = x_col,
               y = y_col,
               colour = x_col
             )) +
  geom_boxplot(
        outlier.shape = NA,
        outlier.colour = "black",
        alpha = 0.20,
        size = 1.5 
      ) +
  labs(title = title,x = x_label,y = y_label) +
  scale_color_manual(values = couleurs, name = legend_title) +
  theme_classic(base_size = 12, base_family = "Arial") +
  theme(axis.text = element_text(size = 10),
        axis.title.y = element_text(
          vjust = 5, size = 12, face = "bold"),
        axis.title.x = element_text(face = "bold"),
        axis.ticks.length = unit(0.2, "cm"),
        legend.position = "none",  # Cette ligne supprime la lÃ©gende
        #legend.position = "right",
        legend.text = element_text(size = 10),
        legend.title = element_text(size = 12, face = "bold"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        plot.title = element_text(size = 14, face = "bold"),
        plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), "cm")
      )
    if (affiche_point) {
      graphe <-
        graphe + geom_jitter(position = position_jitter(seed = 0.5), size = 0.8)
    }
    
    if (y_col %in% names(donnee)) {
      graphe <- graphe +
        coord_cartesian(ylim = c(ymin, ymax))
    }
  
    graphe = graphe + stat_summary(
      fun.y = mean,
      geom = "point",
      shape = 15,
      size = 1.5,
      col = "black",
      fill = "black"
    )
    
    return(graphe)
}



#pour le  pairwise.t.test() -----------------------------------------------------
tri.to.squ <- function(x) {
  rn <- row.names(x)
  cn <- colnames(x)
  an <- unique(c(cn, rn))
  myval <- x[!is.na(x)]
  mymat <-
    matrix(
      1,
      nrow = length(an),
      ncol = length(an),
      dimnames = list(an, an)
    )
  for (ext in 1:length(cn))
  {
    for (int in 1:length(rn))
    {
      if (is.na(x[row.names(x) == rn[int], colnames(x) == cn[ext]]))
        next
      mymat[row.names(mymat) == rn[int], colnames(mymat) == cn[ext]] <-
        x[row.names(x) == rn[int], colnames(x) == cn[ext]]
      mymat[row.names(mymat) == cn[ext], colnames(mymat) == rn[int]] <-
        x[row.names(x) == rn[int], colnames(x) == cn[ext]]
    }
  }
  return(mymat)
}



# Selection interaction -------------------------------
select_inter <- function(response_var, df, explanatory_vars) {
  results <- data.frame()
  combinations <- combn(explanatory_vars, 2, simplify = FALSE)

  for(i in seq_along(combinations)) {

    formula <- as.formula(paste(response_var, "~", paste(combinations[[i]], collapse = "*")))
    model <- gam(formula, data = df)
    r_squared <- summary(model)$r.sq
    aic <- AIC(model)
    results <- rbind(results, data.frame("variables" = paste0(combinations[[i]], collapse = ".inter."), 
                                         "r_squared" = r_squared, 
                                 "aic" = aic))
  }
  return(results)
}

# Comparaion betwen predtited and observed -----------------------------------
plot_comp = function (df,ylabel, title_class, legende = TRUE,plotly = FALSE,xlabel = "observations",title=""){ 

  
  p = ggplot(df, aes(x = observation)) + 
  #graph representant observed
  geom_point(aes(y = Observed, color = "Observed valuess")) +
  geom_line(aes(y = Observed, color = "Observed valuess")) + 
  
  #graph representant  preticted
  geom_point(aes(y = Predicted, color="Predicted values")) +
  geom_line(aes(y = Predicted, color="Predicted values")) + 
  # ggtitle(title)
  theme(plot.title = element_text(hjust = 0.5)) + 
  labs(title = title,x=xlabel, y=ylabel, color = "Legend :") + 
  ylim(min(c(min(df$Predicted), min(df$Observed))),
            max(c(max(df$Predicted), max(df$Observed)))+1  ) +
    
  scale_color_manual(values = c("Observed valuess"='red', "Predicted values"='green')) +
  annotate("text", x = 8, y =  max(c(max(df$Predicted), max(df$Observed)))+1, 
           label = title_class, col = "black", size = 3)

  
  if (!legende) {
    p <- p + theme(legend.position = "none")
  }
  
  if(plotly){
    p = ggplotly(p)
  }

return (p)

}


# Calcul R²
calcule_R2 = function(x, y) {cor(x, y)^2}

```

## Plan {.unnumbered}

<!-- -   Database exploration -->

<!-- -   Earthworms data -->

<!-- -   Soil data extraction -->

<!-- -   Climate data extraction -->

<!-- -   Exploratory analysis -->

<!-- -   Importance of variables -->

<!-- -   Predictive modeling -->

<!-- -   Results  -->

<!-- -   Conclusion -->



- Explorations de l'occurence des espèces (  [species.html](species.html){target="_blank"} )

<!-- Exploration des relations entre les espèces et les predicteurs -->

- ACP sur les predicteurs de l'abondance, la biomasse et la richesse ( [ACP])


- Resultats ( [Réduction et effets des variables] )


-  Plan du rapport (  [Stage_abdou.docx](C:/Users/diall/OneDrive/Bureau/M2_MODE/stage_abdou_m2/Redaction/Stage_abdou.docx){target="_blank"} )



- To do next ( [To do next] )




# Database import

-   Import of database **LandWorm_dataset_site_V1.9.xlsx** (february 22, 2024)

```{r import,echo=FALSE}
chemin_fichier_excel = "C:/Users/diall/Downloads/datas/LandWorm_dataset_site_V1.9.xlsx"
bdd <- read.xlsx(chemin_fichier_excel, sheet = "Sheet1")
```

-   The database contains **`r nrow(bdd)`** rows and **`r ncol(bdd)`** columns

```{r conversion,echo=FALSE}
col_en_factor = c("Programme","Annee","ID_Site","Code_Parcelle","postal_code","clcm_lvl1",
                  "clcm_lvl2","clcm_lvl3","Modalite","Bloc","Protocole","land_cover_detail","type_tillage","fertilisation","ferti_min_product","ferti_orga_product")
bdd = conv_col(bdd, col_en_factor, "factor")
```

## Data selection: EcoBioSoil

```{r selection dc1,echo=FALSE}
n_line=nrow(bdd)
bdd$owner=as.factor(bdd$owner)
summary_df <- as.data.frame(summary(bdd$owner))
colnames(summary_df) <- c("Numbers")
kable(summary_df)
```

```{r selection dc2,echo=FALSE}
bdd <- subset(bdd, owner == "dc")
bdd$owner=droplevels(bdd$owner)

```

-   The database therefore changes from **`r n_line`** to **`r nrow(bdd)`** observations.

# Database exploration

-   CR = Completion rate

## Complete columns

```{r Complete columns, echo=TRUE}
df_col=taux_completion(bdd,TRUE,trie=FALSE)
df_col = df_col[df_col$Variables != "Total",]
#print("table")
kable(df_col, caption = "", col.width = c("75%", "25%"))
# cat(                                                    )
# head(bdd[, "ID"])
```

## Non-complete columns

```{r Non-complete columns, scrollable = TRUE}
df_col= taux_completion(bdd,FALSE,trie = TRUE)
df_col = df_col[df_col$Variables != "Total",]
kable(df_col, caption = " ", col.width = c("75%", "25%"))
```

## Focus on GPS coordinates

-   There is **`r sum(is.na(bdd$gps_x))`** NA (CR = `r df_col[df_col$Variable=="gps_x", "CR"]`) in **GPS_X**
-   There is **`r sum(is.na(bdd$gps_y))`** NA (CR = `r df_col[df_col$Variable=="gps_y", "CR"]`) in **GPS_Y**

```{r GPS,echo=TRUE}
n_line= nrow(bdd)
bdd$gps_x <- as.numeric(gsub("[^0-9.-]", "", bdd$gps_x))
bdd$gps_y <- as.numeric(gsub("[^0-9.-]", "", bdd$gps_y))
bdd <- bdd[complete.cases(bdd$gps_x, bdd$gps_y), ]
bdd <- bdd %>%filter(!is.na(gps_x) & !is.na(gps_y))
#sum(is.na(bdd$gps_x))
#sum(is.na(bdd$gps_y))
```

-   We delete the *NA* lines in the GPS coordinates
-   The database therefore changes from **`r n_line`** to **`r nrow(bdd)`** observations.
-   Merging database and climat database

```{r mergins & climat, echo=TRUE}
# Ajout variables climatiques (voir chunk extraction données climatiques)
chemin_fichier <- "C:/Users/diall/OneDrive/Bureau/M2_MODE/stage_abdou_m2/datas/bdd_climat_ok.rds"
# saveRDS(bdd_climat_ok, chemin_fichier)
bdd_climat_ok <- readRDS(chemin_fichier)
df_fusion <- subset(bdd_climat_ok, select = -c(gps_x, gps_y))

rows_not_in_df_fusion <- anti_join(bdd, df_fusion, by = "ID")
merged_df <- merge(bdd, df_fusion, by = "ID")

ids_not_matching <- anti_join( merged_df,bdd, by = "ID")

bdd = merged_df

#bdd <- cbind(bdd, df_fusion) # all = TRUE pour garder toutes les lignes
```

## Cartography

```{r Cartography,echo=TRUE}
n_ligne= nrow(bdd)
df_coord <- bdd[, c("gps_x", "gps_y")] %>% mutate(gps_x = as.numeric(gps_x),gps_y = as.numeric(gps_y))

df_coord$num_ligne <- seq(nrow(df_coord))
carte <- leaflet(df_coord) %>%
  addTiles() %>%
  addCircleMarkers(lng = ~gps_x, lat = ~gps_y, radius = 0.8, fillOpacity = 0.8, fillColor = "blue")
carte
```

```{r outside France}
hors_france= read.csv(file = "C:/Users/diall/Downloads/datas/hors_france.csv", header = TRUE)

bdd <- bdd[!(bdd$gps_x %in% hors_france$gps_x & bdd$gps_y %in% hors_france$gps_y), ]
bdd <- droplevels(bdd)
```

-   We delete points outside France (**`r nrow(hors_france)`**)
-   The database therefore changes from **`r n_ligne`** to **`r nrow(bdd)`** observations.

## Focus on years

-   Cleaning the Annee column 
<br/> 
<!--
```{r years1, echo=TRUE}
# levels(bdd$Annee) # parfois années et jours et ou mois
# bdd$Annee= as.factor(bdd$Annee)
# bdd$Annee <- gsub("^(\\d{4}).*$", "\\1", bdd$Annee) # on prend uniquement les 04 premier chiffre
# bdd$Annee= as.factor(bdd$Annee)

```
-->

-   CR of Annee = **`r df_col[df_col$Variable=="Annee", "CR"]`** (`r length(levels(bdd$Annee))` levels)

```{r years2, echo=TRUE, scrollable = TRUE}
bdd$Annee= as.factor(bdd$Annee)
summary_df <- as.data.frame(summary(bdd$Annee))
colnames(summary_df) <- c("Numbers")
kable(summary_df)
```


<!-- 
-   We remove all the years before **1990** and the NA 

```{r years3, echo=TRUE, scrollable = TRUE}
n_ligne =nrow(bdd)
bdd <- bdd %>%filter(!is.na(Annee))# on enleve les NA
annes_omit= c("1821", "1960", "1978", "1982", "1983", "1984", "1986", "1988", "1989") # annee sup
bdd <- bdd[!bdd$Annee %in% annes_omit, ]
bdd=droplevels(bdd)
#levels (bdd$Annee)
#summary (bdd$Annee)
summary_df <- as.data.frame(summary(bdd$Annee))
colnames(summary_df) <- c("Numbers")
kable(summary_df,padding = 5)
```

-   The database therefore changes from **`r n_ligne`** to **`r nrow(bdd)`** observations. 
-->



<!--
## Table land use & protocol ( start )

-   clcm_lvl1 & protocol

```{r clcm_lvl1 & protocol, echo=TRUE}
kable(table(bdd$clcm_lvl1, bdd$Protocole,exclude = NULL),padding = 10,align = "c")

```

\n\n\n

-   clcm_lvl2 & protocol

```{r clcm_lvl2 & protocol, echo=TRUE}
kable(table(bdd$clcm_lvl2, bdd$Protocole,exclude = NULL),padding = 10,align = "c")
```

\n\n\n

-   clcm_lvl3 & protocol

```{r clcm_lvl3 & protocol, echo=TRUE}
kable(table(bdd$clcm_lvl3, bdd$Protocole,exclude = NULL),padding = 0,align = "c")
```
-->


## Focus on protocols

-   List of protocols available on the database ( `r length(levels(bdd$Protocole))` levels)

```{r protocols,echo=TRUE}
bdd$Protocole = as.factor(bdd$Protocole)
summary_df <- as.data.frame(summary(bdd$Protocole))
colnames(summary_df) <- c("Numbers")
kable(summary_df,padding = 5)
```


<!--
-   Selection of protocols: **F_HS, FHS, hand sorting, HS**
-->

-   Selection of protocols: **F_HS, HS**

```{r select protocols,echo=TRUE}
n_ligne = nrow(bdd)
#select_protocole =c("F_HS", "FHS", "hand sorting" ,"HS")
select_protocole =c("F_HS", "HS")
bdd <- bdd[bdd$Protocole %in% select_protocole, ]
bdd=droplevels(bdd)
bdd$Protocole = as.factor(bdd$Protocole)
summary_df <- as.data.frame(summary(bdd$Protocole))
colnames(summary_df) <- c("Numbers")
kable(summary_df,padding = 5)
```

-   The database therefore changes from **`r n_ligne`** to **`r nrow(bdd)`** observations.


<!--
-   Merging levels :

    -   F_HS $=$ F_HS $+$ FHS
    -   HS $=$ HS $+$ hand sorting

    ```{r merging protocols,echo=TRUE}
    levels(bdd$Protocole)[levels(bdd$Protocole) == "FHS"] <- "F_HS"
    levels(bdd$Protocole)[levels(bdd$Protocole) == "hand sorting"] <- "HS"
    bdd$Protocole = as.factor(bdd$Protocole)
    summary_df <- as.data.frame(summary(bdd$Protocole))
    colnames(summary_df) <- c("Numbers")
    kable(summary_df,padding = 5)
    ```
-->

## Focus on clcm_lvl1

-   CR of clcm_lvl1 = **`r df_col[df_col$Variable=="clcm_lvl1","CR"]`** (`r length(levels(bdd$clcm_lvl1))` levels)

```{r clcm_lvl1, echo=TRUE}
bdd$clcm_lvl1= as.factor(bdd$clcm_lvl1)
summary_df <- as.data.frame(summary(bdd$clcm_lvl1))
colnames(summary_df) <- c("Numbers")
# kable(summary_df,padding = 5)
```

-   Merging levels

```{r merging clcm_lvl1, echo=TRUE}
levels(bdd$clcm_lvl1)[levels(bdd$clcm_lvl1) == "1_Naturel"] <- "Forest and semi natural areas"
levels(bdd$clcm_lvl1)[levels(bdd$clcm_lvl1) == "2_Agricole"] <- "Agricultural areas"

bdd$clcm_lvl1= as.factor(bdd$clcm_lvl1)
summary_df <- as.data.frame(summary(bdd$clcm_lvl1))
colnames(summary_df) <- c("Numbers")
kable(summary_df,padding = 5)
```

-   Update **code_clcm_lvl1**

```{r code_clcm_lvl1, echo=TRUE}
#bdd$code_clcm_lvl1 = as.factor(bdd$code_clcm_lvl1)

bdd$code_clcm_lvl1 <- ifelse(bdd$clcm_lvl1 == "Forest and semi natural areas", 3, bdd$code_clcm_lvl1)

bdd$code_clcm_lvl1 <- ifelse(bdd$clcm_lvl1 == "Agricultural areas", 2, bdd$code_clcm_lvl1)
```

-   For the moment, we will keep the NA of **clcm_lvl1**

## Focus on clcm_lvl2

-   CR of clcm_lvl2 = **`r df_col[df_col$Variable=="clcm_lvl2","CR"]`** (`r length(levels(bdd$clcm_lvl2))` levels)

```{r clcm_lvl2 , echo=TRUE}
bdd$clcm_lvl2= as.factor(bdd$clcm_lvl2)
summary_df <- as.data.frame(summary(bdd$clcm_lvl2))
colnames(summary_df) <- c("Numbers")
# kable(summary_df,padding = 8)
```

-   Merging levels

```{r merging clcm_lvl2, echo=TRUE}
levels(bdd$clcm_lvl2)[levels(bdd$clcm_lvl2) == "21_Agricole ouvert"] <- "Arable land"

bdd$clcm_lvl2= as.factor(bdd$clcm_lvl2)
summary_df <- as.data.frame(summary(bdd$clcm_lvl2))
colnames(summary_df) <- c("Numbers")
kable(summary_df,padding = 5)
```

-   Update **code_clcm_lvl2**

```{r code_clcm_lvl2, echo=TRUE}

bdd$code_clcm_lvl2 <- ifelse(bdd$clcm_lvl2 == "Arable land", 21, bdd$code_clcm_lvl2)

```

## Focus on clcm_lvl3

-   CR of clcm_lvl3 = **`r df_col[df_col$Variable=="clcm_lvl3","CR"]`** (`r length(levels(bdd$clcm_lvl3))` levels)

```{r clcm_lvl3, echo=TRUE, scrollable = TRUE}
bdd$clcm_lvl3= as.factor(bdd$clcm_lvl3)
summary_df <- as.data.frame(summary(bdd$clcm_lvl3))
colnames(summary_df) <- c("Numbers")
kable(summary_df,padding = 5)

```

## Land use selection (clcm_lvl3)


<!--
-   **Broad-leaved forest** 
-   **Coniferous forest** 
-   **Mixed forest** 

-   **Pastures, meadows and other permanent grasslands under agricultural use** 

-   **Non-irrigated arable land** 

-   **Vineyards**

-   **Green urban areas**

-   **Natural grasslands**
-->

```{r select clcm_lvl3, echo=TRUE}
select_os= c("Broad-leaved forest", "Coniferous forest", "Mixed forest", 
"Pastures, meadows and other permanent grasslands under agricultural use", "Non-irrigated arable land", 
"Vineyards","Green urban areas","Natural grasslands")

bdd <- bdd[bdd$clcm_lvl3 %in% select_os, ]
bdd=droplevels(bdd)
bdd$clcm_lvl3 = as.factor(bdd$clcm_lvl3)
summary_df <- as.data.frame(summary(bdd$clcm_lvl3))
colnames(summary_df) <- c("Numbers")
kable(summary_df)

```

-   **Maybe, we can merge the three types of forest ?**

## Land use & protocol overview

```{r LU & protocol overview, echo=TRUE}
# kable (table(bdd$clcm_lvl1, bdd$Protocole,exclude = NULL), align = "c", format = "pipe", padding = 10)
# kable (table(bdd$clcm_lvl2, bdd$Protocole,exclude = NULL), align = "c", format = "pipe", padding = 10)
kable (table(bdd$clcm_lvl3, bdd$Protocole,exclude = NULL), align = "c", format = "pipe", padding = 10)
```



# Earthworms data

## Total richness calculation method

-   Removal of columns with only NA (**`r length(colnames(bdd)[colSums(is.na(bdd)) == nrow(bdd)])`**) and/or only 0
-   Identify columns beginning with **AB\_**
-   Deletion of **AB\_** columns that are not species
-   Calculate richness by assigning **1** to each column if the value is different from 0 and NA
-   Total richness = **1** if the plot has a value in AB and/or BM


```{r calcul richness, echo=FALSE}
# on supprime tout les colonnes ayant que des NA
colonnes_na <- colnames(bdd)[colSums(is.na(bdd)) == nrow(bdd)]
# summary(bdd[, colonnes_na])
bdd <- bdd[, !colnames(bdd) %in% colonnes_na]



# On supprimme toutes les colonnes ayant que des NA et des 0
colonnes_numeriques <- sapply(bdd, is.numeric)
somme_colonnes_numeriques <- colSums(bdd[, colonnes_numeriques],na.rm=TRUE)
colonnes_zeros <- names(somme_colonnes_numeriques[somme_colonnes_numeriques == 0])
#summary(bdd[, colonnes_zeros])
bdd <- bdd[, !colnames(bdd) %in% colonnes_zeros]



# On récupère toutes les colonnes qui commencent par **AB_**
colonnes_AB <- grep("^AB_", names(bdd), value = TRUE)



# On supprimme les colonnes AB_ qui ne sont pas des espèces dans le calcule
ab_supprimee =  c("AB_AD","AB_JV","AB_SA","AB_STAD_X","AB_indéterminable","AB_Indéterminable","AB_indéterminable_endogeic","AB_tot","AB_Indéterminable_epigeic","AB_indéterminable_endogeic","AB_Ep.X","AB_vide", "AB_Ep.X1","AB_Ep.X2","AB_A.X","AB_Adult","AB_cocon","AB_indéterminé","AB_Juvenile","AB_Sub.adult","AB_Indéterminé","AB_Lumbricidae")
colonnes_AB <- colonnes_AB[!colonnes_AB %in% ab_supprimee]



# On calcule la richesse en attribiant 1 à chaque colonne si la valeur est différent de 0 et de NA
bdd$Richesse_tot <- 0
bdd$Richesse_tot <- rowSums(!is.na(bdd[colonnes_AB]) & bdd[colonnes_AB] != 0)
#sum (is.na(bdd$Richesse_tot) )
#summary(bdd$Richesse_tot)



# Check des lignes ayant des 0 richesse et X AB ou BM : 0 lignes
# vdt_a_checker = bdd[bdd$Richesse_tot == 0 & (bdd$Total_AB !=0 | bdd$BM_to !=0), c("ID_Site","AB_tot","BM_tot","Richesse_tot")]
# vdt_a_checker = subset(vdt_a_checker, Richesse_tot==0)
# View(vdt_a_checker)
# vdt_a_checker$Richesse_tot <- 1
# Mettre à jour les ligne correspondant dans la bdd 
# bdd[rownames(bdd) %in% rownames(vdt_a_checker), "Richesse_tot"] <- 1



# Check si y a des ligne ayant que des NA dans AB, BM et Richesse : nop
resultat <- subset(bdd, is.na(AB_tot) & is.na(BM_tot) & is.na(Richesse_tot))
# View(resultat[, c("AB_tot","BM_tot", "Richesse_tot")])



# Check si y a des ligne ayant que des zéros ou des NA dans AB, BM et Richesse_tot: 66 ligne
vdt <- c("AB_tot", "BM_tot", "Richesse_tot")
lignes_zero <- which(rowSums(bdd[vdt] != 0, na.rm = TRUE) == 0)
# View(bdd[lignes_zero,c("ID_Site","AB_tot", "BM_tot", "Richesse_tot")])



# Check des lignes ayant de BM mais pas de AB
bm_sans_ab <- subset(bdd, AB_tot == 0 & BM_tot != 0)
# bm_sans_ab[, c("ID","ID_Site", "Programme", "Protocole", "AB_tot", "BM_tot")]

ab_sans_bm <- subset(bdd, BM_tot == 0 & AB_tot != 0) # 1 parcelles
# ab_sans_bm[, c("ID","ID_Site", "Programme", "Protocole", "AB_tot", "BM_tot")]


# Check des doublons

#duplicated_rows <- subset(bdd, duplicated(bdd[, c("ID", "AB_tot", "BM_tot")]) | #duplicated(bdd[, c("ID", "AB_tot", "BM_tot")], fromLast = TRUE))

```



## Total abundance (CR = 100 % )

```{r fig AB_tot,fig.align='center',fig.height=10}
AB_tot_aberant = bdd[,c("ID","Programme", "Annee", "ID_Site","clcm_lvl1","clcm_lvl2","clcm_lvl3","Protocole","AB_tot", "Richesse_tot")]
# summary(bdd$AB_tot) 
df_cleaned = bdd

df_cleaned$AB_tot = as.numeric(df_cleaned$AB_tot)
explo_num(nom_col = 'AB_tot', titre = 'AB_tot (before cleaning)', df = df_cleaned)
df_cleaned <- test_grub(df_cleaned, 'AB_tot', direction = 'maxi')
df_cleaned <- test_grub(df_cleaned, 'AB_tot', direction = 'mini')
explo_num(nom_col = 'AB_tot', titre = 'AB_tot (after cleaning)', df = df_cleaned)
# summary(df_cleaned$AB_tot) 
bdd = df_cleaned

```


## Total biomass (CR = `r df_col[df_col$Variable=="BM_tot","CR"]`)

```{r fig BM_tot,fig.align='center',fig.height=10}
# summary(bdd$BM_tot) 
df_cleaned = bdd

df_cleaned$BM_tot = as.numeric(df_cleaned$BM_tot)
explo_num(nom_col = 'BM_tot', titre = 'BM_tot (before cleaning)', df = df_cleaned)
df_cleaned <- test_grub(df_cleaned, 'BM_tot', direction = 'maxi')
df_cleaned <- test_grub(df_cleaned, 'BM_tot', direction = 'mini')
explo_num(nom_col = 'BM_tot', titre = 'BM_tot (after cleaning)', df = df_cleaned)
# summary(df_cleaned$BM_tot) 
bdd = df_cleaned
```




## Total taxonomic richness (CR = 100 % )

```{r fig richness, fig.align='center',fig.height=10}
df_cleaned = bdd

df_cleaned$Richesse_tot = as.numeric(df_cleaned$Richesse_tot)
explo_num(nom_col = 'Richesse_tot', titre = 'Richesse_tot (before cleaning)', df = df_cleaned)
df_cleaned <- test_grub(df_cleaned, 'Richesse_tot', direction = 'maxi')
df_cleaned <- test_grub(df_cleaned, 'Richesse_tot', direction = 'mini')
explo_num(nom_col = 'Richesse_tot', titre = 'Richesse_tot (after cleaning)', df = df_cleaned)
# summary(df_cleaned$Richesse_tot) 
bdd = df_cleaned

```


## Graphe valeurs aberant AB_tot

```{r,fig.align='center', echo=TRUE}
# summary(AB_tot_aberant)
AB_tot_aberant_2 = AB_tot_aberant[AB_tot_aberant$AB_tot > max(bdd$AB_tot),]
AB_tot_aberant_2$clcm_lvl1 =as.factor(AB_tot_aberant_2$clcm_lvl1)
AB_tot_aberant_2$clcm_lvl2 =as.factor(AB_tot_aberant_2$clcm_lvl2)
AB_tot_aberant_2$clcm_lvl3 =as.factor(AB_tot_aberant_2$clcm_lvl3)
AB_tot_aberant_2 = droplevels(AB_tot_aberant_2)
kable(unique(AB_tot_aberant_2[,c("Programme","Annee","clcm_lvl3")]))

df = AB_tot_aberant_2
df$observation = 1:nrow(df)
df$Richesse_tot_10 = df$Richesse_tot*100
g_AB_tot_aberant = ggplot(df, aes(x = observation)) + 
  geom_point(aes(y = AB_tot, color = "Abundance")) +
  geom_line(aes(y = AB_tot, color = "Abundance")) + 
  geom_point(aes(y = Richesse_tot_10, color="Richness*100")) +
  geom_line(aes(y = Richesse_tot_10, color="Richness*100")) + 
  # ggtitle(title)
  theme(plot.title = element_text(hjust = 0.5)) + 
  labs(title = "  ",x="Observation", y="Values", color = "Legend:") +
  scale_color_manual(values = c("Abundance"='red', "Richness*100"='green'))
ggsave("g_AB_tot_aberant.png", plot = g_AB_tot_aberant, dpi = 300)
g_AB_tot_aberant = ggplotly(g_AB_tot_aberant)
g_AB_tot_aberant 
```

<!-- <p align="center"> -->
<!--   <img src="g_AB_tot_aberant.png"> -->
<!-- </p> -->


<!--
# Synthèse du taux de remplissage

## Complete columns

```{r synt CR, echo=TRUE}
df_col=taux_completion(bdd,TRUE,trie=FALSE)
df_col = df_col[df_col$Variables != "Total",]
kable(df_col, caption = " ", col.width = c("75%", "25%"))
```

## Non-complete columns

```{r, scrollable = TRUE}
df_col= taux_completion(bdd,FALSE,trie = TRUE)
df_col = df_col[df_col$Variables != "Total",]
kable(df_col, caption = " ", col.width = c("75%", "25%"))
```

-->




# Climate data extraction
## The source database ([CHELSA V2](https://chelsa-climate.org/bioclim/){target="_blank"})

```{r Climate df extraction,echo=TRUE}

# Lire le fichier Excel
chemin_fichier_excel <- "C:/Users/diall/Downloads/datas/ODMAP.xlsx"
climat <- read.xlsx(chemin_fichier_excel, sheet = "climat")

# Fusions des cellules des colonnes avec des éléments dupliqués
for (col in names(climat)) {
  climat[[col]] <- ifelse(duplicated(climat[[col]]), "", climat[[col]])
}

# Affichage du tableau avec kableExtra et centrage du contenu des cellules
kableExtra::kable(climat) %>%
  kableExtra::kable_styling() %>%
  kableExtra::column_spec(1:ncol(climat)) 

```

## Extraction method

-   Link recovery ( see file [link .tif](https://1drv.ms/t/s!Avfm81EzNGBHjIZWw8YePljXaGSpCQ?e=qIPeWR){target="_blank"} )

-   Extracting variable names

-   Uses of the **extraction()** function

-   Convert columns to correct format and unit

-   Adding variables to the LANDWORM database

```{r Extraction method,echo=TRUE}
liens_tif = read.table(file = "C:/Users/diall/Downloads/datas/envidatS3paths.txt")
liens_tif$shortname <- str_extract(liens_tif$V1, "(?<=CHELSA_).*?(?=_1981)")
liens_tif[liens_tif$shortname=="rsds","shortname"]=c("rsds_max","rsds_mean","rsds_min","rsds_range")

#all(is.na(bdd$gps_x))
#all(is.na(bdd$gps_y))

bdd_climat= bdd[, c("ID","gps_x","gps_y")]

temp_1=Sys.time()
#for( i in 1:nrow(liens_tif)){
  #nom=liens_tif[i,c("shortname")]
  #df_ext <- extraction(nom_col = nom,df = bdd_climat,conv = 1, 
                  #tif_file_path = liens_tif[i,c("V1")] ) 
  #bdd_climat[[nom]] <- df_ext [,nom]
  #rm("df_ext","nom")
  #cat("Extraction: ",i,"/",nrow(liens_tif), "\n")
#}
temp_2=Sys.time()
duree= difftime(temp_2,temp_1)

chemin_fichier <- "C:/Users/diall/OneDrive/Bureau/M2_MODE/stage_abdou_m2/datas/bdd_climat.rds"
# saveRDS(bdd_climat, chemin_fichier)
#bdd_climat <- readRDS(chemin_fichier)

# debut cnversion ------------------------------------------------------------
conv_df_climat= data.frame(shortname =liens_tif$shortname )

# unit = 1
conv_df_climat$unit = rep(1)
# unit = 100
unit_100=c("bio4")
conv_df_climat$unit <- ifelse(conv_df_climat$shortname %in% unit_100, 100, 1)


# scale = 0.1
conv_df_climat$scale = rep(0.1)
# scale = 1
scale_1=c("fcf","fgd","gddlgd0","gddlgd5","gddlgd10","gdgfgd0","gdgfgd5","gdgfgd10","gsl","kg0","kg1" ,"kg2" ,"kg3" ,"kg4" ,"kg5","lgd","ngd0","ngd5","ngd10","scd")

# scale = 0.01
scale_01=c("hurs_max","hurs_mean","hurs_min","hurs_range","pet_penman_max",
       "pet_penman_mean","pet_penman_min","pet_penman_range")

# scale = 0.001
scale_001=c("rsds","sfcWind_max","sfcWind_mean","sfcWind_min","sfcWind_range","pet_penman_max","pet_penman_mean","pet_penman_min","pet_penman_range","rsds_max","rsds_mean","rsds_min","rsds_range")

# Remplacement des valeurs de l'échelle en fonction des conditions
conv_df_climat$scale <- ifelse(conv_df_climat$shortname %in% scale_1, 1,
              ifelse(conv_df_climat$shortname %in% scale_01, 0.01,
                    ifelse(conv_df_climat$shortname %in% scale_001,0.001, 0.1)))

# offset = 0
conv_df_climat$offset = rep(0)
# offset = - 273.15
offset_273=c("bio1","bio5","bio6","bio8","bio9","bio10","bio11","gdgfgd10","gsl","gst")
conv_df_climat$offset = ifelse(conv_df_climat$shortname %in% offset_273, -273.15, 0)

# Pas present dans dans le pdf explicative donc pas de conversion
pas_pdf=c( "ai","swb", "clt_max","clt_mean","clt_min","clt_range")
verif=c(unit_100,scale_1,scale_01,scale_001,offset_273)
pas_pdf_2=setdiff(conv_df_climat$shortname, verif)
conv_df_climat[conv_df_climat$shortname %in% pas_pdf,"scale"] = 1

#bdd_climat_ok=bdd_climat[,c("ID","gps_x","gps_y")]

#for ( i in conv_df_climat$shortname){
  #if (i %in% names(bdd_climat)){
  #unitee= conv_df_climat[conv_df_climat$shortname ==i,"unit"]
  #echelle = conv_df_climat[conv_df_climat$shortname ==i,"scale"]
  #decalage = conv_df_climat[conv_df_climat$shortname ==i,"offset"]
  #bdd_climat_ok[[i]] = ((bdd_climat[[i]] / unitee)* echelle) + decalage
  #}else {
    #cat("Attention ",i, "n'exite pas dans la bdd_climat","\n")
  #}
#}


# chemin_fichier <- "C:/Users/diall/OneDrive/Bureau/M2_MODE/stage_abdou_m2/datas/bdd_climat_ok.rds"
# saveRDS(bdd_climat_ok, chemin_fichier)
# bdd_climat_ok <- readRDS(chemin_fichier)
# fin conversion

#df_fusion <- subset(bdd_climat_ok, select = -c(ID,gps_x, gps_y))
#bdd <- cbind(bdd, df_fusion) # all = TRUE pour garder toutes les lignes
```

## List of variables

[Variable description](https://chelsa-climate.org/wp-admin/download-page/CHELSA_tech_specification_V2.pdf){target="_blank"}

```{r}
summary(bdd_climat_ok)
```




## Temperature

-   Average annual air temperature (°C) = bio1

```{r Temperature,fig.align='center',fig.height=8}
summary(bdd$bio1)
explo_num(nom_col = "bio1", titre = "temp°.")
```

## Precipitation

-   Annual precipitation (kg/m²) = bio12

```{r Precipitation,fig.align='center',fig.height=8}
summary(bdd$bio12)
explo_num(nom_col = "bio12", titre = "Précipitat°.")
```





<!--
# Questions

```{r Questions}
ID_Site_dupliques <- bdd$ID_Site[duplicated(bdd$ID_Site)]
#length(ID_Site_dupliques)

lignes_dupliquees <- subset(bdd, duplicated(ID_Site))

lignes_unique <- unique(lignes_dupliquees$ID_Site)
#length(lignes_unique)

# nrow(bdd) - length(ID_Site_dupliques) + length(lignes_unique)
```

-   Comment gérer la répétition temporelle des parcelles ?
    -   Avec répétition : **`r nrow(bdd)`** observations
    -   Sans répétition : **`r nrow(bdd) - length(ID_Site_dupliques) + length(lignes_unique)`** observations
-   Liens des données du sol (sable, argile et limon) de data.gouv.fr ?

-->

# Soil data extraction

```{r Soil data extraction}
# Calcul des distances euclidiennes entre les sites
distances <- dist(cbind(bdd$gps_x, bdd$gps_y))
distance_moyenne <- mean(distances)
# distance_moyenne

df_col= taux_completion(bdd,FALSE,trie = TRUE)
df_col = df_col[df_col$Variables != "Total",]
```


## The source database ([openlandmap](https://openlandmap.org/?center=25,39&zoom=4&opacity=72&base=OpenStreetMap&layer=lc_glc.fcs30d&time=2022){target="_blank"})

```{r soil source database,echo=TRUE}
chemin_fichier_excel <- "C:/Users/diall/Downloads/datas/ODMAP.xlsx"
pedo <- read.xlsx(chemin_fichier_excel, sheet = "pedo")

# Fusion des cellules des colonnes avec des éléments dupliqués
for (col in names(pedo)) {
  pedo[[col]] <- ifelse(duplicated(pedo[[col]]), "", pedo[[col]])
}

#tableau avec kableExtra et centrage du contenu des cellules
kableExtra::kable(pedo) %>%
  kableExtra::kable_styling() %>%
  kableExtra::column_spec(1:ncol(pedo))  # Centrer le contenu de toutes les colonnes
```

\n
-   Average values between surface (0 cm) and 30 cm depth


## Changing the resolution ![](https://logowik.com/content/uploads/images/python.jpg){width="200"}

-   Long compilation time in R

-   GDAL module with the resampleAlg = bilinear method

-   Resolution = 0.0083 = 30 arc-second \~ 1km

```{r changing resolution}
    test_resolution = bdd
    tif_file_path_origine = "C:/Users/diall/Downloads/datas/raster_origine/sol_ph.h2o_usda.4c1a2a_m_250m_b10..10cm_1950..2017_v0.2.tif"
    raster_ph_origine <- raster(tif_file_path_origine)
    test_resolution <- extraction(nom_col = "ph_10_origine",df = test_resolution,conv = 10, 
                      tif_file_path = tif_file_path_origine)


    tif_file_path_rech = "C:/Users/diall/Downloads/datas/raster_modif/sol_ph.h2o_usda.4c1a2a_m_250m_b10..10cm_1950..2017_v0.2.tif"
    raster_ph_rech <- raster(tif_file_path_rech)
    test_resolution <- extraction(nom_col = "ph_10_rech",df = test_resolution,conv = 10, 
                      tif_file_path = tif_file_path_rech)

    par(mforw=c(1,2))
    image(raster_ph_origine,main="pH at 10cm: original raster (0.002)")
    image(raster_ph_rech, main = "pH at 10cm: raster modify (0.008)")


    bdd_echan = test_resolution
    bdd_echan <- bdd_echan %>%filter(!is.na(ph_10_origine) & !is.na(ph_10_rech))

    # graphique avec ggplot
        # coefficient de corrélation
    correlation <- cor(as.numeric(bdd_echan$ph_10_origine), bdd_echan$ph_10_rech)
    p <- ggplot(bdd_echan, aes(x = ph_10_origine, y = ph_10_rech)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = FALSE, color = "red") + 
      labs(subtitle = paste("r = ", round(correlation, 2)),
           x = "Original pH", y = "Resampled pH") + 
      theme_classic() 

    p

    ```





## Soil organic carbone (g/kg)

```{r extract C,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = "c_orga_0",df = bdd,conv = 5, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_organic.carbon_usda.6a1c_m_250m_b0..0cm_1950..2017_v0.2.tif")
bdd <- extraction(nom_col = "c_orga_10",df = bdd,conv = 5, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_organic.carbon_usda.6a1c_m_250m_b10..10cm_1950..2017_v0.2.tif")
bdd <- extraction(nom_col = "c_orga_30",df = bdd,conv = 5, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_organic.carbon_usda.6a1c_m_250m_b30..30cm_1950..2017_v0.2.tif")
bdd = moyenne_val_extrct(nom_col = "c_orga_0_a_30", vec_col = c("c_orga_0","c_orga_10","c_orga_30"),df=bdd)



df_cleaned = bdd

df_cleaned$c_orga_0_a_30 = as.numeric(df_cleaned$c_orga_0_a_30)
explo_num(nom_col = 'c_orga_0_a_30', titre = 'c_orga_0_a_30 (before cleaning)', df = df_cleaned)
df_cleaned <- test_grub(df_cleaned, 'c_orga_0_a_30', direction = 'maxi')
df_cleaned <- test_grub(df_cleaned, 'c_orga_0_a_30', direction = 'mini')
explo_num(nom_col = 'c_orga_0_a_30', titre = 'c_orga_0_a_30 (after cleaning)', df = df_cleaned)
# summary(df_cleaned$c_orga_0_a_30) 
bdd = df_cleaned


```

## pH

**Extracted values**

```{r extract pH,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = "ph_0",df = bdd,conv = 10, 
                  tif_file_path ="C:/Users/diall/Downloads/datas/raster_modif/sol_ph.h2o_usda.4c1a2a_m_250m_b10..10cm_1950..2017_v0.2.tif")

bdd <- extraction(nom_col = "ph_10" ,df = bdd,conv = 10, 
                  tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_ph.h2o_usda.4c1a2a_m_250m_b10..10cm_1950..2017_v0.2.tif")
bdd <- extraction(nom_col = "ph_30" ,df = bdd,conv = 10, 
                  tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_ph.h2o_usda.4c1a2a_m_250m_b30..30cm_1950..2017_v0.2.tif")
bdd = moyenne_val_extrct(nom_col = "ph_0_a_30", vec_col = c("ph_0","ph_10","ph_30"),df=bdd)
# summary(bdd$ph_0_a_30)

df_cleaned = bdd

df_cleaned$ph_0_a_30 = as.numeric(df_cleaned$ph_0_a_30)
explo_num(nom_col = 'ph_0_a_30', titre = 'ph_0_a_30 (before cleaning)', df = df_cleaned)
df_cleaned <- test_grub(df_cleaned, 'ph_0_a_30', direction = 'maxi')
df_cleaned <- test_grub(df_cleaned, 'ph_0_a_30', direction = 'mini')
explo_num(nom_col = 'ph_0_a_30', titre = 'ph_0_a_30 (after cleaning)', df = df_cleaned)
# summary(df_cleaned$ph_0_a_30) 
bdd = df_cleaned

```

**Measured values & extracted values**

-   Clean pH column

```{r clean pH,echo=TRUE}
# On recupere les deux colonnes du pH
df_comp=bdd[, c("ID", "ID_Site","ph_eau","ph_0_a_30" )]
df_comp =df_comp[complete.cases(df_comp$ph_eau),] 
df_comp =df_comp[complete.cases(df_comp$ph_0_a_30),] 
df_comp <- df_comp[!grepl("[^0-9.]", df_comp$ph_eau), ]
df_comp$ph_eau <- as.numeric(df_comp$ph_eau)
df_comp$ph_0_a_30 <- as.numeric(df_comp$ph_0_a_30)


df_comp = df_comp[!df_comp$ph_eau== 44140.00,]
df_comp = df_comp[!df_comp$ph_eau== "NA",]
df_comp = df_comp[!df_comp$ph_0_a_30== "NA",]
df_comp = droplevels(df_comp)
```

```{r,echo=TRUE}
ID_Site_dupliques <- df_comp$ID_Site[duplicated(df_comp$ID_Site)]
#length(ID_Site_dupliques)

lignes_dupliquees <- subset(df_comp, duplicated(ID_Site) & duplicated(ph_eau))

lignes_unique <- unique(lignes_dupliquees$ID_Site )
#length(lignes_unique)

# nrow(df_comp) - length(ID_Site_dupliques) + length(lignes_unique)


dupliquees <- duplicated(df_comp$ID_Site)
df_comp <- df_comp[!dupliquees, ]
df_comp=droplevels(df_comp)

# correlation <- cor.test(df_comp$ph_eau, df_comp$ph_0_a_30,method = "pearson")
#resultat_test <- t.test(df_comp$ph_eau, df_comp$ph_0_a_30)

df_comp$ph_eau <- as.numeric(df_comp$ph_eau)
df_comp$ph_0_a_30 <- as.numeric(df_comp$ph_0_a_30)

```

<!-- <br/>  -->

::: columns
::: {.column width="60%"}
-   Method ?

-   Depth ?

-   Measured values (CR = `r df_col[df_col$Variable=="ph_eau","CR"]`)

```{r}
  summary(df_comp$ph_eau)
```

-   Extracted values

```{r}
  summary(df_comp$ph_0_a_30)
```
:::

::: {.column width="40%"}
```{r fig cor pH,fig.align='center',fig.height=5,fig.width=4}
    correlation <- cor(as.numeric(df_comp$ph_eau), df_comp$ph_0_a_30)
# graphique avec ggplot
    p <- ggplot(df_comp, aes(x = ph_eau, y = ph_0_a_30)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = FALSE, color = "red") + 
      labs(subtitle =paste("r = ", round(correlation, 2)),x = "pH measured values", y = "pH extracted values") + 
      theme_classic() 
p
# plot(as.numeric(df_comp$ph_eau))
```
:::
:::

<!--
## Bulk density (kg / m-cube)

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = "d_ap_0",df = bdd,conv = 10, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_bulkdens.fineearth_usda.4a1h_m_250m_b0..0cm_1950..2017_v0.2.tif")
bdd <- extraction(nom_col = "d_ap_10",df = bdd,conv = 10, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_bulkdens.fineearth_usda.4a1h_m_250m_b10..10cm_1950..2017_v0.2.tif")
bdd <- extraction(nom_col = "d_ap_30",df = bdd,conv = 10, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_bulkdens.fineearth_usda.4a1h_m_250m_b30..30cm_1950..2017_v0.2.tif")
bdd = moyenne_val_extrct(nom_col = "d_ap_0_a_30", vec_col = c("d_ap_0","d_ap_10","d_ap_30"),bdd)
summary(bdd$d_ap_0_a_30)
explo_num(nom_col = "d_ap_0_a_30", titre = "Bulk density (0 - 30 cm)")
```


## Sand content (% kg/kg)

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = "sable_0",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_sand.wfraction_usda.3a1a1a_m_250m_b0..0cm_1950..2017_v0.2.tif")
bdd <- extraction(nom_col = "sable_10",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_sand.wfraction_usda.3a1a1a_m_250m_b10..10cm_1950..2017_v0.2.tif")
bdd <- extraction(nom_col = "sable_30",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_sand.wfraction_usda.3a1a1a_m_250m_b30..30cm_1950..2017_v0.2.tif")
bdd = moyenne_val_extrct(nom_col = "sable_0_a_30", vec_col = c("sable_0","sable_10","sable_30"),df=bdd)
summary(bdd$sable_0_a_30)
explo_num(nom_col = "sable_0_a_30", titre = "Sand (0 - 30 cm)")
```



-->

## Sand

**Extracted values (g/kg, 0 - 30 cm)**

```{r extract sand,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = "sable.0_5",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sable.0_5.tif")

bdd <- extraction(nom_col = "sable.5_15",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sable.5_15.tif")

bdd <- extraction(nom_col = "sable.15_30",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sable.15_30.tif")

bdd = moyenne_val_extrct(nom_col = "sable.0_30", vec_col = c("sable.0_5","sable.5_15","sable.15_30"),df=bdd)

# summary(bdd$sable.0_30)


df_cleaned = bdd

df_cleaned$sable.0_30 = as.numeric(df_cleaned$sable.0_30)
explo_num(nom_col = 'sable.0_30', titre = 'sable.0_30 (before cleaning)', df = df_cleaned)
df_cleaned <- test_grub(df_cleaned, 'sable.0_30', direction = 'maxi')
df_cleaned <- test_grub(df_cleaned, 'sable.0_30', direction = 'mini')
explo_num(nom_col = 'sable.0_30', titre = 'sable.0_30 (after cleaning)', df = df_cleaned)
# summary(df_cleaned$sable.0_30) 
bdd = df_cleaned

```

**Measured values & extracted values**

-   Clean sand column

```{r,echo=TRUE}
# On recupere les deux colonnes du pH
df_comp=bdd[, c("ID", "ID_Site","sand","sable.0_30" )]
df_comp =df_comp[complete.cases(df_comp$sand),] 
df_comp =df_comp[complete.cases(df_comp$sable.0_30),] 
df_comp <- df_comp[!grepl("[^0-9.]", df_comp$sand), ]
df_comp$sand <- as.numeric(df_comp$sand)
df_comp$sable.0_30 <- as.numeric(df_comp$sable.0_30)
# colSums(is.na(df_comp))

df_comp = df_comp[!df_comp$sand== "NA",]
df_comp = df_comp[!df_comp$sable.0_30== "NaN",]
df_comp = droplevels(df_comp)
```

```{r,echo=TRUE}
# -   Deleting duplicate measured values

ID_Site_dupliques <- df_comp$ID_Site[duplicated(df_comp$ID_Site)]
#length(ID_Site_dupliques)

lignes_dupliquees <- subset(df_comp, duplicated(ID_Site) & duplicated(sand))

lignes_unique <- unique(lignes_dupliquees$ID_Site )
#length(lignes_unique)
# nrow(df_comp) - length(ID_Site_dupliques) + length(lignes_unique)

dupliquees <- duplicated(df_comp$ID_Site)
df_comp <- df_comp[!dupliquees, ]
df_comp=droplevels(df_comp)
df_comp$sand <- as.numeric(df_comp$sand)
df_comp$sable.0_30 <- as.numeric(df_comp$sable.0_30)

# summary(df_comp$sand)
# explo_num(nom_col = "sand", titre = "Sand extracted values",df = df_comp)
id_ligne <- df_comp[which(df_comp$sand >=83), "ID"] 
df_comp <- df_comp[!df_comp$ID %in% id_ligne, ]
df_comp=droplevels(df_comp)


# 
# summary(df_comp$sable.0_30)
# explo_num(nom_col = "sable.0_30", titre = "Sand extracted values",df = df_comp)
id_ligne <- df_comp[which(df_comp$sable.0_30 >=60), "ID"] 
df_comp <- df_comp[!df_comp$ID %in% id_ligne, ]
df_comp=droplevels(df_comp)

```

::: columns
::: {.column width="60%"}
-   Method ?

-   Depth ?

-   Measured values (CR = `r df_col[df_col$Variable=="sand","CR"]`)

```{r}
  summary(df_comp$sand)
```

-   Extracted values

```{r}
  summary(df_comp$sable.0_30)
```
:::

::: {.column width="40%"}
\n\n\n

```{r,fig.align='center',fig.height=5,fig.width=4}
# graphique avec ggplot
correlation <- cor(as.numeric(df_comp$sand), df_comp$sable.0_30)
    p <- ggplot(df_comp, aes(x = sand, y = sable.0_30)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = FALSE, color = "red") + 
      labs(subtitle =paste("r = ", round(correlation, 2)) ,x = "Sand measured values", y = "Sand extracted values") + 
      theme_classic() 
p
# plot(as.numeric(df_comp$sand))
# plot(df_comp$sable.0_30)
```

:::
:::

## Silt

**Extracted values (g/kg, 0 - 30 cm)**

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = "limon.0_5",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/limon.0_5.tif")

bdd <- extraction(nom_col = "limon.5_15",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/limon.5_15.tif")

bdd <- extraction(nom_col = "limon.15_30",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/limon.15_30.tif")

bdd = moyenne_val_extrct(nom_col = "limon.0_30", vec_col = c("limon.0_5","limon.5_15","limon.15_30"),df=bdd)

df_cleaned = bdd

df_cleaned$limon.0_30 = as.numeric(df_cleaned$limon.0_30)
explo_num(nom_col = 'limon.0_30', titre = 'limon.0_30 (before cleaning)', df = df_cleaned)
df_cleaned <- test_grub(df_cleaned, 'limon.0_30', direction = 'maxi')
df_cleaned <- test_grub(df_cleaned, 'limon.0_30', direction = 'mini')
explo_num(nom_col = 'limon.0_30', titre = 'limon.0_30 (after cleaning)', df = df_cleaned)
# summary(df_cleaned$limon.0_30) 
bdd = df_cleaned

```

**Measured values & extracted values**

-   Clean silt column

```{r,echo=TRUE}
# On recupere les deux colonnes du pH
df_comp=bdd[, c("ID", "ID_Site","silt","limon.0_30" )]
df_comp =df_comp[complete.cases(df_comp$silt),] 
df_comp =df_comp[complete.cases(df_comp$limon.0_30),] 
df_comp <- df_comp[!grepl("[^0-9.]", df_comp$silt), ]
df_comp$silt <- as.numeric(df_comp$silt)
df_comp$limon.0_30 <- as.numeric(df_comp$limon.0_30)
# colSums(is.na(df_comp))


df_comp = df_comp[!df_comp$silt== "NA",]
df_comp = df_comp[!df_comp$limon.0_30== "NaN",]
df_comp = droplevels(df_comp)
```

```{r,echo=TRUE}
# -   Deleting duplicate measured values

ID_Site_dupliques <- df_comp$ID_Site[duplicated(df_comp$ID_Site)]
#length(ID_Site_dupliques)

lignes_dupliquees <- subset(df_comp, duplicated(ID_Site) & duplicated(silt))

lignes_unique <- unique(lignes_dupliquees$ID_Site )
#length(lignes_unique)
# nrow(df_comp) - length(ID_Site_dupliques) + length(lignes_unique)


dupliquees <- duplicated(df_comp$ID_Site)
df_comp <- df_comp[!dupliquees, ]
df_comp=droplevels(df_comp)
df_comp$silt <- as.numeric(df_comp$silt)
df_comp$limon.0_30 <- as.numeric(df_comp$limon.0_30)




# summary(df_comp$silt)
# explo_num(nom_col = "silt", titre = "Silt",df = df_comp)
id_ligne <- df_comp[which(df_comp$silt <=7.3), "ID"] 
df_comp <- df_comp[!df_comp$ID %in% id_ligne, ]
df_comp=droplevels(df_comp)



# summary(df_comp$limon.0_30)
# explo_num(nom_col = "limon.0_30", titre = "limon.0_30",df = df_comp)
id_ligne <- df_comp[which(df_comp$limon.0_30 >=80), "ID"] 
df_comp <- df_comp[!df_comp$ID %in% id_ligne, ]
df_comp=droplevels(df_comp)


```

::: columns
::: {.column width="60%"}
-   Method ?

-   Depth ?

-   Measured values (CR = `r df_col[df_col$Variable=="silt","CR"]`)

```{r}
  summary(df_comp$silt)
```

-   Extracted values

```{r}
  summary(df_comp$limon.0_30)
```
:::

::: {.column width="40%"}
\n\n\n

```{r,fig.align='center',fig.height=5,fig.width=4}
# graphique avec ggplot
correlation <- cor(as.numeric(df_comp$silt), df_comp$limon.0_30)
    p <- ggplot(df_comp, aes(x = silt, y = limon.0_30)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = FALSE, color = "red") + 
      labs(subtitle =paste("r = ", round(correlation, 2)) ,x = "Silt measured values", y = "Silt extracted values") + 
      theme_classic() 
p

# plot(as.numeric(df_comp$limon.0_30))
```


:::
:::

## Clay

**Extracted values (g/kg, 0 - 30 cm)**

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = "argile.0_5",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/argile.0_5.tif")

bdd <- extraction(nom_col = "argile.5_15",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/argile.5_15.tif")

bdd <- extraction(nom_col = "argile.15_30",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/argile.15_30.tif")

bdd = moyenne_val_extrct(nom_col = "argile.0_30", vec_col = c("argile.0_5","argile.5_15","argile.15_30"),df=bdd)


df_cleaned = bdd

df_cleaned$argile.0_30 = as.numeric(df_cleaned$argile.0_30)
explo_num(nom_col = 'argile.0_30', titre = 'argile.0_30 (before cleaning)', df = df_cleaned)
df_cleaned <- test_grub(df_cleaned, 'argile.0_30', direction = 'maxi')
df_cleaned <- test_grub(df_cleaned, 'argile.0_30', direction = 'mini')
explo_num(nom_col = 'argile.0_30', titre = 'argile.0_30 (after cleaning)', df = df_cleaned)
# summary(df_cleaned$argile.0_30) 
bdd = df_cleaned

```

**Measured values & extracted values** - Clean clay column

```{r,echo=TRUE}
# On recupere les deux colonnes du pH
df_comp=bdd[, c("ID", "ID_Site","clay","argile.0_30" )]
df_comp =df_comp[complete.cases(df_comp$clay),] 
df_comp =df_comp[complete.cases(df_comp$argile.0_30),] 
df_comp <- df_comp[!grepl("[^0-9.]", df_comp$clay), ]
df_comp$clay <- as.numeric(df_comp$clay)
df_comp$argile.0_30 <- as.numeric(df_comp$argile.0_30)
# colSums(is.na(df_comp))

df_comp = df_comp[!df_comp$clay== "NA",]
df_comp = df_comp[!df_comp$argile.0_30== "NaN",]
df_comp = droplevels(df_comp)
```

```{r,echo=TRUE}
# -   Deleting duplicate measured values

ID_Site_dupliques <- df_comp$ID_Site[duplicated(df_comp$ID_Site)]
#length(ID_Site_dupliques)

lignes_dupliquees <- subset(df_comp, duplicated(ID_Site) & duplicated(clay))

lignes_unique <- unique(lignes_dupliquees$ID_Site )
#length(lignes_unique)
# nrow(df_comp) - length(ID_Site_dupliques) + length(lignes_unique)

dupliquees <- duplicated(df_comp$ID_Site)
df_comp <- df_comp[!dupliquees, ]
df_comp=droplevels(df_comp)
df_comp$clay <- as.numeric(df_comp$clay)
df_comp$argile.0_30 <- as.numeric(df_comp$argile.0_30)


df_comp$clay = as.numeric(df_comp$clay)/10 # pour conv en %

```

::: columns
::: {.column width="60%"}
-   Method ?

-   Depth ?

-   Measured values (CR = `r df_col[df_col$Variable=="clay","CR"]`)

```{r}
  summary(df_comp$clay)
```

-   Extracted values

```{r}
  summary(df_comp$argile.0_30)
```
:::

::: {.column width="40%"}
\n\n\n

```{r,fig.align='center',fig.height=5,fig.width=4}
# graphique avec ggplot
correlation <- cor(as.numeric(df_comp$clay), df_comp$argile.0_30)
    p <- ggplot(df_comp, aes(x = clay, y = argile.0_30)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = FALSE, color = "red") + 
      labs(subtitle =paste("r = ", round(correlation, 2)) ,x = "Clay measured values", y = "Clay extracted values") + 
      theme_classic() 
p
```
:::
:::

## Elevation

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = "elevation",df = bdd,conv = 1, 
                  tif_file_path ="C:/Users/diall/Downloads/datas/raster_modif/GMTED2010_Spatial.tif")


df_cleaned = bdd

df_cleaned$elevation = as.numeric(df_cleaned$elevation)
explo_num(nom_col = 'elevation', titre = 'elevation (before cleaning)', df = df_cleaned)
df_cleaned <- test_grub(df_cleaned, 'elevation', direction = 'maxi')
df_cleaned <- test_grub(df_cleaned, 'elevation', direction = 'mini')
explo_num(nom_col = 'elevation', titre = 'elevation (after cleaning)', df = df_cleaned)
# summary(df_cleaned$elevation) 
bdd = df_cleaned

```


<!--
## pH_H2O_CaCl 

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = 'jrc_pH_H2O_CaCl', df = bdd, conv = 1, tif_file_path = 'C:/Users/diall/Downloads/datas/raster_modif/pH_H2O_CaCl.tif')

summary(bdd$jrc_pH_H2O_CaCl)

explo_num(nom_col = 'jrc_pH_H2O_CaCl', titre = 'jrc_pH_H2O_CaCl')
```

## pH_H2O 

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = 'jrc_pH_H2O', df = bdd, conv = 1, tif_file_path = 'C:/Users/diall/Downloads/datas/raster_modif/pH_H2O.tif')

summary(bdd$jrc_pH_H2O)

explo_num(nom_col = 'jrc_pH_H2O', titre = 'jrc_pH_H2O')

df_comp=bdd[, c("ID", "ID_Site","ph_eau","jrc_pH_H2O" )]
df_comp =df_comp[complete.cases(df_comp$ph_eau),] 
df_comp =df_comp[complete.cases(df_comp$jrc_pH_H2O),] 
df_comp <- df_comp[!grepl("[^0-9.]", df_comp$ph_eau), ]
df_comp$ph_eau <- as.numeric(df_comp$ph_eau)
df_comp$jrc_pH_H2O <- as.numeric(df_comp$jrc_pH_H2O)


df_comp = df_comp[!df_comp$ph_eau== 44140.00,]
df_comp = df_comp[!df_comp$ph_eau== "NA",]
df_comp = df_comp[!df_comp$jrc_pH_H2O== "NA",]
df_comp = droplevels(df_comp)

correlation <- cor(as.numeric(df_comp$ph_eau), df_comp$jrc_pH_H2O)
```

## pH_CaCl 

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = 'jrc_pH_CaCl', df = bdd, conv = 1, tif_file_path = 'C:/Users/diall/Downloads/datas/raster_modif/pH_CaCl.tif')

summary(bdd$jrc_pH_CaCl)

explo_num(nom_col = 'jrc_pH_CaCl', titre = 'jrc_pH_CaCl')
```

-->


## Phosphore (P, mg/kg)

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = 'P', df = bdd, conv = 1, tif_file_path = 'C:/Users/diall/Downloads/datas/raster_modif/P.tif')

df_cleaned = bdd

df_cleaned$P = as.numeric(df_cleaned$P)
explo_num(nom_col = 'P', titre = 'P (before cleaning)', df = df_cleaned)
df_cleaned <- test_grub(df_cleaned, 'P', direction = 'maxi')
df_cleaned <- test_grub(df_cleaned, 'P', direction = 'mini')
explo_num(nom_col = 'P', titre = 'P (after cleaning)', df = df_cleaned)
# summary(df_cleaned$P) 
bdd = df_cleaned
```

## Azote (N, g/kg)

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = 'N', df = bdd, conv = 1, tif_file_path = 'C:/Users/diall/Downloads/datas/raster_modif/N.tif')


df_cleaned = bdd

df_cleaned$N = as.numeric(df_cleaned$N)
explo_num(nom_col = 'N', titre = 'N (before cleaning)', df = df_cleaned)
df_cleaned <- test_grub(df_cleaned, 'N', direction = 'maxi')
df_cleaned <- test_grub(df_cleaned, 'N', direction = 'mini')
explo_num(nom_col = 'N', titre = 'N (after cleaning)', df = df_cleaned)
# summary(df_cleaned$N) 
bdd = df_cleaned
```

## Potassium (K, mg/kg)

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = 'K', df = bdd, conv = 1, tif_file_path = 'C:/Users/diall/Downloads/datas/raster_modif/K.tif')


df_cleaned = bdd

df_cleaned$K = as.numeric(df_cleaned$K)
explo_num(nom_col = 'K', titre = 'K (before cleaning)', df = df_cleaned)
df_cleaned <- test_grub(df_cleaned, 'K', direction = 'maxi')
df_cleaned <- test_grub(df_cleaned, 'K', direction = 'mini')
explo_num(nom_col = 'K', titre = 'K (after cleaning)', df = df_cleaned)
# summary(df_cleaned$N) 
bdd = df_cleaned
```

## C/N

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = 'CN', df = bdd, conv = 1, tif_file_path = 'C:/Users/diall/Downloads/datas/raster_modif/CN.tif')


df_cleaned = bdd

df_cleaned$CN = as.numeric(df_cleaned$CN)
explo_num(nom_col = 'CN', titre = 'CN (before cleaning)', df = df_cleaned)
df_cleaned <- test_grub(df_cleaned, 'CN', direction = 'maxi')
df_cleaned <- test_grub(df_cleaned, 'CN', direction = 'mini')
explo_num(nom_col = 'CN', titre = 'CN (after cleaning)', df = df_cleaned)
# summary(df_cleaned$N) 
bdd = df_cleaned
```

## Capacité d'échange de cations (CEC, cmol/kg)

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = 'CEC', df = bdd, conv = 1, tif_file_path = 'C:/Users/diall/Downloads/datas/raster_modif/CEC.tif')

df_cleaned = bdd

df_cleaned$CEC = as.numeric(df_cleaned$CEC)
explo_num(nom_col = 'CEC', titre = 'CEC (before cleaning)', df = df_cleaned)
df_cleaned <- test_grub(df_cleaned, 'CEC', direction = 'maxi')
df_cleaned <- test_grub(df_cleaned, 'CEC', direction = 'mini')
explo_num(nom_col = 'CEC', titre = 'CEC (after cleaning)', df = df_cleaned)
# summary(df_cleaned$N) 
bdd = df_cleaned
```

## Carbonates de calcium (CaCO3, g/kg)

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = 'CaCO3', df = bdd, conv = 1, tif_file_path = 'C:/Users/diall/Downloads/datas/raster_modif/CaCO3.tif')


df_cleaned = bdd

df_cleaned$CaCO3 = as.numeric(df_cleaned$CaCO3)
explo_num(nom_col = 'CaCO3', titre = 'CaCO3 (before cleaning)', df = df_cleaned)
df_cleaned <- test_grub(df_cleaned, 'CaCO3', direction = 'maxi')
df_cleaned <- test_grub(df_cleaned, 'CaCO3', direction = 'mini')
explo_num(nom_col = 'CaCO3', titre = 'CaCO3 (after cleaning)', df = df_cleaned)
# summary(df_cleaned$N) 
bdd = df_cleaned
```

# Exploratory analysis

**Data set reduction**

```{r ana explo,echo=TRUE,fig.height=8,fig.show='animate',fig.align='center'}
id_col=c("ID","Programme","Annee","ID_Site","Protocole")

vdt_col=c("AB_tot", "BM_tot", "Richesse_tot")

land_cover_col=c("clcm_lvl3")

topo_col=c("elevation","gps_x","gps_y")


soil_col=c("ph_0_a_30","sable.0_30","limon.0_30","argile.0_30","c_orga_0_a_30","P","N","K","CN","CEC","CaCO3")


climate_col=c()
for (i in 1:19){
  climate_col=c(climate_col, paste0("bio",i) )
}
climate_col=c(climate_col,"cmi_mean","gdd0","gdd10","hurs_mean","pet_penman_mean")

bdd_explo= bdd[,c(id_col,vdt_col,land_cover_col,topo_col,soil_col,climate_col)]
# str(bdd_explo)
bdd_explo$ID = as.factor(bdd_explo$ID)


# Renome
new_soil_col=c("pH","sand","silt","clay","C","P","N","K","CN","CEC","CaCO3")
bdd_explo <- rename(bdd_explo, !!setNames(soil_col, new_soil_col))

bdd_explo <- bdd_explo %>% 
  rename(PET = pet_penman_mean)
climate_col=c()
for (i in 1:19){
  climate_col=c(climate_col, paste0("bio",i) )
}
climate_col=c(climate_col,"cmi_mean","gdd0","gdd10","hurs_mean","PET")


col_graph=c(vdt_col,land_cover_col,topo_col,new_soil_col,climate_col)
# for (i in names(bdd_explo[,col_graph])){
#   par(mfrow=c(2,2))
#   plot(bdd_explo[[i]], main=i)
# }





levels(bdd_explo$clcm_lvl3)[levels(bdd_explo$clcm_lvl3) == "Broad-leaved forest"] <- "Forest"
levels(bdd_explo$clcm_lvl3)[levels(bdd_explo$clcm_lvl3) == "Coniferous forest"] <- "Forest"
levels(bdd_explo$clcm_lvl3)[levels(bdd_explo$clcm_lvl3) == "Mixed forest"] <- "Forest"

bdd_explo$clcm_lvl3= as.factor(bdd_explo$clcm_lvl3)

cl_original <- levels(bdd_explo$clcm_lvl3)
new_cl <- c("f","gua", "ng", "nial", "p", "v")
bdd_explo$clcm_lvl3 <- factor(bdd_explo$clcm_lvl3, levels = cl_original, labels = new_cl)

```

**Fork (Bifurcation) **
```{r}
bdd_all = bdd
bdd_sp = bdd

write.csv2(x =bdd,file = "datas/Non_repeated_data/bdd_all.csv", row.names = FALSE)
```


## Total abundance distributions

```{r abundance dist,fig.align='center',fig.height=4,fig.width=4}
df <- data.frame(y =bdd_explo$AB_tot)
# Test de Shapiro-Wilk
AB_tot_test_nor = shapiro.test(df$y)
AB_tot_p.value =round(AB_tot_test_nor$p.value,3)
 if(AB_tot_p.value ==0){
   AB_tot_p.value = "; p.value > 0.001"
 } else {
   AB_tot_p.value = paste0("; p.value = ",AB_tot_p.value)   
 }
AB_tot_sub = paste0("Shapiro-Wilk; W = ",round(AB_tot_test_nor$statistic,2),AB_tot_p.value)

```



::: columns
::: {.column width="50%"}

<p>
  <img src="lamda_boxcox.png">
</p>

<br/>
```{r}
# Histogramme
ggplot(df, aes(x=y)) +
  geom_histogram(aes(y=..density..), fill="#69b3a2", color="#e9ecef", bins=30, alpha=2) +
  geom_density(fill="black", alpha=0.2) +
  theme_gray() +
  labs(title="Abundance", subtitle =AB_tot_sub, x="Value", y="Density") +
  theme(plot.title = element_text(hjust = 0.5))
```

-   Transformation sqrt
```{r ,fig.dpi=300,fig.align='center',fig.height=4,fig.width=4}
df_2 <- data.frame(y =sqrt(bdd_explo$AB_tot))
# Test de Shapiro-Wilk
AB_tot_test_nor = shapiro.test(df_2$y)
AB_tot_p.value =round(AB_tot_test_nor$p.value,3)
 if(AB_tot_p.value ==0){
   AB_tot_p.value = "; p.value > 0.001"
 } else {
   AB_tot_p.value = paste0("; p.value = ",AB_tot_p.value)   
 }
AB_tot_sub = paste0("Shapiro-Wilk; W = ",round(AB_tot_test_nor$statistic,2),AB_tot_p.value)

```

```{r}
# Histogramme
ggplot(df_2, aes(x=y)) +
  geom_histogram(aes(y=..density..), fill="#69b3a2", color="#e9ecef", bins=30, alpha=2) +
  geom_density(fill="black", alpha=0.2) +
  theme_gray() +
  labs(title="Abundance", subtitle =AB_tot_sub, x="Value", y="Density") +
  theme(plot.title = element_text(hjust = 0.5))

```
:::

::: {.column width="50%"}
```{r}
# https://r-coder.com/box-cox-transformation-r/?utm_content=cmp-true
# bdd_explo$AB_tot[bdd_explo$AB_tot < 0]
x = as.numeric ( bdd_explo$AB_tot )+1


b <- boxcox(lm(x ~ 1),plotit = FALSE) # ou bestNormalize

# Exact lambda
lambda1 <- b$x[which.max(b$y)]

boxcox(lm(x ~ 1),plotit = TRUE)


```
    lamda = `r lambda1`

<br/> 

```{r}
# QQ-plot
qqnorm(df$y)
qqline(df$y)
```

<br/> 
<br/> 
```{r}

# QQ-plot
qqnorm(df_2$y)
qqline(df_2$y)
```

:::
:::

## Total biomass distributions

```{r biomass dist,fig.align='center',fig.height=4,fig.width=4}
df <- data.frame(y =bdd_explo$BM_tot)
# Test de Shapiro-Wilk
BM_tot_test_nor = shapiro.test(df$y)
BM_tot_p.value =round(BM_tot_test_nor$p.value,3)
 if(BM_tot_p.value ==0){
   BM_tot_p.value = "; p.value > 0.001"
 } else {
   BM_tot_p.value = paste0("; p.value = ",BM_tot_p.value)   
 }
BM_tot_sub = paste0("Shapiro-Wilk; W = ",round(BM_tot_test_nor$statistic,2),BM_tot_p.value)

```


::: columns
::: {.column width="50%"}

<p>
  <img src="lamda_boxcox.png">
</p>

<br/>
```{r}
# Histogramme
ggplot(df, aes(x=y)) +
  geom_histogram(aes(y=..density..), fill="#69b3a2", color="#e9ecef", bins=30, alpha=2) +
  geom_density(fill="black", alpha=0.2) +
  theme_gray() +
  labs(title="biomass", subtitle =BM_tot_sub, x="Value", y="Density") +
  theme(plot.title = element_text(hjust = 0.5))
```

-   Transformation sqrt
```{r ,fig.dpi=300,fig.align='center',fig.height=4,fig.width=4}
df_2 <- data.frame(y =sqrt(bdd_explo$BM_tot))
# Test de Shapiro-Wilk
BM_tot_test_nor = shapiro.test(df_2$y)
BM_tot_p.value =round(BM_tot_test_nor$p.value,3)
 if(BM_tot_p.value ==0){
   BM_tot_p.value = "; p.value > 0.001"
 } else {
   BM_tot_p.value = paste0("; p.value = ",BM_tot_p.value)   
 }
BM_tot_sub = paste0("Shapiro-Wilk; W = ",round(BM_tot_test_nor$statistic,2),BM_tot_p.value)

```

```{r}
# Histogramme
ggplot(df_2, aes(x=y)) +
  geom_histogram(aes(y=..density..), fill="#69b3a2", color="#e9ecef", bins=30, alpha=2) +
  geom_density(fill="black", alpha=0.2) +
  theme_gray() +
  labs(title="biomass", subtitle =BM_tot_sub, x="Value", y="Density") +
  theme(plot.title = element_text(hjust = 0.5))

```
:::

::: {.column width="50%"}
```{r}
# https://r-coder.com/box-cox-transformation-r/?utm_content=cmp-true
# bdd_explo$BM_tot[bdd_explo$BM_tot < 0]
x = as.numeric ( bdd_explo$BM_tot )+1


b <- boxcox(lm(x ~ 1),plotit = FALSE)

# Exact lambda
lambda <- b$x[which.max(b$y)]

lambda = round(lambda,3)
boxcox(lm(x ~ 1),plotit = TRUE)


```
    lamda = `r lambda`

<br/> 

```{r}
# QQ-plot
qqnorm(df$y)
qqline(df$y)
```

<br/> 
<br/> 
```{r}

# QQ-plot
qqnorm(df_2$y)
qqline(df_2$y)
```

:::
:::



## Total taxonomic richness distributions

```{r richness dist,fig.align='center',fig.height=4,fig.width=4}
df <- data.frame(y =bdd_explo$Richesse_tot)
# Test de Shapiro-Wilk
Richesse_tot_test_nor = shapiro.test(df$y)
Richesse_tot_p.value =round(Richesse_tot_test_nor$p.value,3)
 if(Richesse_tot_p.value ==0){
   Richesse_tot_p.value = "; p.value > 0.001"
 } else {
   Richesse_tot_p.value = paste0("; p.value = ",Richesse_tot_p.value)   
 }
Richesse_tot_sub = paste0("Shapiro-Wilk; W = ",round(Richesse_tot_test_nor$statistic,2),Richesse_tot_p.value)

```



::: columns
::: {.column width="50%"}

<p>
  <img src="lamda_boxcox.png">
</p>

<br/>
```{r}
# Histogramme
ggplot(df, aes(x=y)) +
  geom_histogram(aes(y=..density..), fill="#69b3a2", color="#e9ecef", bins=30, alpha=2) +
  geom_density(fill="black", alpha=0.2) +
  theme_gray() +
  labs(title="richness", subtitle =Richesse_tot_sub, x="Value", y="Density") +
  theme(plot.title = element_text(hjust = 0.5))
```

-   Transformation sqrt
```{r ,fig.dpi=300,fig.align='center',fig.height=4,fig.width=4}
df_2 <- data.frame(y =sqrt(bdd_explo$Richesse_tot))
# Test de Shapiro-Wilk
Richesse_tot_test_nor = shapiro.test(df_2$y)
Richesse_tot_p.value =round(Richesse_tot_test_nor$p.value,3)
 if(Richesse_tot_p.value ==0){
   Richesse_tot_p.value = "; p.value > 0.001"
 } else {
   Richesse_tot_p.value = paste0("; p.value = ",Richesse_tot_p.value)   
 }
Richesse_tot_sub = paste0("Shapiro-Wilk; W = ",round(Richesse_tot_test_nor$statistic,2),Richesse_tot_p.value)

```

```{r}
# Histogramme
ggplot(df_2, aes(x=y)) +
  geom_histogram(aes(y=..density..), fill="#69b3a2", color="#e9ecef", bins=30, alpha=2) +
  geom_density(fill="black", alpha=0.2) +
  theme_gray() +
  labs(title="richness", subtitle =Richesse_tot_sub, x="Value", y="Density") +
  theme(plot.title = element_text(hjust = 0.5))

```
:::

::: {.column width="50%"}
```{r}
# https://r-coder.com/box-cox-transformation-r/?utm_content=cmp-true
# bdd_explo$Richesse_tot[bdd_explo$Richesse_tot < 0]
x = as.numeric ( bdd_explo$Richesse_tot )+1


b <- boxcox(lm(x ~ 1),plotit = FALSE)

# Exact lambda
lambda1 <- b$x[which.max(b$y)]

boxcox(lm(x ~ 1),plotit = TRUE)


```
    lamda = `r lambda1`

<br/> 

```{r}
# QQ-plot
qqnorm(df$y)
qqline(df$y)
```

<br/> 
<br/> 
```{r}

# QQ-plot
qqnorm(df_2$y)
qqline(df_2$y)
```

:::
:::

## Standarization
```{r}
bdd_explo_non_t = bdd_explo
bdd_explo$AB_tot = sqrt(bdd_explo$AB_tot)
bdd_explo$BM_tot = sqrt(bdd_explo$BM_tot)
# bdd_explo$Richesse_tot = sqrt(bdd_explo$Richesse_tot)



# for(i in names(bdd_explo)){ cat (i,"," )}
predicteurs = c('elevation' ,'gps_x' ,'gps_y' ,'pH' ,'sand' ,'silt' ,'clay' ,'C' ,'P' ,'N' ,'K' ,'CN' ,'CEC' ,'CaCO3' ,'bio1' ,'bio2' ,'bio3' ,'bio4' ,'bio5' ,'bio6' ,'bio7' ,'bio8' ,'bio9' ,'bio10' ,'bio11' ,'bio12' ,'bio13' ,'bio14' ,'bio15' ,'bio16' ,'bio17' ,'bio18' ,'bio19' ,'cmi_mean' ,'gdd0' ,'gdd10' ,'hurs_mean' ,'PET')

bdd_explo[,predicteurs] = scale(bdd_explo[,predicteurs])



data_lm = bdd_explo # données pour les models GLM, GAM et polY
data_deep = bdd_explo # données pour les models RF, GBM et ANN

```
-   Transformation sqrt des variables de réponses

-   Transformation centrée reduite des prédicteurs

# Nettoyages des variables pour les GLM, GAM et POLY

## Test de correlation : intra catégories

-   **Topographie**

Colonnes supprimée : *gps_x*

```{r,echo=FALSE,fig.align='center'}
# ggpairs(data_lm[,topo_col])
correlation_matrix <- cor(data_lm[,topo_col],use = "na.or.complete")
# 
corrplot::corrplot(correlation_matrix, method = "color", type = "lower", order = "hclust", addCoef.col = "black",diag = FALSE,cl.cex = 1,number.cex = 1)
topo_sup=c("gps_x")
data_lm <- data_lm[, setdiff(names(data_lm), topo_sup)]

```


<br/>
-   **Soil data**

```{r,echo=FALSE}
# ggpairs(data_lm[,new_soil_col])
soil_sup=c("sand")
```

::: columns
::: {.column width="25%"}
<br/>
Colonnes supprimée : *`r soil_sup`*
:::

::: {.column width="75%"}

```{r,echo=FALSE,fig.height=6,fig.width=9,fig.align='right'}
correlation_matrix <- cor(data_lm[,new_soil_col],use = "na.or.complete")
corrplot::corrplot(correlation_matrix, method = "color", type = "lower", order = "hclust", addCoef.col = "black",diag = FALSE,cl.cex = 1,number.cex = 0.7)

data_lm <- data_lm[, setdiff(names(data_lm), soil_sup)]
```
:::
:::



<br/>
-   **Climat data**

```{r,echo=FALSE,out.height="100%",out.width="100%"}
# correlation_matrix <- cor(data_lm[,climate_col],use = "na.or.complete")
# 
# chemin="C:/Users/diall/OneDrive/Bureau/M2_MODE/stage_abdou_m2/R_Stage_M2/Results/climat_corrplot.png"
# png(chemin, width = 2000, height = 1000,res = 110)
# 
# 
# corrplot::corrplot(correlation_matrix, method = "color", type = "lower", order = "hclust", addCoef.col = "black",diag = FALSE,cl.cex = 1,number.cex = 0.7,mar = c(0,0,0,0))
# 
# dev.off()
# ![](Results/climat_corrplot.png)


# cor_function_seuil(data = data_lm[,climate_col], seuil = 0.7)
climat_sup=c("bio2","bio4","bio5","bio6","bio7", "bio9", "bio10","bio11","bio13","bio16","bio17","bio18","bio19","gdd0","gdd10", "cmi_mean","PET")
```

::: columns
::: {.column width="25%"}
<br/>
Colonnes supprimmées :*`r climat_sup`*
:::

::: {.column width="75%"}
```{r,echo=FALSE,fig.height=7,fig.width=9,fig.align='right'}
climat_selec = climate_col[!climate_col %in% climat_sup]
# ggpairs(data_lm[,climat_selec])
correlation_matrix <- cor(data_lm[,climat_selec],use = "na.or.complete")
corrplot::corrplot(correlation_matrix, method = "color", type = "lower", order = "hclust", addCoef.col = "black",diag = FALSE,cl.cex = 1,number.cex = 0.9,mar = c(0,0,0,0))

data_lm <- data_lm[, setdiff(names(data_lm), climat_sup)]
```
:::
:::



## Test de correlation : inter catégories

Colonnes supprimée : *gps_y*
```{r,echo=FALSE}
# correlation_matrix <- cor(data_lm[,10:31],use = "na.or.complete")
# corrplot::corrplot(correlation_matrix, method = "color", type = "lower", order = "hclust", addCoef.col = "black",diag = FALSE,cl.cex = 0.5,number.cex = 0.5,mar = c(0,0,0,0))

# var=c("elevation","pH","silt","clay","C","P","N","K","CN","CEC","CaCO3","bio1","bio3","bio8","bio12","bio14","bio15","hurs_mean","gps_y")
# 
# correlation_matrix <- cor(data_lm[,var],use = "na.or.complete")
# 
# chemin="C:/Users/diall/OneDrive/Bureau/M2_MODE/stage_abdou_m2/R_Stage_M2/Results/interC_corrplot.png"
# png(chemin, width = 1200, height = 1000,res = 110)
# 
# 
# corrplot::corrplot(correlation_matrix, method = "color", type = "lower", order = "hclust", addCoef.col = "black",diag = FALSE,cl.cex = 1,number.cex = 0.7,mar = c(0,0,0,0))
# 
# dev.off()


data_lm <- data_lm[, setdiff(names(data_lm), c("gps_y"))]
```

![](Results/interC_corrplot.png) {width="1200",aligne="center"}



## VIF

**Suppression de la variable bio14**

```{r VIF var}
var_avant=c("elevation","pH","silt","clay","C","P","N","K","CN","CEC","CaCO3","bio1","bio3",
      "bio8","bio12","bio14","bio15","hurs_mean")
#usdm::vif(data_lm[,var_avant])
#usdm::vifcor(data_lm[,var_avant], th = 0.9, keep = NULL, method = 'pearson')
usdm::vifstep(data_lm[,var_avant], th = 10, keep = NULL, method = 'pearson')
# -   On enleve "bio14" car la plus forte VIF
data_lm <- data_lm[, setdiff(names(data_lm), c("bio14"))]

var=c("elevation","pH","silt","clay","C","P","N","K","CN","CEC","CaCO3","bio1","bio3","bio8","bio12","bio15","hurs_mean")
```


<!--
## Création des variables factices

```{r factices var ,echo=TRUE}
data_lm$clcm_lvl3= as.factor(data_lm$clcm_lvl3)
summary_df <- as.data.frame(summary(data_lm$clcm_lvl3))
colnames(summary_df) <- c("Numbers")
kable(summary_df,padding = 5)
```

-   Merging:
    *Forest = Broad\_leaved forest +* 
                    *Coniferous forest +* 
                    *Mixed forest*

```{r clc3 levels, echo=FALSE}
levels(data_lm$clcm_lvl3)[levels(data_lm$clcm_lvl3) == "Broad-leaved forest"] <- "Forest"
levels(data_lm$clcm_lvl3)[levels(data_lm$clcm_lvl3) == "Coniferous forest"] <- "Forest"
data_lm$clcm_lvl3= as.factor(data_lm$clcm_lvl3)
# summary_df <- as.data.frame(summary(data_lm$clcm_lvl3))
# colnames(summary_df) <- c("Numbers")
# kable(summary_df,padding = 5)
```

-   Abréviation des levels

```{r clc3 names, echo=FALSE}
cl_original <- levels(data_lm$clcm_lvl3)
new_cl <- c("f","gua", "ng", "nial", "p", "v")
data_lm$clcm_lvl3 <- factor(data_lm$clcm_lvl3, levels = cl_original, labels = new_cl)

data_lm <- data_lm %>% 
  rename(clc3 = clcm_lvl3)
```


-   Variables

```{r head clc var ,echo=FALSE}
data_lm <- dummy_cols(data_lm, select_columns = c("clc3"))
clc3_col= c("clc3_mf","clc3_gua", "clc3_ng", "clc3_nial", "clc3_p", "clc3_v")
head(data_lm[,clc3_col])
# str(data_lm[,clc3_col])
```


```{r VIF and clc3}
# usdm::vif(data_lm[,c(var,clc3_col)])
# usdm::vifcor(data_lm[,c(var,clc3_col)], th = 0.9, keep = NULL, method = 'pearson')
# usdm::vifstep(data_lm[,c(var,clc3_col)], th = 10, keep = NULL, method = 'pearson')
# summary(data_lm$clc3)
```

-->

# Selecting variables with regsubsets()
```{r select var, echo=TRUE}
# colSums(is.na(data_lm))
data_lm <- data_lm[apply(data_lm[, !colnames(data_lm) %in% "BM_tot"], 1, function(x) all(!is.na(x))), ]
# colSums(is.na(data_lm))
# dim(data_lm)
```

## Selection for total abundance
```{r abundance selection, echo=TRUE, fig.align='center'}
# names(data_lm)
clc3_col= c("clc3_mf","clc3_gua", "clc3_ng", "clc3_nial", "clc3_p", "clc3_v")
supp = c("ID","Programme","Annee","ID_Site", "Protocole","BM_tot", "Richesse_tot","clcm_lvl3")
df_AB_tot= data_lm[, setdiff(names(data_lm), supp)]
df_AB_tot= df_AB_tot[, setdiff(names(df_AB_tot), clc3_col)]
# str(df_AB_tot)
# colSums(is.na(df_AB_tot))
results_AB_tot <- regsubsets(AB_tot ~ ., data = df_AB_tot,method = "exhaustive",nvmax =17 )
# plot(results_AB_tot, scale = "r2", main='R² criteria')
# summary(results_AB_tot)
rsq_AB_tot= round (summary(results_AB_tot)$rsq,2)
adjr2_AB_tot= round(summary(results_AB_tot)$adjr2,2)
cp_AB_tot=round (summary(results_AB_tot)$cp,2)
bic_AB_tot=round(summary(results_AB_tot)$bic,2)
```


-   Selection by R² adj : stable from `r which.max(adjr2_AB_tot)` variables
```{r, echo=FALSE, fig.align='center'}
plot(results_AB_tot, scale = "adjr2",main='R² adj criteria')
```

-   Selection by Cp : stable from `r which.min(cp_AB_tot)` variables
```{r, echo=FALSE, fig.align='center'}
plot(results_AB_tot, scale = "Cp",main='Mallows Cp criteria')
```

-   Selection by BIC : stable from `r which.min(bic_AB_tot)` variables
```{r, echo=FALSE, fig.align='center'}
plot(results_AB_tot, scale = "bic",main="BIC criteria")
```

```{r}
nbr_AB_tot= max(which.max(rsq_AB_tot),which.max(adjr2_AB_tot),which.min(cp_AB_tot),which.min(bic_AB_tot))
best_var_AB_tot=coefficients(results_AB_tot, id = nbr_AB_tot)
best_var_AB_tot=names(best_var_AB_tot)[-1]
```
-   Les `r length(best_var_AB_tot)` meilleurs variables sont: **`r best_var_AB_tot`**



## Selection for total biomass

```{r biomass selection, echo=TRUE, fig.align='center'}
# names(data_lm)
supp = c("ID","Programme","Annee","ID_Site", "Protocole","AB_tot", "Richesse_tot","clcm_lvl3")
df_BM_tot= data_lm[, setdiff(names(data_lm), supp)]
df_BM_tot= df_BM_tot[, setdiff(names(df_BM_tot), clc3_col)]
df_BM_tot=drop_na(df_BM_tot)
# str(df_BM_tot)
# colSums(is.na(df_BM_tot))
results_BM_tot <- regsubsets(BM_tot ~ ., data = df_BM_tot,method = "exhaustive",nvmax =17 )
# summary(results_BM_tot)
rsq_BM_tot= round (summary(results_BM_tot)$rsq,2)
adjr2_BM_tot= round(summary(results_BM_tot)$adjr2,2)
cp_BM_tot=round (summary(results_BM_tot)$cp,2)
bic_BM_tot=round(summary(results_BM_tot)$bic,2)
```

-   Suppression de `r sum(is.na(data_lm$BM_tot))` lignes de NA de BM_tot (nrow = `r nrow(df_BM_tot)`)

-   Selection by R² adj : stable from `r which.max(adjr2_BM_tot)` variables
```{r, echo=FALSE, fig.align='center'}
plot(results_BM_tot, scale = "adjr2",main='R² adj criteria')
```

-   Selection by Cp : stable from `r which.min(cp_BM_tot)` variables
```{r, echo=FALSE, fig.align='center'}
plot(results_BM_tot, scale = "Cp",main='Mallows Cp criteria')
```

-   Selection by BIC : stable from `r which.min(bic_BM_tot)` variables
```{r, echo=FALSE, fig.align='center'}
plot(results_BM_tot, scale = "bic",main="BIC criteria")
```

```{r}
nbr_BM_tot= max(which.max(rsq_BM_tot),which.max(adjr2_BM_tot),which.min(cp_BM_tot),which.min(bic_BM_tot))
best_var_BM_tot=coefficients(results_BM_tot, id = nbr_BM_tot)
best_var_BM_tot=names(best_var_BM_tot)[-1]
```
-   Les `r length(best_var_BM_tot)` meilleurs variables sont: **`r best_var_BM_tot`**



## Selection for total taxonomic richness

```{r richness selection, echo=TRUE, fig.align='center'}
# names(data_lm)
supp = c("ID","Programme","Annee","ID_Site", "Protocole","BM_tot", "AB_tot","clcm_lvl3")
df_Richesse_tot= data_lm[, setdiff(names(data_lm), supp)]
df_Richesse_tot= df_Richesse_tot[, setdiff(names(df_Richesse_tot), clc3_col)]
# str(df_Richesse_tot)
# colSums(is.na(df_Richesse_tot))
results_Richesse_tot <- regsubsets(Richesse_tot ~ ., data = df_Richesse_tot,method = "exhaustive",nvmax =17 )
# summary(results_Richesse_tot)
rsq_Richesse_tot= round (summary(results_Richesse_tot)$rsq,2)
adjr2_Richesse_tot= round(summary(results_Richesse_tot)$adjr2,2)
cp_Richesse_tot=round (summary(results_Richesse_tot)$cp,2)
bic_Richesse_tot=round(summary(results_Richesse_tot)$bic,2)
```


-   Selection by R² adj : stable from `r which.max(adjr2_Richesse_tot)` variables
```{r, echo=FALSE, fig.align='center'}
plot(results_Richesse_tot, scale = "adjr2",main='R² adj criteria')
```

-   Selection by Cp : stable from `r which.min(cp_Richesse_tot)` variables
```{r, echo=FALSE, fig.align='center'}
plot(results_Richesse_tot, scale = "Cp",main='Mallows Cp criteria')
```

-   Selection by BIC : stable from `r which.min(bic_Richesse_tot)` variables
```{r, echo=FALSE, fig.align='center'}
plot(results_Richesse_tot, scale = "bic",main="BIC criteria")
```

```{r}
nbr_Richesse_tot= max(which.max(rsq_Richesse_tot),which.max(adjr2_Richesse_tot),which.min(cp_Richesse_tot),which.min(bic_Richesse_tot))
best_var_Richesse_tot=coefficients(results_Richesse_tot, id = nbr_Richesse_tot)
best_var_Richesse_tot=names(best_var_Richesse_tot)[-1]
```
-   Les `r length(best_var_Richesse_tot)` meilleurs variables sont: **`r best_var_Richesse_tot`**


<!--
## Summary
```{r synth selection}
all_var=unique(c(best_var_AB_tot,best_var_BM_tot,best_var_Richesse_tot))

col_max=max(c(length(best_var_AB_tot),length(best_var_BM_tot),length(best_var_Richesse_tot)))


# Colonnes communes
colonnes_communes <- intersect(intersect(best_var_AB_tot, best_var_BM_tot), best_var_Richesse_tot)

resultat_tableau <- data.frame(
  
  Var_AB_tot =c(best_var_AB_tot, rep(" ", col_max - length(best_var_AB_tot))),
  
  Var_BM_tot =  c(best_var_BM_tot, rep(" ", col_max - length(best_var_BM_tot))),
  
  Var_Richesse_tot =  c(best_var_Richesse_tot, rep(" ", col_max -    length(best_var_Richesse_tot))),
  
  Communes =c(colonnes_communes, rep(" ", col_max - length(colonnes_communes)))
)
kable(resultat_tableau,padding=10)

# write.csv2(resultat_tableau,"Results/selct_var_reg.csv")

```
**bio1** = mean annual air temperature

**bio3** = isothermality

**bio8** = mean air temperatures of the wettest quarter

**bio8** = mean air temperatures of the wettest quarter

**bio12** = annual precipitation amount

**bio15** = precipitation seasonality

**hurs_mean** = Mean monthly near-surface relative humidity


-->








# Selection des variables pour RF, GBM et ANN

## Importance of variables for total abundance
```{r}
ForetAlea <- function(var_rep, df_app, df_valid,mtry =9,ntree= 2000,maxnodes = NULL){
  
  set.seed(1234)
  col_posi <- which(names(df_app) == var_rep)
  ForeVDT <- randomForest::randomForest(df_app[-col_posi], df_app[[col_posi]], mtry = mtry, ntree = ntree, maxnodes = maxnodes)
  
  # Prediction on the validation dataset
  col_posi <- which(names(df_valid) == var_rep)
  pred.RF <- predict(ForeVDT, newdata = df_valid[, -col_posi])
  
  # Calculate RMSE to evaluate model quality
  rmse <- sqrt(mean((df_valid[, col_posi] - pred.RF)^2))
  
  
  # Calcul du R² ajusté pour train
  R_adj_train <- calcule_R2(df_app[,var_rep],  predict(ForeVDT, data=df_app))
  n_train <- nrow(df_app)
  p_train <- ncol(df_app) - 1
  r_adj_train <- 1 - ((1 - R_adj_train) * (n_train - 1) / (n_train - p_train - 1))
  
  # Calcul du R² ajusté pour test
  R_adj_test <-calcule_R2(df_valid[,col_posi],pred.RF)
  n_test <- nrow(df_valid)
  p_test <- ncol(df_valid) - 1
  r_adj_test <- 1 - ((1 - R_adj_test) * (n_test - 1) / (n_test - p_test - 1))
  
  
  # Calculate MAE
  MAE <- mean(abs(pred.RF - df_valid[, col_posi]))
  
  # Round results
  rmse <- round(rmse, 2)
  r_adj_train <- round(r_adj_train, 2)
  r_adj_test <- round(r_adj_test, 2)
  MAE <- round(MAE, 2)
  
  results <- list(RMSE = rmse, R_adj_train = r_adj_train, R_adj_test = r_adj_test, MAE = MAE, model = ForeVDT, predit = pred.RF)
  
  return(results)
}

Predictors_f = c("CaCO3" ,"gps_x" ,"N" ,"bio3" ,"gps_y" ,"clay" ,
                 "silt" ,"clcm_lvl3" ,"bio4" ,"bio16" ,"PET" ,"P" ,"bio12" ,"bio1")
```

```{r imp abundance}
# names(data_deep)
df_explo_AB_tot = data_deep[,c("AB_tot","clcm_lvl3",predicteurs )]
df_explo_AB_tot = drop_na(df_explo_AB_tot)
# summary(df_explo_AB_tot)

set.seed(1234)  # Pour rendre les résultats reproductibles
index <- createDataPartition(df_explo_AB_tot$clcm_lvl3, p = 0.8, list = FALSE)

# Séparer les données en ensembles d'entraînement et de test
df_train_AB_tot <- df_explo_AB_tot[index, ]  # Données d'entraînement
df_test_AB_tot <- df_explo_AB_tot[-index, ]  # Données de test
df_train_AB_tot = droplevels(df_train_AB_tot)
df_test_AB_tot = droplevels(df_test_AB_tot)


all_var_AB_tot=ForetAlea(var_rep ="AB_tot", df_app=df_train_AB_tot, df_valid = df_test_AB_tot,mtry =9,ntree= 2000,maxnodes = NULL)

# all_var_AB_tot$RMSE
# all_var_AB_tot$R_adj_train
# all_var_AB_tot$R_adj_test
# AB_tot_fit_rf = all_var_AB_tot$model
# AB_tot_fit_rf #display fitted model
# which.min(AB_tot_fit_rf$mse) #find number of trees that produce lowest test MSE
# sqrt(AB_tot_fit_rf$mse[which.min(AB_tot_fit_rf$mse)]) #find RMSE of best model
# varImpPlot(AB_tot_fit_rf) #produce variable importance plot

# Obtention de l'importance des variables
importance_rf <- as.data.frame(importance(all_var_AB_tot$model))
importance_rf$nom=rownames(importance_rf)
importance_rf <- importance_rf[order(importance_rf$IncNodePurity,decreasing = TRUE), ]

best_20_AB_tot = c(importance_rf$nom[1:20])
var_a_sup_AB_tot= setdiff(y = best_20_AB_tot,x = importance_rf$nom)




best20_var_AB_tot=ForetAlea(var_rep ="AB_tot", df_app=df_train_AB_tot[,c("AB_tot",best_20_AB_tot)], df_valid = df_test_AB_tot [,c("AB_tot",best_20_AB_tot)],mtry =9,ntree= 2000,maxnodes = NULL)

# best20_var_AB_tot$RMSE # 5.4
# best20_var_AB_tot$R_adj_train # 0.37
# best20_var_AB_tot$R_adj_test # 0.36
# best20_var_AB_tot$MAE
# rf.model = best20_var_AB_tot$model
# rf.model # 36.74, 36.73



# varImpPlot(best20_var_AB_tot$model, main = "Abundance") #produce variable importance plot
importance_rf <- as.data.frame(importance(best20_var_AB_tot$model))
importance_rf$nom=rownames(importance_rf)
importance_rf <- importance_rf[order(importance_rf$IncNodePurity,decreasing = TRUE), ]
row.names(importance_rf)=NULL
importance_rf$percent <- importance_rf$IncNodePurity / sum(importance_rf$IncNodePurity)*100
barplot(importance_rf$percent, main = "Importance of variables for total abundance", names.arg = importance_rf$nom, ylab = "Importance (%)", las = 2)






### sans OS


best20_var_AB_tot=ForetAlea(var_rep ="AB_tot", df_app=df_train_AB_tot[,c("AB_tot",best_20_AB_tot[-1])], df_valid = df_test_AB_tot [,c("AB_tot",best_20_AB_tot[-1])],mtry =9,ntree= 2000)

# varImpPlot(best20_var_AB_tot$model, main = "Abundance") #produce variable importance plot

importance_rf <- as.data.frame(importance(best20_var_AB_tot$model))
importance_rf$nom=rownames(importance_rf)
importance_rf <- importance_rf[order(importance_rf$IncNodePurity,decreasing = TRUE), ]
row.names(importance_rf)=NULL
importance_rf$percent <- importance_rf$IncNodePurity / sum(importance_rf$IncNodePurity)*100
barplot(importance_rf$percent, main = "Importance of variables for total abundance \n Without land cover", names.arg = importance_rf$nom, ylab = "Importance (%)", las = 2)

```



## Importance of variables for total biomass
```{r imp biomass}
# names(data_deep)
df_explo_BM_tot = data_deep[,c("BM_tot","clcm_lvl3",predicteurs )]
df_explo_BM_tot = drop_na(df_explo_BM_tot)


set.seed(1234)  # Pour rendre les résultats reproductibles
index <- createDataPartition(df_explo_BM_tot$clcm_lvl3, p = 0.8, list = FALSE)

# Séparer les données en ensembles d'entraînement et de test
df_train_BM_tot <- df_explo_BM_tot[index, ]  # Données d'entraînement
df_test_BM_tot <- df_explo_BM_tot[-index, ]  # Données de test
df_train_BM_tot = droplevels(df_train_BM_tot)
df_test_BM_tot = droplevels(df_test_BM_tot)


all_var_BM_tot= ForetAlea(var_rep ="BM_tot", df_app=df_train_BM_tot, df_valid = df_test_BM_tot,mtry =9,ntree= 2000)
BM_tot_fit_rf = all_var_BM_tot$model


# # Etape 5 importance des variables
# BM_tot_fit_rf #display fitted model
# which.min(BM_tot_fit_rf$mse) #find number of trees that produce lowest test MSE
# sqrt(BM_tot_fit_rf$mse[which.min(BM_tot_fit_rf$mse)]) #find RMSE of best model
# sqrt(mean(BM_tot_fit_rf$mse))
# mean(100*BM_tot_fit_rf$rsq)
# varImpPlot(BM_tot_fit_rf) #produce variable importance plot

# Obtention de l'importance des variables
importance_rf <- as.data.frame(importance(BM_tot_fit_rf))
importance_rf$nom=rownames(importance_rf)
importance_rf <- importance_rf[order(importance_rf$IncNodePurity,decreasing = TRUE), ]
best_20_BM_tot = c(importance_rf$nom[1:20])



var_a_sup_BM_tot= setdiff(y = best_20_BM_tot,x = importance_rf$nom)

best20_var_BM_tot= ForetAlea(var_rep ="BM_tot", df_app=df_train_BM_tot[, c("BM_tot",best_20_BM_tot)], df_valid = df_test_BM_tot [, c("BM_tot",best_20_BM_tot)],mtry =9,ntree= 2000)
# varImpPlot(best20_var_BM_tot$model, main = "Biomass") #produce variable importance plot


# library(rms) 
# test_metric = lrm(df_test_BM_tot$BM_tot  ~ best20_var_BM_tot$predit, x= TRUE, y = TRUE)
# test_metric$stats

# partialPlot(best20_var_BM_tot$model,  df_train_BM_tot[, c("BM_tot",best_20_BM_tot)], "gps_x", plot = TRUE)


importance_rf <- as.data.frame(importance(best20_var_BM_tot$model))
importance_rf$nom=rownames(importance_rf)
importance_rf <- importance_rf[order(importance_rf$IncNodePurity,decreasing = TRUE), ]
row.names(importance_rf)=NULL
importance_rf$percent <- importance_rf$IncNodePurity / sum(importance_rf$IncNodePurity)*100
barplot(importance_rf$percent, main = "Importance of variables for total biomass", names.arg = importance_rf$nom, ylab = "Importance (%)", las = 2)




### sans OS
best20_var_BM_tot= ForetAlea(var_rep ="BM_tot", df_app=df_train_BM_tot[, c("BM_tot",best_20_BM_tot[-1])], df_valid = df_test_BM_tot [, c("BM_tot",best_20_BM_tot[-1])],mtry =9,ntree= 2000)
# varImpPlot(best20_var_BM_tot$model, main = "Biomass") #produce variable importance plot


importance_rf <- as.data.frame(importance(best20_var_BM_tot$model))
importance_rf$nom=rownames(importance_rf)
importance_rf <- importance_rf[order(importance_rf$IncNodePurity,decreasing = TRUE), ]
row.names(importance_rf)=NULL
importance_rf$percent <- importance_rf$IncNodePurity / sum(importance_rf$IncNodePurity)*100
barplot(importance_rf$percent, main = "Importance of variables for total biomass \n Without land cover", names.arg = importance_rf$nom, ylab = "Importance (%)", las = 2)


```


## Importance of variables for total taxonomic richness

```{r imp richness}
# names(data_mod)
df_explo_Richesse_tot = data_deep[,c("Richesse_tot","clcm_lvl3",predicteurs )]
df_explo_Richesse_tot = drop_na(df_explo_Richesse_tot)


set.seed(1234)  # Pour rendre les résultats reproductibles
index <- createDataPartition(df_explo_Richesse_tot$clcm_lvl3, p = 0.8, list = FALSE)

# Séparer les données en ensembles d'entraînement et de test
df_train_Richesse_tot <- df_explo_Richesse_tot[index, ]  # Données d'entraînement
df_test_Richesse_tot <- df_explo_Richesse_tot[-index, ]  # Données de test
df_train_Richesse_tot = droplevels(df_train_Richesse_tot)
df_test_Richesse_tot = droplevels(df_test_Richesse_tot)



all_var_Richesse_tot= ForetAlea(var_rep ="Richesse_tot", df_app=df_train_Richesse_tot, df_valid = df_test_Richesse_tot,mtry =9,ntree= 2000)
Richesse_tot_fit_rf = all_var_Richesse_tot$model



# Richesse_tot_fit_rf #display fitted model
# which.min(Richesse_tot_fit_rf$mse) #find number of trees that produce lowest test MSE
# sqrt(Richesse_tot_fit_rf$mse[which.min(Richesse_tot_fit_rf$mse)]) #find RMSE of best model
# sqrt(mean(Richesse_tot_fit_rf$mse))
# mean(100*Richesse_tot_fit_rf$rsq)
# varImpPlot(Richesse_tot_fit_rf) #produce variable importance plot

# Obtention de l'importance des variables
importance_rf <- as.data.frame(importance(Richesse_tot_fit_rf))
importance_rf$nom=rownames(importance_rf)
importance_rf <- importance_rf[order(importance_rf$IncNodePurity,decreasing = TRUE), ]

best_20_Richesse_tot = c(importance_rf$nom[1:20])
var_a_sup_Richesse_tot= setdiff(y = best_20_Richesse_tot,x = importance_rf$nom)


best20_var_Richesse_tot= ForetAlea(var_rep ="Richesse_tot", df_app=df_train_Richesse_tot[, c("Richesse_tot",best_20_Richesse_tot)], df_valid = df_test_Richesse_tot [, c("Richesse_tot",best_20_Richesse_tot)],mtry =9,ntree= 2000)


# varImpPlot(best20_var_Richesse_tot$model,main = "Richness") #produce variable importance plot

importance_rf <- as.data.frame(importance(best20_var_Richesse_tot$model))
importance_rf$nom=rownames(importance_rf)
importance_rf <- importance_rf[order(importance_rf$IncNodePurity,decreasing = TRUE), ]
row.names(importance_rf)=NULL
importance_rf$percent <- importance_rf$IncNodePurity / sum(importance_rf$IncNodePurity)*100
barplot(importance_rf$percent, main = "Importance of variables for richness", names.arg = importance_rf$nom, ylab = "Importance (%)", las = 2)


### sans OS

best20_var_Richesse_tot= ForetAlea(var_rep ="Richesse_tot", df_app=df_train_Richesse_tot[, c("Richesse_tot",best_20_Richesse_tot[-1])], df_valid = df_test_Richesse_tot [, c("Richesse_tot",best_20_Richesse_tot[-1])],mtry =9,ntree= 2000)


# varImpPlot(best20_var_Richesse_tot$model,main = "Richness") #produce variable importance plot

importance_rf <- as.data.frame(importance(best20_var_Richesse_tot$model))
importance_rf$nom=rownames(importance_rf)
importance_rf <- importance_rf[order(importance_rf$IncNodePurity,decreasing = TRUE), ]
row.names(importance_rf)=NULL
importance_rf$percent <- importance_rf$IncNodePurity / sum(importance_rf$IncNodePurity)*100
barplot(importance_rf$percent, main = "Importance of variables for richness \n Without land cover", names.arg = importance_rf$nom, ylab = "Importance (%)", las = 2)

```

# Relationship between variables
[Explanatory power of variables]

## Abundance
```{r re AB_tot,fig.align='center',fig.show='animate'}
df_re_AB_tot= data_deep[,c("AB_tot",best_20_AB_tot)]
for (i in names(df_re_AB_tot[,-1])){
  plot(df_re_AB_tot[,i],df_re_AB_tot[,1], main = paste("AB_tot &",i), xlab = i,ylab="Abundance" )
   # col_posi <- which(names(df_re_AB_tot) == i)
   # g =ggpairs(df_re_AB_tot[,c(1,col_posi)])
   # print(g)
}
```
-   Plots
```{r re AB_tot2,fig.align='center'}
df_re_AB_tot= data_deep[,c("AB_tot",best_20_AB_tot)]
for (i in names(df_re_AB_tot[,-1])){
  # plot(df_re_AB_tot[,i],df_re_AB_tot[,1], main = paste("AB_tot &",i), xlab = i,ylab="Abundance" )
  cat(paste("- Abundance &",i,"\n"))
   col_posi <- which(names(df_re_AB_tot) == i)
   g =ggpairs(df_re_AB_tot[,c(1,col_posi)])
   print(g)
}

```

## Biomass
```{r re BM_tot,fig.align='center',fig.show='animate'}
df_re_BM_tot= data_deep[,c("BM_tot",best_20_BM_tot)]
for (i in names(df_re_BM_tot[,-1])){
  plot(df_re_BM_tot[,i],df_re_BM_tot[,1], main = paste("BM_tot &",i), xlab = i,ylab="Biomass" )
   # col_posi <- which(names(df_re_BM_tot) == i)
   # g =ggpairs(df_re_BM_tot[,c(1,col_posi)])
   # print(g)
}
```
-   Plots
```{r re BM_tot2,fig.align='center'}
df_re_BM_tot= data_deep[,c("BM_tot",best_20_BM_tot)]
for (i in names(df_re_BM_tot[,-1])){
  # plot(df_re_BM_tot[,i],df_re_BM_tot[,1], main = paste("BM_tot &",i), xlab = i,ylab="Biomass" )
  cat(paste("- Biomass &",i,"\n"))
   col_posi <- which(names(df_re_BM_tot) == i)
   g =ggpairs(df_re_BM_tot[,c(1,col_posi)])
   print(g)
}

```

## Richness
```{r re Richesse_tot,fig.align='center',fig.show='animate'}
df_re_Richesse_tot= data_deep[,c("Richesse_tot",best_20_Richesse_tot)]
for (i in names(df_re_Richesse_tot[,-1])){
  plot(df_re_Richesse_tot[,i],df_re_Richesse_tot[,1], main = paste("Richesse_tot &",i), xlab = i,ylab="Richness" )
   # col_posi <- which(names(df_re_Richesse_tot) == i)
   # g =ggpairs(df_re_Richesse_tot[,c(1,col_posi)])
   # print(g)
}
```
-   Plots
```{r re Richesse_tot2,fig.align='center'}
df_re_Richesse_tot= data_deep[,c("Richesse_tot",best_20_Richesse_tot)]
for (i in names(df_re_Richesse_tot[,-1])){
  # plot(df_re_Richesse_tot[,i],df_re_Richesse_tot[,1], main = paste("Richesse_tot &",i), xlab = i,ylab="Richness" )
  cat(paste("- Richness &",i,"\n"))
   col_posi <- which(names(df_re_Richesse_tot) == i)
   g =ggpairs(df_re_Richesse_tot[,c(1,col_posi)])
   print(g)
}

```

# Species


See [species explorations](https://posit.cloud/content/7997063){target="_blank"}


# ACP

[Plan]

## Abundance

```{r ACP abundance, echo=TRUE, fig.align='center'}

df_acp_AB_tot = data_deep[,c("AB_tot",best_20_AB_tot)]

idf = c("OPVT_IDF","TRAMBIOSOIL")
bzh = c("Dephy Bio","Dephy", "Breizh Sukr", "Sols de Bretagne","Kerguéhennec")
bfc=c("TIGA")
df_region = data_deep
df_region$regions <- ifelse(df_region$Programme %in% idf, "idf",
                            ifelse(df_region$Programme %in% bzh, "bzh",
                                   ifelse(df_region$Programme %in% bfc, "bfc",
                                          "autres")))
df_acp_AB_tot = df_region[,c("AB_tot",best_20_AB_tot,"regions")]
# # # colnames(df_acp_AB_tot)[colnames(df_acp_AB_tot) == "clcm_lvl3"] <- "clc3"
# dummy_vars <- model.matrix(~ clcm_lvl3 - 1, data = df_acp_AB_tot)
# df_acp_AB_tot <- cbind(df_acp_AB_tot, dummy_vars)
# df_acp_AB_tot <- df_acp_AB_tot[, -which(names(df_acp_AB_tot) == "clcm_lvl3")]

df_acp_AB_tot = drop_na(df_acp_AB_tot)
df_acp_AB_tot = droplevels(df_acp_AB_tot)


acp_AB_tot <- PCA(df_acp_AB_tot, graph = FALSE,quanti.sup=1,quali.sup=c(2,22))

## Choix du nombre d'axes
# acp_AB_tot$eig
fviz_eig(acp_AB_tot, addlabels = TRUE) # on prend les trois premiers axes
contrib_axes <- acp_AB_tot$var$contrib[, 1:2]  # 3 premiers axes
contrib_axes <- round(contrib_axes, 2)   # Plus facile a lire
fviz_contrib(acp_AB_tot, choice = "var", axes = 1)
fviz_contrib(acp_AB_tot, choice = "var", axes = 2)
# fviz_contrib(acp_AB_tot, choice = "var", axes = 3)


# coloree les variables selon leurs contributions aux axes
fviz_pca_var(
  axes = c(1, 2),
  acp_AB_tot,
  col.var = "contrib",
  gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
  repel = TRUE
) # evite le chevauchement de texte


seuil <- 1 / ncol(df_acp_AB_tot[,-c(1:2)]) * 100
lignes_superieures <- rownames(contrib_axes)[apply(contrib_axes, 1,
                                                   function(x)
                                                     any(x >= seuil))]

#  En consderant les deux premiers axes, les predicteurs les moins importants sont:
names(df_acp_AB_tot[,-c(1:2)]) [! names(df_acp_AB_tot[,-c(1:2)]) %in% lignes_superieures]





coul = c("yellow", "green", "violet", "blue", "black", "red")
coul <- c("077255000", "255166255", "204242077", "255255168", "230230077", "230128000")

# Convertir les codes de couleur hexadécimaux en couleurs RGB
colors <- sapply(coul, function(hex) {
  r <- as.numeric(substr(hex, 1, 3)) / 255
  g <- as.numeric(substr(hex, 4, 6)) / 255
  b <- as.numeric(substr(hex, 7, 9)) / 255
  rgb(r, g, b)
})
colors = unname(colors)


fviz_pca_ind(
  axes = c(1, 2),
  acp_AB_tot,
  geom.ind = "point",
  pointshape = 21,
  pointsize = 2,
  palette = "viridus",
  # addEllipses = TRUE,
  legend.title = "Abundance",
  fill.ind = df_acp_AB_tot$AB_tot^2
)



fviz_pca_ind(
  axes = c(1, 2),
  acp_AB_tot,
  geom.ind = "point",
  pointshape = 21,
  pointsize = 2,
  palette = colors,
  addEllipses = TRUE,
  legend.title = "Land use",
  fill.ind = df_acp_AB_tot$clcm_lvl3
  )


# Graphique biplot
fviz_pca_biplot(
  axes = c(1, 2),
  acp_AB_tot,
  geom.ind = "point",
  pointshape = 21,
  pointsize = 2,
  palette = colors,
  addEllipses = TRUE,
  legend.title = "Land use",
  fill.ind = df_acp_AB_tot$clcm_lvl3
  )




# Tracez les individus avec les variables qualitatives supplémentaires
fviz_pca_ind(acp_AB_tot, 
              habillage = 22, 
             geom.ind = "point",
              palette = "jco",        # Choisissez une palette de couleurs
              addEllipses = TRUE,     # Ajoutez des ellipses de confiance
              ellipse.type = "t",     # Type d'ellipse
              legend.title = "~ Regions"
              )



```


```{r hcp abundance}
#Classification
# class1 = HCPC(acp_AB_tot)
# plot(class1$call$t$inert.gain, type = "s")
#Je choisis finalement 4 groupes
class1 = HCPC(acp_AB_tot, nb.clust = 3)
# fviz_dend(class1)
# fviz_cluster(class1)
# fviz_cluster(class1, ellipse.type = "norm", ellipse.level = 0.8)    #Groupes
```



## Biomass

```{r ACP Biomass, echo=TRUE, fig.align='center'}

df_acp_BM_tot = data_deep[,c("BM_tot",best_20_BM_tot)]
# # # colnames(df_acp_BM_tot)[colnames(df_acp_BM_tot) == "clcm_lvl3"] <- "clc3"
# dummy_vars <- model.matrix(~ clcm_lvl3 - 1, data = df_acp_BM_tot)
# df_acp_BM_tot <- cbind(df_acp_BM_tot, dummy_vars)
# df_acp_BM_tot <- df_acp_BM_tot[, -which(names(df_acp_BM_tot) == "clcm_lvl3")]

df_acp_BM_tot = drop_na(df_acp_BM_tot)
df_acp_BM_tot = droplevels(df_acp_BM_tot)


acp_BM_tot <- PCA(df_acp_BM_tot, graph = FALSE,quanti.sup=1,quali.sup=2)

## Choix du nombre d'axes
# acp_BM_tot$eig
fviz_eig(acp_BM_tot, addlabels = TRUE) # on prend les trois premiers axes
contrib_axes <- acp_BM_tot$var$contrib[, 1:2]  # 3 premiers axes
contrib_axes <- round(contrib_axes, 2)   # Plus facile a lire
fviz_contrib(acp_BM_tot, choice = "var", axes = 1)
fviz_contrib(acp_BM_tot, choice = "var", axes = 2)
# fviz_contrib(acp_BM_tot, choice = "var", axes = 3)


# coloree les variables selon leurs contributions aux axes
fviz_pca_var(
  axes = c(1, 2),
  acp_BM_tot,
  col.var = "contrib",
  gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
  repel = TRUE
) # evite le chevauchement de texte


seuil <- 1 / ncol(df_acp_BM_tot[,-c(1:2)]) * 100
lignes_superieures <- rownames(contrib_axes)[apply(contrib_axes, 1,
                                                   function(x)
                                                     any(x >= seuil))]

#  En consderant les deux premiers axes, les predicteurs les moins importants sont:
names(df_acp_BM_tot[,-c(1:2)]) [! names(df_acp_BM_tot[,-c(1:2)]) %in% lignes_superieures]





coul = c("yellow", "green", "violet", "blue", "black", "red")
coul <- c("077255000", "255166255", "204242077", "255255168", "230230077", "230128000")

# Convertir les codes de couleur hexadécimaux en couleurs RGB
colors <- sapply(coul, function(hex) {
  r <- as.numeric(substr(hex, 1, 3)) / 255
  g <- as.numeric(substr(hex, 4, 6)) / 255
  b <- as.numeric(substr(hex, 7, 9)) / 255
  rgb(r, g, b)
})
colors = unname(colors)


fviz_pca_ind(
  axes = c(1, 2),
  acp_BM_tot,
  geom.ind = "point",
  pointshape = 21,
  pointsize = 2,
  palette = "viridus",
  # addEllipses = TRUE,
  legend.title = "Biomass",
  fill.ind = df_acp_BM_tot$BM_tot^2
)



fviz_pca_ind(
  axes = c(1, 2),
  acp_BM_tot,
  geom.ind = "point",
  pointshape = 21,
  pointsize = 2,
  palette = colors,
  addEllipses = TRUE,
  legend.title = "Land use",
  fill.ind = df_acp_BM_tot$clcm_lvl3
  )


# Graphique biplot
fviz_pca_biplot(
  axes = c(1, 2),
  acp_BM_tot,
  geom.ind = "point",
  pointshape = 21,
  pointsize = 2,
  palette = colors,
  addEllipses = TRUE,
  legend.title = "Land use",
  fill.ind = df_acp_BM_tot$clcm_lvl3
  )


```


```{r hcp biomass}
#Classification
# class1 = HCPC(acp_BM_tot)
# plot(class1$call$t$inert.gain, type = "s")
#Je choisis finalement 4 groupes
class1 = HCPC(acp_BM_tot, nb.clust = 3)
# fviz_dend(class1)
# fviz_cluster(class1)
# fviz_cluster(class1, ellipse.type = "norm", ellipse.level = 0.8)    #Groupes
```





## Richness

```{r ACP Richness, echo=TRUE, fig.align='center'}

df_acp_Richesse_tot = data_deep[,c("Richesse_tot",best_20_Richesse_tot)]
# # # colnames(df_acp_Richesse_tot)[colnames(df_acp_Richesse_tot) == "clcm_lvl3"] <- "clc3"
# dummy_vars <- model.matrix(~ clcm_lvl3 - 1, data = df_acp_Richesse_tot)
# df_acp_Richesse_tot <- cbind(df_acp_Richesse_tot, dummy_vars)
# df_acp_Richesse_tot <- df_acp_Richesse_tot[, -which(names(df_acp_Richesse_tot) == "clcm_lvl3")]

df_acp_Richesse_tot = drop_na(df_acp_Richesse_tot)
df_acp_Richesse_tot = droplevels(df_acp_Richesse_tot)

posi_co = which(names(df_acp_Richesse_tot)=="clcm_lvl3")

acp_Richesse_tot <- PCA(df_acp_Richesse_tot, graph = FALSE,quanti.sup=1,quali.sup=posi_co)

## Choix du nombre d'axes
# acp_Richesse_tot$eig
fviz_eig(acp_Richesse_tot, addlabels = TRUE) # on prend les trois premiers axes
contrib_axes <- acp_Richesse_tot$var$contrib[, 1:2]  # 3 premiers axes
contrib_axes <- round(contrib_axes, 2)   # Plus facile a lire
fviz_contrib(acp_Richesse_tot, choice = "var", axes = 1)
fviz_contrib(acp_Richesse_tot, choice = "var", axes = 2)
# fviz_contrib(acp_Richesse_tot, choice = "var", axes = 3)


# coloree les variables selon leurs contributions aux axes
fviz_pca_var(
  axes = c(1, 2),
  acp_Richesse_tot,
  col.var = "contrib",
  gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
  repel = TRUE
) # evite le chevauchement de texte


seuil <- 1 / ncol(df_acp_Richesse_tot[,-c(1,posi_co)]) * 100
lignes_superieures <- rownames(contrib_axes)[apply(contrib_axes, 1,
                                                   function(x)
                                                     any(x >= seuil))]

#  En consderant les deux premiers axes, les predicteurs les moins importants sont:
names(df_acp_Richesse_tot[,-c(1,posi_co)]) [! names(df_acp_Richesse_tot[,-c(1:2)]) %in% lignes_superieures]





coul = c("yellow", "green", "violet", "blue", "black", "red")
coul <- c("077255000", "255166255", "204242077", "255255168", "230230077", "230128000")

# Convertir les codes de couleur hexadécimaux en couleurs RGB
colors <- sapply(coul, function(hex) {
  r <- as.numeric(substr(hex, 1, 3)) / 255
  g <- as.numeric(substr(hex, 4, 6)) / 255
  b <- as.numeric(substr(hex, 7, 9)) / 255
  rgb(r, g, b)
})
colors = unname(colors)


fviz_pca_ind(
  axes = c(1, 2),
  acp_Richesse_tot,
  geom.ind = "point",
  pointshape = 21,
  pointsize = 2,
  palette = "viridus",
  # addEllipses = TRUE,
  legend.title = "Richness",
  fill.ind = df_acp_Richesse_tot$Richesse_tot
)



fviz_pca_ind(
  axes = c(1, 2),
  acp_Richesse_tot,
  geom.ind = "point",
  pointshape = 21,
  pointsize = 2,
  palette = colors,
  addEllipses = TRUE,
  legend.title = "Land use",
  fill.ind = df_acp_Richesse_tot$clcm_lvl3
  )


# Graphique biplot
fviz_pca_biplot(
  axes = c(1, 2),
  acp_Richesse_tot,
  geom.ind = "point",
  pointshape = 21,
  pointsize = 2,
  palette = colors,
  addEllipses = TRUE,
  legend.title = "Land use",
  fill.ind = df_acp_Richesse_tot$clcm_lvl3
  )


```


```{r hcp richness}
#Classification
# class1 = HCPC(acp_Richesse_tot)
# plot(class1$call$t$inert.gain, type = "s")
#Je choisis finalement 4 groupes
class1 = HCPC(acp_Richesse_tot, nb.clust = 4)
# fviz_dend(class1)
# fviz_cluster(class1)
# fviz_cluster(class1, ellipse.type = "norm", ellipse.level = 0.8)    #Groupes
```



# Modeling
## Data preparation
```{r modeling AB_tot, echo=TRUE}
# AB_tot --------------------------------------------------------------------------
# df_mod_AB_tot = data_deep[,c("AB_tot",best_20_AB_tot)]
# # # # colnames(df_mod_AB_tot)[colnames(df_mod_AB_tot) == "clcm_lvl3"] <- "clc3"
# dummy_vars <- model.matrix(~ clcm_lvl3 - 1, data = df_mod_AB_tot)
# df_mod_AB_tot <- cbind(df_mod_AB_tot, dummy_vars)
# df_mod_AB_tot <- df_mod_AB_tot[, -which(names(df_mod_AB_tot) == "clcm_lvl3")]
# 
# df_mod_AB_tot = drop_na(df_mod_AB_tot)
# df_mod_AB_tot = droplevels(df_mod_AB_tot)
# 
# 
# # Partition
# set.seed(1234)
# ind <- sample(2, nrow(df_mod_AB_tot), replace = T, prob = c(.8, .2))
# AB_tot_train <- df_mod_AB_tot[ind==1,]
# AB_tot_test <- df_mod_AB_tot[ind==2,]
# 
# write.csv2(x =AB_tot_train,file = "datas/AB_tot_train.csv", row.names = FALSE)
# write.csv2(x =AB_tot_test,file = "datas/AB_tot_test.csv", row.names = FALSE)

AB_tot_train = read.csv2("datas/AB_tot_train.csv")
AB_tot_test = read.csv2("datas/AB_tot_test.csv")
df_mod_AB_tot = rbind(AB_tot_train,AB_tot_test)

AB_tot_train = as.data.frame(AB_tot_train)
AB_tot_test = as.data.frame(AB_tot_test)

# df <- data.frame(y =AB_tot_train[,"AB_tot"])
# abundance_dist_train = ggplot(df, aes(x=y)) +
#   geom_histogram(aes(y=..density..), fill="#69b3a2", color="#e9ecef", bins=30, alpha=2) +
#   geom_density(fill="black", alpha=0.2) +
#   theme_gray() +
#   labs(title="Abundance: Train", x="Value", y="Density") +
#   theme(plot.title = element_text(hjust = 0.5))
# ggsave("Results/abundance_dist_train.png", plot = abundance_dist_train, dpi = 300,width = 3,height = 2)
# 
# df <- data.frame(y =AB_tot_test[,"AB_tot"])
# abundance_dist_test = ggplot(df, aes(x=y)) +
#   geom_histogram(aes(y=..density..), fill="#69b3a2", color="#e9ecef", bins=30, alpha=2) +
#   geom_density(fill="black", alpha=0.2) +
#   theme_gray() +
#   labs(title="Abundance: Test", x="Value", y="Density") +
#   theme(plot.title = element_text(hjust = 0.5))
# ggsave("Results/abundance_dist_test.png", plot = abundance_dist_test, dpi = 300,width = 3,height = 2)
# 
# # Distrvitbution de var rep dans train et de test: est ce homogene ?
# abundance_dist_train_and_test = ggarrange(abundance_dist_train, abundance_dist_test,
#                           labels = c('(a)', '(b)'),
#                           common.legend = TRUE,
#                           legend = 'right'
# )
# 
# ggsave("Results/abundance_dist_train_and_test.png", plot = abundance_dist_train_and_test, dpi = 300,height = 2,width = 4)

```


```{r modeling BM_tot}
# BM_tot --------------------------------------------------------------------------
# df_mod_BM_tot = data_deep[,c("BM_tot",best_20_BM_tot)]
# # # # colnames(df_mod_BM_tot)[colnames(df_mod_BM_tot) == "clcm_lvl3"] <- "clc3"
# dummy_vars <- model.matrix(~ clcm_lvl3 - 1, data = df_mod_BM_tot)
# df_mod_BM_tot <- cbind(df_mod_BM_tot, dummy_vars)
# df_mod_BM_tot <- df_mod_BM_tot[, -which(names(df_mod_BM_tot) == "clcm_lvl3")]
# 
# df_mod_BM_tot = drop_na(df_mod_BM_tot)
# df_mod_BM_tot = droplevels(df_mod_BM_tot)
# 
# # Partition
# set.seed(1234)
# ind <- sample(2, nrow(df_mod_BM_tot), replace = T, prob = c(.8, .2))
# BM_tot_train <- df_mod_BM_tot[ind==1,]
# BM_tot_test <- df_mod_BM_tot[ind==2,]
# 
# 
# write.csv2(x =BM_tot_train,file = "datas/BM_tot_train.csv", row.names = FALSE)
# write.csv2(x =BM_tot_test,file = "datas/BM_tot_test.csv", row.names = FALSE)


BM_tot_train = read.csv2("datas/BM_tot_train.csv")
BM_tot_test = read.csv2("datas/BM_tot_test.csv")
df_mod_BM_tot = rbind(BM_tot_train,BM_tot_test)


BM_tot_train = as.data.frame(BM_tot_train)
BM_tot_test = as.data.frame(BM_tot_test)



# # Distribution de var rep
# df <- data.frame(y =BM_tot_train[,"BM_tot"])
# biomass_dist_train = ggplot(df, aes(x=y)) +
#   geom_histogram(aes(y=..density..), fill="#69b3a2", color="#e9ecef", bins=30, alpha=2) +
#   geom_density(fill="black", alpha=0.2) +
#   theme_gray() +
#   labs(title="Biomass: Train", x="Value", y="Density") +
#   theme(plot.title = element_text(hjust = 0.5))
# ggsave("Results/biomass_dist_train.png", plot = biomass_dist_train, dpi = 300,width = 3,height = 2)
# 
# df <- data.frame(y =BM_tot_test[,"BM_tot"])
# biomass_dist_test = ggplot(df, aes(x=y)) +
#   geom_histogram(aes(y=..density..), fill="#69b3a2", color="#e9ecef", bins=30, alpha=2) +
#   geom_density(fill="black", alpha=0.2) +
#   theme_gray() +
#   labs(title="Biomass: Test", x="Value", y="Density") +
#   theme(plot.title = element_text(hjust = 0.5))
# ggsave("Results/biomass_dist_test.png", plot = biomass_dist_test, dpi = 300,width = 3,height = 2)
# 
# 
# # Distrvitbution de var rep dans train et de test: est ce homogene ?
# biomass_dist_train_and_test = ggarrange(biomass_dist_train, biomass_dist_test,
#                                           labels = c('(a)', '(b)'),
#                                           common.legend = TRUE,
#                                           legend = 'right')
# 
# ggsave("Results/biomass_dist_train_and_test.png", plot = biomass_dist_train_and_test, dpi = 300 ,height = 2,width = 4)
```


```{r modeling Richesse_tot}
# Richesse_tot --------------------------------------------------------------------------
# df_mod_Richesse_tot = data_deep[,c("Richesse_tot",best_20_Richesse_tot)]
# # # # colnames(df_mod_Richesse_tot)[colnames(df_mod_Richesse_tot) == "clcm_lvl3"] <- "clc3"
# dummy_vars <- model.matrix(~ clcm_lvl3 - 1, data = df_mod_Richesse_tot)
# df_mod_Richesse_tot <- cbind(df_mod_Richesse_tot, dummy_vars)
# df_mod_Richesse_tot <- df_mod_Richesse_tot[, -which(names(df_mod_Richesse_tot) == "clcm_lvl3")]
# 
# df_mod_Richesse_tot = drop_na(df_mod_Richesse_tot)
# df_mod_Richesse_tot = droplevels(df_mod_Richesse_tot)
# 
# 
# # Partition
# set.seed(1234)
# ind <- sample(2, nrow(df_mod_Richesse_tot), replace = T, prob = c(.8, .2))
# Richesse_tot_train <- df_mod_Richesse_tot[ind==1,]
# Richesse_tot_test <- df_mod_Richesse_tot[ind==2,]
# 
# write.csv2(x =Richesse_tot_train,file = "datas/Richesse_tot_train.csv", row.names = FALSE)
# write.csv2(x =Richesse_tot_test,file = "datas/Richesse_tot_test.csv", row.names = FALSE)

Richesse_tot_train = read.csv2("datas/Richesse_tot_train.csv")
Richesse_tot_test = read.csv2("datas/Richesse_tot_test.csv")
df_mod_Richesse_tot = rbind(Richesse_tot_train,Richesse_tot_test)


Richesse_tot_train = as.data.frame(Richesse_tot_train)
Richesse_tot_test = as.data.frame(Richesse_tot_test)



# Distribution de var rep
# df <- data.frame(y =Richesse_tot_train[,"Richesse_tot"])
# richness_dist_train = ggplot(df, aes(x=y)) +
#   geom_histogram(aes(y=..density..), fill="#69b3a2", color="#e9ecef", bins=30, alpha=2) +
#   geom_density(fill="black", alpha=0.2) +
#   theme_gray() +
#   labs(title="Richness: Train", x="Value", y="Density") +
#   theme(plot.title = element_text(hjust = 0.5))
# ggsave("Results/richness_dist_train.png", plot = richness_dist_train, dpi = 300 ,width = 3,height = 2)
# 
# df <- data.frame(y =Richesse_tot_test[,"Richesse_tot"])
# richness_dist_test = ggplot(df, aes(x=y)) +
#   geom_histogram(aes(y=..density..), fill="#69b3a2", color="#e9ecef", bins=30, alpha=2) +
#   geom_density(fill="black", alpha=0.2) +
#   theme_gray() +
#   labs(title="Richness: Test", x="Value", y="Density") +
#   theme(plot.title = element_text(hjust = 0.5))
# ggsave("Results/richness_dist_test.png", plot = richness_dist_test, dpi = 300 ,width = 3,height = 2)
# 
# # Distrvitbution de var rep dans train et de test: est ce homogene ?
# richness_dist_train_and_test = ggarrange(richness_dist_train, richness_dist_test,
#                                           labels = c('(a)', '(b)'),
#                                           common.legend = TRUE,
#                                           legend = 'right')
# 
# ggsave("Results/richness_dist_train_and_test.png", plot = richness_dist_train_and_test, dpi = 300 ,height = 2,width = 4)

```


    
::: columns
::: {.column width="50%"}

**Abundance **

-   Data partition (`r dim(df_mod_AB_tot)`):

    -   train data (80 %) = `r dim(AB_tot_train) `
    
    -   test data (20 %) = `r dim(AB_tot_test) `
    


**Biomasse **

-   Data partition (`r dim(df_mod_BM_tot)`):

    -   train data (80 %) = `r dim(BM_tot_train) `
    
    -   test data (20 %) = `r dim(BM_tot_test) `
    


**Richness **

-   Data partition (`r dim(df_mod_Richesse_tot)`):

    -   train data (80 %) = `r dim(Richesse_tot_train) `
    
    -   test data (20 %) = `r dim(Richesse_tot_test) ` 

:::

::: {.column width="50%"}
<p>
  <img src="Results/abundance_dist_train_and_test.png">
</p>

<p>
  <img src="Results/biomass_dist_train_and_test.png">
</p>

<p>
  <img src="Results/richness_dist_train_and_test.png">
</p>
:::
:::



    

```{r interaction, eval=FALSE}
# AB_tot ------------------------------------------------------------------

inter_AB_tot <- select_inter("AB_tot", df_mod_AB_tot, best_20_AB_tot[-1])
inter_AB_tot <- arrange(inter_AB_tot, desc(r_squared))
best_inte_AB_tot = inter_AB_tot[1:20, "variables"] 


AB_tot_inter_train = AB_tot_train
for (interaction in best_inte_AB_tot) {
  vars <- strsplit(interaction, ".inter.")[[1]]
  # cat(vars[1], ":", vars[2], "\n")
  # la colonne d'interaction
  AB_tot_inter_train[[interaction]] <- AB_tot_inter_train[[vars[1]]] * AB_tot_inter_train[[vars[2]]]
}
# names(AB_tot_inter_train)


AB_tot_inter_test = AB_tot_test
for (interaction in best_inte_AB_tot) {
  vars <- strsplit(interaction, ".inter.")[[1]]
  # cat(vars[1], ":", vars[2], "\n")
  # la colonne d'interaction
  AB_tot_inter_test[[interaction]] <- AB_tot_inter_test[[vars[1]]] * AB_tot_inter_test[[vars[2]]]
}
# names(AB_tot_inter_test)




# BM_tot ------------------------------------------------------------------

# inter_BM_tot <- select_inter("BM_tot", df_mod_BM_tot, best_20_BM_tot[-1])
# write.csv2(x =inter_BM_tot,file = "Results/inter_BM_tot.csv", row.names = FALSE)
inter_BM_tot = read.csv2("Results/inter_BM_tot.csv")

inter_BM_tot <- arrange(inter_BM_tot, desc(r_squared))
best_inte_BM_tot = inter_BM_tot[1:20, "variables"] 


BM_tot_inter_train = BM_tot_train
for (interaction in best_inte_BM_tot) {
  vars <- strsplit(interaction, ".inter.")[[1]]
  # cat(vars[1], ":", vars[2], "\n")
  # la colonne d'interaction
  BM_tot_inter_train[[interaction]] <- BM_tot_inter_train[[vars[1]]] * BM_tot_inter_train[[vars[2]]]
}
# names(BM_tot_inter_train)


BM_tot_inter_test = BM_tot_test
for (interaction in best_inte_BM_tot) {
  vars <- strsplit(interaction, ".inter.")[[1]]
  # cat(vars[1], ":", vars[2], "\n")
  # la colonne d'interaction
  BM_tot_inter_test[[interaction]] <- BM_tot_inter_test[[vars[1]]] * BM_tot_inter_test[[vars[2]]]
}
# names(BM_tot_inter_test)



# Richesse_tot ------------------------------------------------------------------
# inter_Richesse_tot <- select_inter("Richesse_tot", df_mod_Richesse_tot, best_20_Richesse_tot[-1])
# write.csv2(x =inter_Richesse_tot,file = "Results/inter_Richesse_tot.csv", row.names = FALSE)
inter_Richesse_tot = read.csv2("Results/inter_Richesse_tot.csv")


inter_Richesse_tot <- arrange(inter_Richesse_tot, desc(r_squared))
best_inte_Richesse_tot = inter_Richesse_tot[1:20, "variables"] 


# Richesse_tot_inter_train = Richesse_tot_train
# for (interaction in best_inte_Richesse_tot) {
#   vars <- strsplit(interaction, ".inter.")[[1]]
#   # cat(vars[1], ":", vars[2], "\n")
#   # la colonne d'interaction
#   Richesse_tot_inter_train[[interaction]] <- Richesse_tot_inter_train[[vars[1]]] * Richesse_tot_inter_train[[vars[2]]]
# }
# # names(Richesse_tot_inter_train)
# 
# 
# Richesse_tot_inter_test = Richesse_tot_test
# for (interaction in best_inte_Richesse_tot) {
#   vars <- strsplit(interaction, ".inter.")[[1]]
#   # cat(vars[1], ":", vars[2], "\n")
#   # la colonne d'interaction
#   Richesse_tot_inter_test[[interaction]] <- Richesse_tot_inter_test[[vars[1]]] * Richesse_tot_inter_test[[vars[2]]]
# }
# names(Richesse_tot_inter_test)
```



```{r}
# correlation_matrix <- cor(df_mod_AB_tot[,c(2,5:8,11:16,20)],use = "na.or.complete")
# corrplot::corrplot(correlation_matrix, method = "color", type = "lower", order = "hclust", addCoef.col = "black",diag = FALSE,cl.cex = 1,number.cex = 1)
# for(i in names(df_mod_AB_tot[,c(2,5:8,11:16,20:26)])){cat(i,",")}
# for(i in names(df_mod_AB_tot)){cat( paste0("s(",i,") + "))}
# for(i in names(AB_tot_inter_train)) {cat( paste0("s(",i,") + "))}

# 
# names(df_mod_BM_tot)
# correlation_matrix <- cor(df_mod_BM_tot[,c(4:5,7,8,10, 12:17,20)],use = "na.or.complete")
# corrplot::corrplot(correlation_matrix, method = "color", type = "lower", order = "hclust", addCoef.col = "black",diag = FALSE,cl.cex = 1,number.cex = 1)
# for(i in names(df_mod_BM_tot[,c(4:5,7,8,10, 12:17,20)])){cat(i,",")}
# for(i in names(df_mod_BM_tot)){cat( paste0("s(",i,") + "))}
# for(i in names(BM_tot_inter_train)) {cat( paste0("s(",i,") + "))}
# 
# 
# 
# names(df_mod_Richesse_tot)
# correlation_matrix <- cor(df_mod_Richesse_tot[,c(4:6,8:13,16,18:20)],use = "na.or.complete")
# corrplot::corrplot(correlation_matrix, method = "color", type = "lower", order = "hclust", addCoef.col = "black",diag = FALSE,cl.cex = 1,number.cex = 1)
# for(i in names(df_mod_Richesse_tot[,c(4:6,8:13,16,18:20)])){cat(i,",")}
# for(i in names(df_mod_Richesse_tot)){cat( paste0("s(",i,") + "))}
# for(i in names(Richesse_tot_inter_train)) {cat( paste0("s(",i,") + "))}
```
 

## GLM
```{r function GLM, echo=TRUE}
GLM <- function(var_rep, df_app, df_valid,family = 'gaussian'){
  
  
  var_predicteurs = names(df_app[,-1])
 
  df_app = df_app[,c(var_rep,var_predicteurs)]
  df_valid = df_valid[,c(var_rep,var_predicteurs)]
  
  formula <- substitute(var_rep ~ ., list(var_rep = as.name(var_rep)))
  
  
  # entrainement du modele sur le jeu d'entrainement
  modelglm<-glm(formula,family = family ,data = df_app)
  
  # Prediction sur le jeu de validation
  pred.GLM<-predict(modelglm,newdata=as.data.frame(df_valid[,var_predicteurs]))
  
  # Calcul du RMSE pour évaluer la qualite du modele
  rmse <- round (sqrt(mean((df_valid[,var_rep] - pred.GLM)^2,na.rm=TRUE)),2)
  
  
 # Calcul du R² ajusté pour train
  R_adj_train <- calcule_R2(df_app[,var_rep],  predict(modelglm, data=df_app))
  n_train <- nrow(df_app)
  p_train <- ncol(df_app) - 1
  r_adj_train <- 1 - ((1 - R_adj_train) * (n_train - 1) / (n_train - p_train - 1))
  
  # Calcul du R² ajusté pour test
  R_adj_test <-calcule_R2(df_valid[,var_rep],pred.GLM)
  n_test <- nrow(df_valid)
  p_test <- ncol(df_valid) - 1
  r_adj_test <- 1 - ((1 - R_adj_test) * (n_test - 1) / (n_test - p_test - 1))

  
  MAE <- mean(abs(pred.GLM - df_valid[,var_rep]),na.rm=TRUE)
  
  # Round results
  rmse <- round(rmse, 2)
  r_adj_train <- round(r_adj_train, 2)
  r_adj_test <- round(r_adj_test, 2)
  MAE <- round(MAE, 2)
  
  
  results <- list(RMSE = rmse, R_adj_train = r_adj_train, R_adj_test = r_adj_test, MAE = MAE, model = modelglm,predit = pred.GLM)
  return(results)
}

```

-   Gaussian distribution

## GAM
```{r function GAM, echo=TRUE}
GAM <- function(var_rep, df_app, df_valid, family = 'gaussian',method = "REML", interaction = FALSE){
  
  var_predicteurs = names(df_app[,-1])
  
  
  
  if (var_rep == "AB_tot"){ 
  if (interaction == FALSE){ 
  modelgam<-gam(AB_tot ~ s(CaCO3) + s(gps_x) + s(bio7) + s(bio4) + s(N) + s(bio3) + s(gps_y) + s(bio18) + s(bio15) + s(P) + s(gdd0) + s(clay) + s(elevation) + s(CEC) + s(silt) + s(hurs_mean) + s(sand) + s(CN) + s(bio6) + clcm_lvl3mf + clcm_lvl3gua + clcm_lvl3ng + clcm_lvl3nial + clcm_lvl3p + clcm_lvl3v,
        family=family,method = method,data = df_app)
  } else {
    
   modelgam<-gam(AB_tot ~ s(CaCO3) + s(bio4) + s(N) + s(bio3) + s(elevation) + s(bio18) + s(CN) + s(P) + s(gdd0) + s(bio6) + s(sand) + s(bio8) + clcm_lvl3mf + clcm_lvl3gua + clcm_lvl3ng + clcm_lvl3nial + clcm_lvl3p + clcm_lvl3v + s(CaCO3.inter.gps_x) + s(CaCO3.inter.bio4) + s(CaCO3.inter.bio18) + s(CaCO3.inter.bio3) + s(CaCO3.inter.bio7) + s(CaCO3.inter.elevation) + s(CaCO3.inter.bio6) + s(CaCO3.inter.N) + s(gps_x.inter.N) + s(CaCO3.inter.CEC) + s(gps_x.inter.bio6) + s(gps_x.inter.silt) + s(CaCO3.inter.silt) + s(CaCO3.inter.bio15) + s(CaCO3.inter.clay) + s(CaCO3.inter.CN) + s(gps_x.inter.bio15) + s(gps_x.inter.sand) + s(gps_x.inter.gdd0) + s(gps_x.inter.bio8),
        family=family,method = method,data = df_app) 
  }
  
  }
  
  
  
  
  
  if (var_rep == "BM_tot"){ 
     if (interaction == FALSE){ 
  modelgam<-gam(BM_tot ~ s(gps_x) + s(CaCO3) + s(P) + s(clay) + s(gps_y) + s(bio4) + s(sand) + s(N) + s(CEC) + s(gdd10) + s(PET) + s(CN) + s(pH) + s(hurs_mean) + s(silt) + s(bio12) + s(K) + s(bio3) + s(bio7) + clcm_lvl3mf + clcm_lvl3gua + clcm_lvl3ng + clcm_lvl3nial + clcm_lvl3p + clcm_lvl3v,
        family=family,method = method,data = df_app)
  
     } else {
    modelgam<-gam(BM_tot ~ s(P) + s(CEC) + s(gps_y) + s(CN) + s(pH) + s(gdd10) + s(silt) + s(bio4) + s(clay) + s(bio3) + s(K) + s(gdd0) + clcm_lvl3mf + clcm_lvl3gua + clcm_lvl3ng + clcm_lvl3nial + clcm_lvl3p + clcm_lvl3v + s(CaCO3.inter.CEC) + s(pH.inter.bio3) + s(gps_x.inter.pH) + s(CN.inter.pH) + s(pH.inter.bio4) + s(pH.inter.bio7) + s(CaCO3.inter.pH) + s(gps_x.inter.N) + s(CaCO3.inter.bio3) + s(gps_x.inter.bio4) + s(gps_x.inter.CN) + s(CaCO3.inter.clay) + s(P.inter.pH) + s(gps_x.inter.bio7) + s(hurs_mean.inter.pH) + s(gps_y.inter.hurs_mean) + s(pH.inter.bio12) + s(pH.inter.N) + s(CEC.inter.pH) + s(gps_x.inter.hurs_mean),
        family=family,method = method,data = df_app)
     }
    
  }
  
  
  
  if(var_rep == "Richesse_tot"){ 
    if (interaction == FALSE){
  modelgam<-gam(Richesse_tot ~ s(gps_x) + s(bio4) + s(bio7) + s(gps_y) + s(pH) + s(CaCO3) + s(bio18) + s(P) + s(CEC) + s(bio13) + s(N) + s(bio16) + s(hurs_mean) + s(bio10) + s(PET) + s(bio15) + s(elevation) + s(gdd10) + s(bio9) + clcm_lvl3mf + clcm_lvl3gua + clcm_lvl3ng + clcm_lvl3nial + clcm_lvl3p + clcm_lvl3v ,
        family=family,method = method,data = df_app)
    } else { 
          modelgam<-gam(Richesse_tot ~ s(gps_x) + s(bio4) + s(bio7) + s(gps_y) + s(pH) + s(CaCO3) + s(bio18) + s(P) + s(CEC) + s(bio13) + s(N) + s(bio16) + s(hurs_mean) + s(bio10) + s(PET) + s(bio15) + s(elevation) + s(gdd10) + s(bio9) + clcm_lvl3mf + clcm_lvl3gua + clcm_lvl3ng + clcm_lvl3nial + clcm_lvl3p + clcm_lvl3v + s(bio4.inter.pH) + s(gps_x.inter.PET) + s(gps_x.inter.hurs_mean) + s(gps_x.inter.gdd10) + s(gps_x.inter.pH) + s(pH.inter.elevation) + s(CEC.inter.hurs_mean) + s(gps_x.inter.bio10) + s(bio7.inter.pH) + s(gps_x.inter.gps_y) + s(bio4.inter.P) + s(CEC.inter.bio10) + s(bio4.inter.CaCO3) + s(pH.inter.bio18) + s(gps_x.inter.bio5) + s(gps_x.inter.CaCO3) + s(bio18.inter.hurs_mean) + s(bio4.inter.PET) + s(bio18.inter.bio10) + s(bio4.inter.CEC),
        family=family,method = method,data = df_app)
    }
  }
  
  
  # Prediction sur le jeu de validation
  pred.GAM <- predict(modelgam,newdata=as.data.frame(df_valid[,var_predicteurs]))
  
  # Calcul du RMSE pour évaluer la qualite du modele
  rmse <- sqrt(mean((df_valid[,var_rep] - pred.GAM)^2,na.rm=TRUE))

  
# Calcul du R² ajusté pour train
  R_adj_train <- calcule_R2(df_app[,var_rep],  predict(modelgam, data=df_app))
  n_train <- nrow(df_app)
  p_train <- ncol(df_app) - 1
  r_adj_train <- 1 - ((1 - R_adj_train) * (n_train - 1) / (n_train - p_train - 1))
  
  # Calcul du R² ajusté pour test
  R_adj_test <-calcule_R2(df_valid[,var_rep],pred.GAM)
  n_test <- nrow(df_valid)
  p_test <- ncol(df_valid) - 1
  r_adj_test <- 1 - ((1 - R_adj_test) * (n_test - 1) / (n_test - p_test - 1))

  # Calcule le MAE
  MAE <- mean(abs(pred.GAM - df_valid[,var_rep]))
  
  # Round results
  rmse <- round(rmse, 2)
  r_adj_train <- round(r_adj_train, 2)
  r_adj_test <- round(r_adj_test, 2)
  MAE <- round(MAE, 2)
  
  
  results <- list(RMSE = rmse, R_adj_train = r_adj_train, R_adj_test = r_adj_test, MAE = MAE, model = modelgam, predit = pred.GAM)
  
  return(results)
}

```

-   Family = gaussian 

-   Link function = identity 

-   Method = REML

-   Tuning



## RF

-   Default model

```{r}
# Grille de hyperparametisation
RF_df_grid <- expand.grid(ntree = c(100,300,500,700,900,1000,1300,1500,1700,2000),
                       mtry = c(2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24),
                       nodesize = c(10 , 20,  30,  40,  50,  60,  70,  80))
```

-   RF model tuning by grid

  -   ntree = $100,300,500,700,900,1000,1300,1500,1700,2000$
  
  -   mtry = $2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24$
  
  -   maxnodes = $10 , 20,  30,  40,  50,  60,  70,  80,  90, 100$
  
 **Total number of models = $ntree * mtry * maxnode = `r nrow(RF_df_grid)`$ **
  
 -    Validation of models on test data
  

```{r function RF, echo=TRUE,fig.align='center'}

ForetAlea <- function(var_rep, df_app, df_valid, mtry, ntree, maxnodes) {
  set.seed(1234)
  col_posi <- which(names(df_app) == var_rep)
  ForeVDT <- randomForest::randomForest(df_app[-col_posi], df_app[[col_posi]], mtry = mtry, ntree = ntree, maxnodes = maxnodes)
  
  # Prediction on the validation dataset
  col_posi <- which(names(df_valid) == var_rep)
  pred.RF <- predict(ForeVDT, newdata = df_valid[, -col_posi])
  
  # Calculate RMSE to evaluate model quality
  rmse <- sqrt(mean((df_valid[, col_posi] - pred.RF)^2))
  
  
  # Calcul du R² ajusté pour train
  R_adj_train <- calcule_R2(df_app[,var_rep],  predict(ForeVDT, data=df_app))
  n_train <- nrow(df_app)
  p_train <- ncol(df_app) - 1
  r_adj_train <- 1 - ((1 - R_adj_train) * (n_train - 1) / (n_train - p_train - 1))
  
  # Calcul du R² ajusté pour test
  R_adj_test <-calcule_R2(df_valid[,col_posi],pred.RF)
  n_test <- nrow(df_valid)
  p_test <- ncol(df_valid) - 1
  r_adj_test <- 1 - ((1 - R_adj_test) * (n_test - 1) / (n_test - p_test - 1))
  
  
  # Calculate MAE
  MAE <- mean(abs(pred.RF - df_valid[, col_posi]))
  
  # Round results
  rmse <- round(rmse, 2)
  r_adj_train <- round(r_adj_train, 2)
  r_adj_test <- round(r_adj_test, 2)
  MAE <- round(MAE, 2)
  
  results <- list(RMSE = rmse, R_adj_train = r_adj_train, R_adj_test = r_adj_test, MAE = MAE, model = ForeVDT, predit = pred.RF)
  
  return(results)
}



# # Pour AB_tot  -----------------------------------------------------------------

AB_tot_RF_tuning = read.csv2("results_tuning/AB_tot_RF_tuning.csv")


AB_tot_RF_tuning = as.data.frame(AB_tot_RF_tuning)
AB_tot_RF_tuning = AB_tot_RF_tuning %>% arrange(mae)
# head(AB_tot_RF_tuning)

AB_tot_best_param = AB_tot_RF_tuning[1,]

# plot(seq(1:nrow(AB_tot_RF_tuning)), AB_tot_RF_tuning$r_squared)



df <- data.frame(x = seq(1:nrow(AB_tot_RF_tuning)), y = AB_tot_RF_tuning$r_squared)
RF_tuning = ggplot(df, aes(x = x, y = y)) +
  geom_point() +
  labs(x = "Index", y = "R Squared", title = "Abundance: R Squared over Index") +
  theme_minimal()


# ggsave("results_tuning/RF_tuning.png", plot = RF_tuning, dpi = 300)

```

<p>
  <img src="results_tuning/RF_tuning.png">
</p>



## GBM

```{r}
# Grille de hyperparametisation
GBM_df_grid <- expand.grid(n.trees = c(1000,1500,1700,2000,3000),
                       interaction.depth = c(3,  5,  6,  8, 10),
                       shrinkage = c(0.01, 0.02, 0.05, 0.001, 0.002, 0.005),
                       n.minobsinnode = c(2 , 5,  10,  30,  50,  70))
```

-   Default model

-   GBM model tuning by grid

  -   n.trees = $1000, 1500, 1700, 2000, 3000$
  
  -   shrinkage = $0.01, 0.02, 0.05, 0.001, 0.002, 0.005$
  
  -   interaction.depth = $3,  5,  6,  8, 10$
  
  -   n.minobsinnode = $2, 5,  10,  30,  50,  70$
  
  **Total number of models = $n.trees * shrinkage * interaction.depth * n.minobsinnode = `r nrow(GBM_df_grid)`$ **
  
-   Validation of models on test data
  

```{r function GBM, echo=TRUE}

GBM <- function(var_rep, df_app, df_valid,distribution = 'gaussian',n.trees ,shrinkage,interaction.depth,n.minobsinnode){

  formula <- substitute(var_rep ~ ., list(var_rep = as.name(var_rep)))

  Gradboost<-gbm(formula, data = df_app,
    distribution = distribution, 
    n.trees = n.trees,
    shrinkage = shrinkage,
    interaction.depth = interaction.depth,
    n.minobsinnode = n.minobsinnode) 
  
  # Prediction sur le jeu de validation :
   col_posi <- which(names(df_valid) == var_rep)
  prev.GBM<-predict(Gradboost,newdata=as.data.frame(df_valid[,-col_posi]))
 
  # Calcul du RMSE pour évaluer la qualité du modele
  rmse <- sqrt(mean((df_valid[,var_rep] - prev.GBM)^2))


# Calcul du R² ajusté pour train
  R_adj_train <- calcule_R2(df_app[,var_rep],  predict(Gradboost, data=df_app))
  n_train <- nrow(df_app)
  p_train <- ncol(df_app) - 1
  r_adj_train <- 1 - ((1 - R_adj_train) * (n_train - 1) / (n_train - p_train - 1))
  
  # Calcul du R² ajusté pour test
  R_adj_test <-calcule_R2(df_valid[,col_posi],prev.GBM)
  n_test <- nrow(df_valid)
  p_test <- ncol(df_valid) - 1
  r_adj_test <- 1 - ((1 - R_adj_test) * (n_test - 1) / (n_test - p_test - 1))

  # calcule MAE
  MAE <- mean(abs(prev.GBM - df_valid[,col_posi])) 
  
    
    # Round results
  rmse <- round(rmse, 2)
  r_adj_train <- round(r_adj_train, 2)
  r_adj_test <- round(r_adj_test, 2)
  MAE <- round(MAE, 2)
  
  
  
  results <- list(RMSE = rmse, R_adj_train = r_adj_train, R_adj_test = r_adj_test, MAE = MAE, model = Gradboost, predit = prev.GBM)
  
  
  return(results)
}


# Pour AB_tot  ------------------------------------------------------------------
AB_tot_GBM_tuning = read.csv2("results_tuning/AB_tot_GBM_tuning.csv")


AB_tot_GBM_tuning = as.data.frame(AB_tot_GBM_tuning)
AB_tot_GBM_tuning = AB_tot_GBM_tuning %>% arrange(mae)
# head(AB_tot_GBM_tuning)
AB_tot_best_param = AB_tot_GBM_tuning[1,]


df <- data.frame(x = seq(1:nrow(AB_tot_GBM_tuning)), y = AB_tot_GBM_tuning$r_squared)
GBM_tuning = ggplot(df, aes(x = x, y = y)) +
  geom_point() +
  labs(x = "Index", y = "R Squared", title = "Abundance: R Squared over Index") +
  theme_minimal()


# ggsave("results_tuning/GBM_tuning.png", plot = GBM_tuning, dpi = 300)

```

<p>
  <img src="results_tuning/GBM_tuning.png">
</p>


## ANN
-   Default model

```{r,include=TRUE}
# for(i in names(df_mod_AB_tot)){ cat(i,"+")}

n <- neuralnet(AB_tot~ clcm_lvl3mf +clcm_lvl3gua +clcm_lvl3ng +clcm_lvl3nial +clcm_lvl3p +clcm_lvl3v + CaCO3 +gps_x +bio7 +bio4 +N +bio3 +gps_y +bio18 +bio15 +P +gdd0 +clay +elevation +CEC +silt +hurs_mean +sand +CN +bio6,
               data = AB_tot_train,
               # hidden = c(1),
               linear.output = F,
               lifesign = 'full',
               rep=1)

plot(n,
     col.hidden = 'black',
     col.hidden.synapse = 'black',
     show.weights = F,
     information = F,
     fill = 'lightblue')




ANN_1 <- keras_model_sequential()
ANN_1 %>% 
  layer_dense(units = 1, activation = 'relu', input_shape = c(25)) %>%
  layer_dense(units = 1)

# Compile
ANN_1 %>% compile(loss = 'mse',
                  optimizer = 'rmsprop',
                  metrics = 'mae')
summary(ANN_1)

# Fit ANN_1
# myANN_1 <- ANN_1 %>%
#   fit(training,
#       trainingtarget,
#       epochs = 100,
#       #batch_size = 1,
#       validation_split = 0.2)
# plot_ANN1 = plot(myANN_1)

# ggsave("models/plot_ANN_default_model.png", plot = plot_ANN1, dpi = 300)

```


<p>
  <img src="models/ANN_default_model.png">
</p>



<p>
  <img src="models/plot_ANN_default_model.png">
</p>



-   Tunning

runs <- tuning_run("Experiment.R",
                    flags = list(dense_units1 = c(32, 64),
                                 dense_units2 = c(16, 32),
                                 dense_units3 = c(8, 16),
                                 dense_units4 = c(4, 8),
                                 dropout1 = c(0.4, 0.5),
                                 dropout1 = c(0.3, 0.4),
                                 dropout1 = c(0.2, 0.3),
                                 dropout1 = c(0.1, 0.2),
                                 batch_size = c(32, 64)))

-   hidden = c(32,32,16,8) 

```{r,include=TRUE}
n <- neuralnet(AB_tot~ clcm_lvl3mf +clcm_lvl3gua +clcm_lvl3ng +clcm_lvl3nial +clcm_lvl3p +clcm_lvl3v + CaCO3 +gps_x +bio7 +bio4 +N +bio3 +gps_y +bio18 +bio15 +P +gdd0 +clay +elevation +CEC +silt +hurs_mean +sand +CN +bio6,
               data = AB_tot_train,
               hidden = c(32,32,16,8),
               linear.output = F,
               lifesign = 'full',
               rep=1)

plot(n,
     col.hidden = 'black',
     col.hidden.synapse = 'black',
     show.weights = F,
     information = F,
     fill = 'lightblue')

# Pour AB_tot  ------------------------------------------------------------------
var_rep="AB_tot"
AB_tot_ANN_tuning = read.csv2("results_tuning/AB_tot_ANN_tuning.csv")

# Best hyperparameter values
AB_tot_ANN_tuning = as.data.frame(AB_tot_ANN_tuning)
AB_tot_ANN_tuning = AB_tot_ANN_tuning %>% arrange(metric_val_mae)
# head(AB_tot_ANN_tuning[,2:16])

best_param = AB_tot_ANN_tuning[1,]


dense_units1 = as.numeric(best_param$flag_dense_units1)
dense_units2 = as.numeric(best_param$flag_dense_units2)
dense_units3 = as.numeric(best_param$flag_dense_units3)
dense_units4 = as.numeric(best_param$flag_dense_units4)

dropout1 =as.numeric(best_param$flag_dropout1)
dropout2 =as.numeric(best_param$flag_dropout2)
dropout3 =as.numeric(best_param$flag_dropout3)
dropout4 =as.numeric(best_param$flag_dropout4)

batch_size =as.numeric(best_param$flag_batch_size)


# AB_tot TUNE MODEL
ANN_tune_AB_tot <- keras_model_sequential()
ANN_tune_AB_tot %>% 
  layer_dense(units = dense_units1, activation = 'relu', input_shape = c(25)) %>%
  layer_dropout(rate = dropout1)  %>%
  layer_dense(units = dense_units2, activation = 'relu') %>%
  layer_dropout(rate = dropout2)  %>%
  layer_dense(units = dense_units3, activation = 'relu') %>%
  layer_dropout(rate = dropout3)  %>%
  layer_dense(units = dense_units4, activation = 'relu') %>%
  layer_dropout(rate = dropout4)  %>%
  layer_dense(units = 1)


# Compile
ANN_tune_AB_tot %>% compile(loss = 'mse',
                  optimizer = 'rmsprop',
                  metrics = 'mae')

summary(ANN_tune_AB_tot)
# #  callback EarlyStopping
# mon_callback <- callback_early_stopping(
#   monitor = "val_mae",  # Surveille la perte sur l'ensemble de validation
#   patience = 10,         # Nombre d'époques sans amélioration avant l'arrêt
#   restore_best_weights = TRUE  # Restaure les poids du meilleur modèle
# )
# 
# 
# # Fit ANN_tune_AB_tot
# myANN_tune_AB_tot <- ANN_tune_AB_tot %>%
#   fit(training,
#       trainingtarget,
#       epochs = 100,
#       batch_size = batch_size,
#       validation_split = 0.2,
#       #callbacks = list(mon_callback)
#       )


```

<p>
  <img src="models/ANN_tuning_model.png">
</p>


<p>
  <img src="Results/fig_ANN_tune_BM_tot.png">
</p>



## Compilation pour chaque algoritme

-   GLM
```{r Compile.GLM}
# # Pour AB_tot ------------------------------------------------------------------




 ### interaction ###
# GLM_result_AB_tot_inter = GLM(var_rep ="AB_tot", 
#                              df_app=AB_tot_inter_train, 
#                              df_valid = AB_tot_inter_test ,
#                              family = 'gaussian')
# GLM_result_AB_tot_inter$RMSE
# GLM_result_AB_tot_inter$MAE
# GLM_result_AB_tot_inter$R_squared
# GLM_result_AB_tot_inter$predit
# GLM_result_AB_tot_inter$model







# # Pour BM_tot ------------------------------------------------------------------






 ### interaction ###
# GLM_result_BM_tot_inter = GLM(var_rep ="BM_tot", 
#                              df_app=BM_tot_inter_train, 
#                              df_valid = BM_tot_inter_test ,
#                              family = 'gaussian')
# GLM_result_BM_tot_inter$RMSE
# GLM_result_BM_tot_inter$MAE
# GLM_result_BM_tot_inter$R_squared
# GLM_result_BM_tot_inter$predit
# GLM_result_BM_tot_inter$model







# # Pour Richesse_tot ------------------------------------------------------------------





 ### interaction ###
# GLM_result_Richesse_tot_inter = GLM(var_rep ="Richesse_tot", 
#                              df_app=Richesse_tot_inter_train, 
#                              df_valid = Richesse_tot_inter_test ,
#                              family = 'gaussian')
# GLM_result_Richesse_tot_inter$RMSE
# GLM_result_Richesse_tot_inter$MAE
# GLM_result_Richesse_tot_inter$R_squared
# GLM_result_Richesse_tot_inter$predit
# GLM_result_Richesse_tot_inter$model

```


-   GAM
```{r Compile.GAM}
# # Pour AB_tot ------------------------------------------------------------------



### interaction ###
# GAM_result_AB_tot_inter = GAM(var_rep ="AB_tot", 
#                              df_app=AB_tot_inter_train, 
#                              df_valid = AB_tot_inter_test ,
#                              family = 'gaussian',
#                              method = "REML",
#                               interaction = TRUE)

# GAM_result_AB_tot_inter$RMSE
# GAM_result_AB_tot_inter$MAE
# GAM_result_AB_tot_inter$R_squared
# GAM_result_AB_tot_inter$predit
# GAM_result_AB_tot_inter$model






# # Pour BM_tot ------------------------------------------------------------------




### intarcation ###
# GAM_result_BM_tot_inter = GAM(var_rep ="BM_tot", 
#                              df_app=BM_tot_inter_train, 
#                              df_valid = BM_tot_inter_test ,
#                              family = 'gaussian',
#                              method = "REML",
#                               interaction = TRUE)
# GAM_result_BM_tot_inter$RMSE
# GAM_result_BM_tot_inter$MAE
# GAM_result_BM_tot_inter$R_squared
# GAM_result_BM_tot_inter$predit
# GAM_result_BM_tot_inter$model




# # Pour Richesse_tot ------------------------------------------------------------------


### intarcation ###
# GAM_result_Richesse_tot_inter = GAM(var_rep ="Richesse_tot", 
#                              df_app=Richesse_tot_inter_train, 
#                              df_valid = Richesse_tot_inter_test ,
#                              family = 'gaussian',
#                              method = "REML",
#                               interaction = TRUE)
# GAM_result_Richesse_tot_inter$RMSE
# GAM_result_Richesse_tot_inter$MAE
# GAM_result_Richesse_tot_inter$R_squared
# GAM_result_Richesse_tot_inter$predit
# GAM_result_Richesse_tot_inter$model


```


-   RF
```{r Compile.RF}
# # Pour AB_tot  -----------------------------------------------------------------


### interaction ###
# RF_result_AB_tot_inter = ForetAlea(var_rep ="AB_tot", 
#                              df_app=AB_tot_inter_train, 
#                              df_valid = AB_tot_inter_test ,
#                              mtry = AB_tot_best_mtry,
#                              ntree= AB_tot_best_ntree,
#                              maxnodes = AB_tot_best_maxnodes)
# RF_result_AB_tot_inter$RMSE
# RF_result_AB_tot_inter$MAE
# RF_result_AB_tot_inter$R_squared
# RF_result_AB_tot_inter$predit
# RF_result_AB_tot_inter$model







# # Pour BM_tot  -----------------------------------------------------------------





### interaction ###
# RF_result_BM_tot_inter = ForetAlea(var_rep ="BM_tot", 
#                              df_app=BM_tot_inter_train, 
#                              df_valid = BM_tot_inter_test ,
#                              mtry = BM_tot_best_mtry,
#                              ntree= BM_tot_best_ntree,
#                              maxnodes = BM_tot_best_maxnodes)
# RF_result_BM_tot_inter$RMSE
# RF_result_BM_tot_inter$MAE
# RF_result_BM_tot_inter$R_squared
# RF_result_BM_tot_inter$predit
# RF_result_BM_tot_inter$model





# # Pour Richesse_tot  -----------------------------------------------------------





### interaction ###
# RF_result_Richesse_tot_inter = ForetAlea(var_rep ="Richesse_tot", 
#                              df_app=Richesse_tot_inter_train, 
#                              df_valid = Richesse_tot_inter_test ,
#                              mtry = Richesse_tot_best_mtry,
#                              ntree= Richesse_tot_best_ntree,
#                              maxnodes = Richesse_tot_best_maxnodes)
# RF_result_Richesse_tot_inter$RMSE
# RF_result_Richesse_tot_inter$MAE
# RF_result_Richesse_tot_inter$R_squared
# RF_result_Richesse_tot_inter$predit
# RF_result_Richesse_tot_inter$model


```


-   GBM
```{r Compile.GBM}
# Pour AB_tot  ------------------------------------------------------------------




### interaction ###
# GBM_result_AB_tot_inter =  GBM(var_rep ="AB_tot", 
#                          df_app=AB_tot_inter_train, 
#                          df_valid = AB_tot_inter_test,
#                          distribution = 'gaussian',
#                          n.trees = AB_tot_best_n.trees,
#                          shrinkage = AB_tot_best_shrinkage,
#                          interaction.depth = AB_tot_best_interaction.depth,
#                          n.minobsinnode = AB_tot_best_n.minobsinnode)

# GBM_result_AB_tot_inter$RMSE
# GBM_result_AB_tot_inter$MAE
# GBM_result_AB_tot_inter$R_squared
# GBM_result_AB_tot_inter$predit
# GBM_result_AB_tot_inter$model








# Pour BM_tot ------------------------------------------------------------------






### interaction ###
# GBM_result_BM_tot_inter =  GBM(var_rep ="BM_tot", 
#                          df_app=BM_tot_inter_train, 
#                          df_valid = BM_tot_inter_test,
#                          distribution = 'gaussian',
#                          n.trees = BM_tot_best_n.trees,
#                          shrinkage = BM_tot_best_shrinkage,
#                          interaction.depth = BM_tot_best_interaction.depth,
#                          n.minobsinnode = BM_tot_best_n.minobsinnode)

# GBM_result_BM_tot_inter$RMSE
# GBM_result_BM_tot_inter$MAE
# GBM_result_BM_tot_inter$R_squared
# GBM_result_BM_tot_inter$predit
# GBM_result_BM_tot_inter$model






# # Pour Richesse_tot ------------------------------------------------------------------




### interaction ###
# GBM_result_Richesse_tot_inter =  GBM(var_rep ="Richesse_tot", 
#                          df_app=Richesse_tot_inter_train, 
#                          df_valid = Richesse_tot_inter_test,
#                          distribution = 'gaussian',
#                          n.trees = Richesse_tot_best_n.trees,
#                          shrinkage = Richesse_tot_best_shrinkage,
#                          interaction.depth = Richesse_tot_best_interaction.depth,
#                          n.minobsinnode = Richesse_tot_best_n.minobsinnode)

# GBM_result_Richesse_tot_inter$RMSE
# GBM_result_Richesse_tot_inter$MAE
# GBM_result_Richesse_tot_inter$R_squared
# GBM_result_Richesse_tot_inter$predit
# GBM_result_Richesse_tot_inter$model



```


-   ANN AB_tot
```{r ANN AB_tot}
# Pour AB_tot  ------------------------------------------------------------------
var_rep="AB_tot"
AB_tot_ANN_tuning = read.csv2("results_tuning/AB_tot_ANN_tuning.csv")

# Best hyperparameter values
AB_tot_ANN_tuning = as.data.frame(AB_tot_ANN_tuning)
AB_tot_ANN_tuning = AB_tot_ANN_tuning %>% arrange(metric_val_mae)
# head(AB_tot_ANN_tuning[,2:16])

best_param = AB_tot_ANN_tuning[1,]


dense_units1 = as.numeric(best_param$flag_dense_units1)
dense_units2 = as.numeric(best_param$flag_dense_units2)
dense_units3 = as.numeric(best_param$flag_dense_units3)
dense_units4 = as.numeric(best_param$flag_dense_units4)

dropout1 =as.numeric(best_param$flag_dropout1)
dropout2 =as.numeric(best_param$flag_dropout2)
dropout3 =as.numeric(best_param$flag_dropout3)
dropout4 =as.numeric(best_param$flag_dropout4)

batch_size =as.numeric(best_param$flag_batch_size)


# data
training = AB_tot_train
test = AB_tot_test

training %<>% mutate_if(is.factor, as.numeric)
ind_var_rep <- which(names(training) == var_rep)
trainingtarget <- training[, ind_var_rep]
training <- training[, -ind_var_rep]
training <- as.matrix(training)
dimnames(training) <- NULL

ind_var_rep <- which(names(test) == var_rep)
testtarget <- test[, ind_var_rep]
test <- test[, -ind_var_rep]
test %<>% mutate_if(is.factor, as.numeric)
test <- as.matrix(test)
dimnames(test) <- NULL


# AB_tot TUNE MODEL
ANN_tune_AB_tot <- keras_model_sequential()
ANN_tune_AB_tot %>% 
  layer_dense(units = dense_units1, activation = 'relu', input_shape = c(25)) %>%
  layer_dropout(rate = dropout1)  %>%
  layer_dense(units = dense_units2, activation = 'relu') %>%
  layer_dropout(rate = dropout2)  %>%
  layer_dense(units = dense_units3, activation = 'relu') %>%
  layer_dropout(rate = dropout3)  %>%
  layer_dense(units = dense_units4, activation = 'relu') %>%
  layer_dropout(rate = dropout4)  %>%
  layer_dense(units = 1)


# Compile
ANN_tune_AB_tot %>% compile(loss = 'mse',
                  optimizer = 'rmsprop',
                  metrics = 'mae')

#  callback EarlyStopping
mon_callback <- callback_early_stopping(
  monitor = "val_mae",  # Surveille la perte sur l'ensemble de validation
  patience = 10,         # Nombre d'époques sans amélioration avant l'arrêt
  restore_best_weights = TRUE  # Restaure les poids du meilleur modèle
)


# Fit ANN_tune_AB_tot
myANN_tune_AB_tot <- ANN_tune_AB_tot %>%
  fit(training,
      trainingtarget,
      epochs = 100,
      batch_size = batch_size,
      validation_split = 0.2,
      #callbacks = list(mon_callback)
      )


# fig_ANN_tune_AB_tot = plot(myANN_tune_AB_tot)
# ggsave("Results/fig_ANN_tune_AB_tot.png", plot = fig_ANN_tune_AB_tot, dpi = 300)

# Evaluate
# ANN_tune_AB_tot %>% evaluate(test, testtarget)
ANN_tune_AB_tot_pred = ANN_tune_AB_tot %>% predict(test)
ANN_tune_AB_tot_mse = mean((testtarget-ANN_tune_AB_tot_pred)^2) # loss -> mse
ANN_tune_AB_tot_mae = mean(abs(ANN_tune_AB_tot_pred - testtarget),na.rm=TRUE) # MAE 
ANN_tune_AB_tot_rmse = sqrt(mean((testtarget - ANN_tune_AB_tot_pred)^2,na.rm=TRUE)) # rmse
ANN_tune_AB_tot_cor = cor(testtarget,ANN_tune_AB_tot_pred)^2 # R²



# Calcul du R² ajusté pour train
  R_adj_train <- calcule_R2(trainingtarget,  ANN_tune_AB_tot %>% predict(training))
  n_train <- nrow(training)
  p_train <- ncol(training)
  r_adj_train <- 1 - ((1 - R_adj_train) * (n_train - 1) / (n_train - p_train - 1))
  
  # Calcul du R² ajusté pour test
  R_adj_test <-calcule_R2(testtarget,ANN_tune_AB_tot_pred)
  n_test <- nrow(test)
  p_test <- ncol(test)
  r_adj_test <- 1 - ((1 - R_adj_test) * (n_test - 1) / (n_test - p_test - 1))
  
  
  
  

ANN_tune_AB_tot_results = data.frame(model = "ANN_tune_AB_tot",
                                     mse = round(ANN_tune_AB_tot_mse,2),                                                        mae = round(ANN_tune_AB_tot_mae,2),
                                     rmse = round(ANN_tune_AB_tot_rmse,2), 
                                     R_adj_train= round(r_adj_train,2),
                                     R_adj_test= round(r_adj_test,2))


# ANN_tune_AB_tot_results

```


-   ANN BM_tot
```{r ANN BM_tot}
# Pour BM_tot  ------------------------------------------------------------------
var_rep="BM_tot"
BM_tot_ANN_tuning = read.csv2("results_tuning/BM_tot_ANN_tuning.csv")

# Best hyperparameter values
BM_tot_ANN_tuning = as.data.frame(BM_tot_ANN_tuning)
BM_tot_ANN_tuning = BM_tot_ANN_tuning %>% arrange(metric_val_mae)
# head(BM_tot_ANN_tuning[,2:16])

best_param = BM_tot_ANN_tuning[1,]

dense_units1 = as.numeric(best_param$flag_dense_units1)
dense_units2 = as.numeric(best_param$flag_dense_units2)
dense_units3 = as.numeric(best_param$flag_dense_units3)
dense_units4 = as.numeric(best_param$flag_dense_units4)

dropout1 =as.numeric(best_param$flag_dropout1)
dropout2 =as.numeric(best_param$flag_dropout2)
dropout3 =as.numeric(best_param$flag_dropout3)
dropout4 =as.numeric(best_param$flag_dropout4)

batch_size =as.numeric(best_param$flag_batch_size)


# data
training = BM_tot_train
test = BM_tot_test

training %<>% mutate_if(is.factor, as.numeric)
ind_var_rep <- which(names(training) == var_rep)
trainingtarget <- training[, ind_var_rep]
training <- training[, -ind_var_rep]
training <- as.matrix(training)
dimnames(training) <- NULL

ind_var_rep <- which(names(test) == var_rep)
testtarget <- test[, ind_var_rep]
test <- test[, -ind_var_rep]
test %<>% mutate_if(is.factor, as.numeric)
test <- as.matrix(test)
dimnames(test) <- NULL


# BM_tot TUNE MODEL
ANN_tune_BM_tot <- keras_model_sequential()
ANN_tune_BM_tot %>% 
  layer_dense(units = dense_units1, activation = 'relu', input_shape = c(25)) %>%
  layer_dropout(rate = dropout1)  %>%
  layer_dense(units = dense_units2, activation = 'relu') %>%
  layer_dropout(rate = dropout2)  %>%
  layer_dense(units = dense_units3, activation = 'relu') %>%
  layer_dropout(rate = dropout3)  %>%
  layer_dense(units = dense_units4, activation = 'relu') %>%
  layer_dropout(rate = dropout4)  %>%
  layer_dense(units = 1)


# Compile
ANN_tune_BM_tot %>% compile(loss = 'mse',
                  optimizer = 'rmsprop',
                  metrics = 'mae')

#  callback EarlyStopping
mon_callback <- callback_early_stopping(
  monitor = "val_mae",  # Surveille la perte sur l'ensemble de validation
  patience = 10,         # Nombre d'époques sans amélioration avant l'arrêt
  restore_best_weights = TRUE  # Restaure les poids du meilleur modèle
)


# Fit ANN_tune_BM_tot
myANN_tune_BM_tot <- ANN_tune_BM_tot %>%
  fit(training,
      trainingtarget,
      epochs = 100,
      batch_size = batch_size,
      validation_split = 0.2,
      #callbacks = list(mon_callback)
      )

# fig_ANN_tune_BM_tot = plot(myANN_tune_BM_tot)
# ggsave("Results/fig_ANN_tune_BM_tot.png", plot = fig_ANN_tune_BM_tot, dpi = 300)


# Evaluate
# ANN_tune_BM_tot %>% evaluate(test, testtarget)
ANN_tune_BM_tot_pred = ANN_tune_BM_tot %>% predict(test)
ANN_tune_BM_tot_mse = mean((testtarget-ANN_tune_BM_tot_pred)^2) # loss -> mse
ANN_tune_BM_tot_mae = mean(abs(ANN_tune_BM_tot_pred - testtarget),na.rm=TRUE) # MAE 
ANN_tune_BM_tot_rmse = sqrt(mean((testtarget - ANN_tune_BM_tot_pred)^2,na.rm=TRUE)) # rmse
ANN_tune_BM_tot_cor = cor(testtarget,ANN_tune_BM_tot_pred)^2 # R²



# Calcul du R² ajusté pour train
  R_adj_train <- calcule_R2(trainingtarget,  ANN_tune_BM_tot %>% predict(training))
  n_train <- nrow(training)
  p_train <- ncol(training)
  r_adj_train <- 1 - ((1 - R_adj_train) * (n_train - 1) / (n_train - p_train - 1))
  
  # Calcul du R² ajusté pour test
  R_adj_test <-calcule_R2(testtarget,ANN_tune_BM_tot_pred)
  n_test <- nrow(test)
  p_test <- ncol(test)
  r_adj_test <- 1 - ((1 - R_adj_test) * (n_test - 1) / (n_test - p_test - 1))
  
  
  
  

ANN_tune_BM_tot_results = data.frame(model = "ANN_tune_BM_tot",
                                     mse = round(ANN_tune_BM_tot_mse,2),                                                        mae = round(ANN_tune_BM_tot_mae,2),
                                     rmse = round(ANN_tune_BM_tot_rmse,2), 
                                     R_adj_train= round(r_adj_train,2),
                                     R_adj_test= round(r_adj_test,2))
# ANN_tune_BM_tot_results

```

-   ANN Richesse_tot
```{r ANN Richesse_tot}
# Pour Richesse_tot  ------------------------------------------------------------------
var_rep="Richesse_tot"
Richesse_tot_ANN_tuning = read.csv2("results_tuning/Richesse_tot_ANN_tuning.csv")

# Best hyperparameter values
Richesse_tot_ANN_tuning = as.data.frame(Richesse_tot_ANN_tuning)
Richesse_tot_ANN_tuning = Richesse_tot_ANN_tuning %>% arrange(metric_val_mae)
# head(Richesse_tot_ANN_tuning[,2:16])

best_param = Richesse_tot_ANN_tuning[1,]

dense_units1 = as.numeric(best_param$flag_dense_units1)
dense_units2 = as.numeric(best_param$flag_dense_units2)
dense_units3 = as.numeric(best_param$flag_dense_units3)
dense_units4 = as.numeric(best_param$flag_dense_units4)

dropout1 =as.numeric(best_param$flag_dropout1)
dropout2 =as.numeric(best_param$flag_dropout2)
dropout3 =as.numeric(best_param$flag_dropout3)
dropout4 =as.numeric(best_param$flag_dropout4)

batch_size =as.numeric(best_param$flag_batch_size)


# data
training = Richesse_tot_train
test = Richesse_tot_test

training %<>% mutate_if(is.factor, as.numeric)
ind_var_rep <- which(names(training) == var_rep)
trainingtarget <- training[, ind_var_rep]
training <- training[, -ind_var_rep]
training <- as.matrix(training)
dimnames(training) <- NULL

ind_var_rep <- which(names(test) == var_rep)
testtarget <- test[, ind_var_rep]
test <- test[, -ind_var_rep]
test %<>% mutate_if(is.factor, as.numeric)
test <- as.matrix(test)
dimnames(test) <- NULL


# Richesse_tot TUNE MODEL
ANN_tune_Richesse_tot <- keras_model_sequential()
ANN_tune_Richesse_tot %>% 
  layer_dense(units = dense_units1, activation = 'relu', input_shape = c(25)) %>%
  layer_dropout(rate = dropout1+0.2)  %>%
  layer_dense(units = dense_units2, activation = 'relu') %>%
  layer_dropout(rate = dropout2+0.2)  %>%
  layer_dense(units = dense_units3, activation = 'relu') %>%
  layer_dropout(rate = dropout3+0.2)  %>%
  layer_dense(units = dense_units4, activation = 'relu') %>%
  layer_dropout(rate = dropout4+0.2)  %>%
  layer_dense(units = 1)


# Compile
ANN_tune_Richesse_tot %>% compile(loss = 'mse',
                  optimizer = 'rmsprop',
                  metrics = 'mae')

#  callback EarlyStopping
mon_callback <- callback_early_stopping(
  monitor = "val_mae",  # Surveille la perte sur l'ensemble de validation
  patience = 10,         # Nombre d'époques sans amélioration avant l'arrêt
  restore_best_weights = TRUE  # Restaure les poids du meilleur modèle
)


# Fit ANN_tune_Richesse_tot
myANN_tune_Richesse_tot <- ANN_tune_Richesse_tot %>%
  fit(training,
      trainingtarget,
      epochs = 100,
      batch_size = batch_size,
      validation_split = 0.2,
      #callbacks = list(mon_callback)
      )


# fig_ANN_tune_Richesse_tot = plot(myANN_tune_Richesse_tot)
# ggsave("Results/fig_ANN_tune_Richesse_tot.png", plot = fig_ANN_tune_Richesse_tot, dpi = 300)

# Evaluate
# ANN_tune_Richesse_tot %>% evaluate(test, testtarget)
ANN_tune_Richesse_tot_pred = ANN_tune_Richesse_tot %>% predict(test)
ANN_tune_Richesse_tot_mse = mean((testtarget-ANN_tune_Richesse_tot_pred)^2) # loss -> mse
ANN_tune_Richesse_tot_mae = mean(abs(ANN_tune_Richesse_tot_pred - testtarget),na.rm=TRUE) # MAE 
ANN_tune_Richesse_tot_rmse = sqrt(mean((testtarget - ANN_tune_Richesse_tot_pred)^2,na.rm=TRUE)) # rmse
ANN_tune_Richesse_tot_cor = cor(testtarget,ANN_tune_Richesse_tot_pred)^2 # R²



# Calcul du R² ajusté pour train
  R_adj_train <- calcule_R2(trainingtarget,  ANN_tune_Richesse_tot %>% predict(training))
  n_train <- nrow(training)
  p_train <- ncol(training)
  r_adj_train <- 1 - ((1 - R_adj_train) * (n_train - 1) / (n_train - p_train - 1))
  
  # Calcul du R² ajusté pour test
  R_adj_test <-calcule_R2(testtarget,ANN_tune_Richesse_tot_pred)
  n_test <- nrow(test)
  p_test <- ncol(test)
  r_adj_test <- 1 - ((1 - R_adj_test) * (n_test - 1) / (n_test - p_test - 1))
  
  
  
  

ANN_tune_Richesse_tot_results = data.frame(model = "ANN_tune_Richesse_tot",
                                     mse = round(ANN_tune_Richesse_tot_mse,2),                                                        mae = round(ANN_tune_Richesse_tot_mae,2),
                                     rmse = round(ANN_tune_Richesse_tot_rmse,2), 
                                     R_adj_train= round(r_adj_train,2),
                                     R_adj_test= round(r_adj_test,2))
# ANN_tune_Richesse_tot_results

```



# Results: Case 1 -> repeated data
```{r}
# coul = c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2")
# coul2 = c("#E69F00", "#1F77B4", "#009E73", "#F0E442", "#9467BD")
# coul3=c("#1F77B4", "#7F7F7F", "#2CA02C", "#D62728", "#9467BD")
couleurs = c("#2CA02C","#E69F00", "#1F77B4","#7F7F7F", "#D62728","#9467BD")
```



## Prediction of total abundance

```{r}
subtitle <- sprintf("Abundance : %.2f ± %.2f ind/m²", mean(data_deep$AB_tot, na.rm = TRUE), sd(data_deep$AB_tot, na.rm = TRUE))
```


```{r predit AB_tot, fig.align='center',}
# Prediction avec GLM -----------------------------------------
GLM_result_AB_tot = GLM(var_rep ="AB_tot", 
                             df_app=AB_tot_train, 
                             df_valid = AB_tot_test,
                             family = 'gaussian')
# GLM_result_AB_tot$RMSE
# GLM_result_AB_tot$MAE
# GLM_result_AB_tot$R_adj_train
# GLM_result_AB_tot$R_adj_test
# GLM_result_AB_tot$predit
# GLM_result_AB_tot$model



GLM_AB_tot_pred <- GLM_result_AB_tot$predit^2

GLM_df_AB_tot = data.frame(Observed=AB_tot_test[,1]^2,Predicted = GLM_AB_tot_pred)

cor_GLM_AB_tot <- cor(GLM_df_AB_tot$Observed, GLM_df_AB_tot$Predicted)

  # graphique avec ggplot
GLM_AB_tot <- ggplot(GLM_df_AB_tot, aes(x = Observed, y = Predicted)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = TRUE, color = "red") + 
      labs(subtitle =paste0(" GLM: R² adj (train) = ", round(GLM_result_AB_tot$R_adj_train,2),
                           "; \n R² adj (test) = ", round(GLM_result_AB_tot$R_adj_test,2),
                           "; \n RMSE (test) = ",  round(GLM_result_AB_tot$RMSE^2,2)),
                            x = "Observed valuess", 
                            y = "Predicted values") +
      theme_classic() 

# Prediction avec GAM -----------------------------------------
GAM_result_AB_tot = GAM(var_rep ="AB_tot", 
                             df_app=AB_tot_train, 
                             df_valid = AB_tot_test,
                             family = 'gaussian',
                             method = "REML")
# GAM_result_AB_tot$RMSE
# GAM_result_AB_tot$MAE
# GAM_result_AB_tot$R_adj_train
# GAM_result_AB_tot$R_adj_test
# GAM_result_AB_tot$predit
# GAM_result_AB_tot$model

# mod_gam1_ab = GAM_result_AB_tot$model
# cv <- gam.check(GAM_result_AB_tot$model)
# print(cv)
# plot(mod_gam1_ab, pages = 1, seWithMean = TRUE)
# plot(mod_gam1_ab, residuals = TRUE, pch = 1)
# plot(ggeffects::ggpredict(mod_gam1_ab), facets = TRUE)
# gratia::draw(mod_gam1_ab, residuals = TRUE)
# # Verification
# par(mfrow = c(2, 2))
# gam.check(mod_gam1_ab)
# shapiro.test(mod_gam1_ab$res)
# concurvity(mod_gam1_ab,full = TRUE)
# concurvity(mod_gam1_ab,full = FALSE)





GAM_AB_tot_pred <- GAM_result_AB_tot$predit^2

GAM_df_AB_tot = data.frame(Observed=AB_tot_test[,1]^2,Predicted = GAM_AB_tot_pred)

cor_GAM_AB_tot <- cor(GAM_df_AB_tot$Observed, GAM_df_AB_tot$Predicted)

  # graphique avec ggplot
GAM_AB_tot <- ggplot(GAM_df_AB_tot, aes(x = Observed, y = Predicted)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = TRUE, color = "red") + 
      labs(subtitle =paste0(" GAM: R² adj (train) = ", round(GAM_result_AB_tot$R_adj_train,2),
                           "; \n R² adj (test) = ", round(GAM_result_AB_tot$R_adj_test,2),
                           "; \n RMSE (test) = ",  round(GAM_result_AB_tot$RMSE^2,2)),
                            x = "Observed valuess", 
                            y = "Predicted values") +
      theme_classic() 

# Prediction avec RF -----------------------------------------
AB_tot_RF_tuning = read.csv2("results_tuning/AB_tot_RF_tuning.csv")


AB_tot_RF_tuning = as.data.frame(AB_tot_RF_tuning)
AB_tot_RF_tuning = AB_tot_RF_tuning %>% arrange(mae)
# head(AB_tot_RF_tuning)

AB_tot_best_param = AB_tot_RF_tuning[1,]

# plot(seq(1:nrow(AB_tot_RF_tuning)), AB_tot_RF_tuning$r_squared)
# Best hyperparameter values
AB_tot_best_mtry = AB_tot_best_param$mtry
AB_tot_best_ntree = AB_tot_best_param$ntree
AB_tot_best_maxnodes = AB_tot_best_param$maxnode


RF_result_AB_tot = ForetAlea(var_rep ="AB_tot", 
                             df_app=AB_tot_train, 
                             df_valid = AB_tot_test,
                             mtry = AB_tot_best_mtry,
                             ntree= AB_tot_best_ntree,
                             maxnodes = AB_tot_best_maxnodes)

# RF_result_AB_tot$RMSE
# RF_result_AB_tot$MAE
# RF_result_AB_tot$R_adj_train
# RF_result_AB_tot$R_adj_test
# RF_result_AB_tot$predit
# RF_result_AB_tot$model

# varImpPlot(RF_result_AB_tot$model)

RF_AB_tot_pred <- RF_result_AB_tot$predit^2

RF_df_AB_tot = data.frame(Observed=AB_tot_test[,1]^2,Predicted = RF_AB_tot_pred)

cor_RF_AB_tot <- cor(RF_df_AB_tot$Observed, RF_df_AB_tot$Predicted)

  # graphique avec ggplot
RF_AB_tot <- ggplot(RF_df_AB_tot, aes(x = Observed, y = Predicted)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = TRUE, color = "red") + 
      labs(subtitle =paste0(" RF: R² adj (train) = ", round(RF_result_AB_tot$R_adj_train,2), 
                           "; \n R² adj (test) = ", round(RF_result_AB_tot$R_adj_test,2),
                           "; \n RMSE (test) = ",  round(RF_result_AB_tot$RMSE^2,2)),
                            x = "Observed valuess", 
                            y = "Predicted values") + 
      theme_classic() 
best_algo_AB_tot = RF_AB_tot


# Prediction avec GBM -----------------------------------------
AB_tot_GBM_tuning = read.csv2("results_tuning/AB_tot_GBM_tuning.csv")


AB_tot_GBM_tuning = as.data.frame(AB_tot_GBM_tuning)
AB_tot_GBM_tuning = AB_tot_GBM_tuning %>% arrange(mae)
# head(AB_tot_GBM_tuning)
AB_tot_best_param = AB_tot_GBM_tuning[1,]


# Best hyperparameter values
AB_tot_best_n.trees = AB_tot_best_param$n.trees
AB_tot_best_shrinkage = AB_tot_best_param$shrinkage
AB_tot_best_interaction.depth = AB_tot_best_param$interaction.depth
AB_tot_best_n.minobsinnode = AB_tot_best_param$n.minobsinnode


GBM_result_AB_tot =  GBM(var_rep ="AB_tot", 
                         df_app=AB_tot_train, 
                         df_valid = AB_tot_test,
                         distribution = 'gaussian',
                         n.trees = AB_tot_best_n.trees,
                         shrinkage = AB_tot_best_shrinkage,
                         interaction.depth = AB_tot_best_interaction.depth,
                         n.minobsinnode = AB_tot_best_n.minobsinnode)

# GBM_result_AB_tot$RMSE
# GBM_result_AB_tot$MAE
# GBM_result_AB_tot$R_adj_train
# GBM_result_AB_tot$R_adj_test
# GBM_result_AB_tot$predit
# GBM_result_AB_tot$model


# summary(GBM_result_AB_tot$model)
# best.iter <- gbm.perf(GBM_result_AB_tot$model, method = "cv")
# summary(GBM_result_AB_tot$model, n.trees = best.iter)




GBM_AB_tot_pred = GBM_result_AB_tot$predit^2

GBM_df_AB_tot = data.frame(Observed=AB_tot_test[,1]^2,Predicted = GBM_AB_tot_pred)

cor_GBM_AB_tot<- cor(GBM_df_AB_tot$Observed, GBM_df_AB_tot$Predicted)

# graphique avec ggplot
GBM_AB_tot <- ggplot(GBM_df_AB_tot, aes(x = Observed, y = Predicted)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = TRUE, color = "red") + 
      labs(subtitle =paste0(" GBM: R² adj (train) = ", round(GBM_result_AB_tot$R_adj_train,2),
                           "; \n R² adj (test) = ", round(GBM_result_AB_tot$R_adj_test,2),
                           "; \n RMSE (test) = ",  round(GBM_result_AB_tot$RMSE^2,2)),
                            x = "Observed valuess", 
                            y = "Predicted values") +
      theme_classic() 





# Prediction avec ANN -----------------------------------------
# ANN_tune_AB_tot_results$mse
# ANN_tune_AB_tot_results$mae
# ANN_tune_AB_tot_results$rmse
# ANN_tune_AB_tot_results$r_adj_train
# ANN_tune_AB_tot_results$r_adj_test


ANN_AB_tot_pred = ANN_tune_AB_tot_pred^2

ANN_df_AB_tot = data.frame(Observed=AB_tot_test[,1]^2,Predicted = ANN_AB_tot_pred)

cor_ANN_AB_tot <- cor(ANN_df_AB_tot$Observed, ANN_df_AB_tot$Predicted)

  # graphique avec ggplot
ANN_AB_tot <- ggplot(ANN_df_AB_tot, aes(x = Observed, y = Predicted)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = TRUE, color = "red") +
labs(subtitle =paste0(" ANN: R² adj (train) = ", round(ANN_tune_AB_tot_results$R_adj_train,2),
                           "; \n R² adj (test) = ", round(ANN_tune_AB_tot_results$R_adj_test,2),
                           "; \n RMSE (test) = ",  round(ANN_tune_AB_tot_results$rmse^2,2)),
                            x = "Observed valuess", 
                            y = "Predicted values") +
      theme_classic()

# best algo --------------------------------------
graphe_AB_tot = ggarrange(GLM_AB_tot, GAM_AB_tot, RF_AB_tot, GBM_AB_tot, ANN_AB_tot,
  labels = c('(a)', '(b)','(c)', '(d)','(e)'),widths = 10,
  common.legend = TRUE,
  legend = 'right'
)
ggsave("Results/graphe_AB_tot.png", plot = graphe_AB_tot, dpi = 300,height = 6,width = 7)




df_tot = RF_df_AB_tot
df_tot$observation = seq(1,nrow(df_tot))



# Calcul des quartiles
q1 <- quantile(df_tot$Observed, 0.25)
median <- quantile(df_tot$Observed, 0.50)
q3 <- quantile(df_tot$Observed, 0.75)
max_value <- max(df_tot$Observed)

# Création des DataFrames en fonction des quartiles
df1 <- df_tot[df_tot$Observed <= q1,]
df2 <- df_tot[df_tot$Observed > q1 & df_tot$Observed <= median,]
df3 <- df_tot[df_tot$Observed > median & df_tot$Observed <= q3,]
df4 <- df_tot[df_tot$Observed > q3,]




AB_tot_p1 = plot_comp(df = df1,ylabel = "",title_class = "  min to Q1",legende = TRUE,xlabel = "",title = "RF: Abundance predicted and Observed valuesss \n for different quartiles")

AB_tot_p2 = plot_comp(df = df2,ylabel = "" ,title_class = "Q1 to median",legende = FALSE,xlabel = "")
AB_tot_p3 = plot_comp(df = df3,ylabel = "" ,title_class = "median to Q3" ,legende = FALSE,xlabel = "")
AB_tot_p4 = plot_comp(df = df4,ylabel = "" ,title_class = " Q3 to max" ,legende = FALSE)


RF_AB_tot_fig = ggarrange(AB_tot_p1, AB_tot_p2, AB_tot_p3, AB_tot_p4,
  # labels = c('(a)', '(b)','(c)', '(d)'),
  ncol = 1,vjust = 0.5,
  common.legend = TRUE,
  legend = 'right'
)

ggsave("Results/RF_AB_tot_fig.png", plot = RF_AB_tot_fig, dpi = 300,height = 8)


# df_tot$diff = abs(df_tot$Observed - df_tot$Predicted)
# df_best = df_tot[df_tot$diff<=15,]
# 
# plot_comp(df = df_best,ylabel = "Abundance" ,title_class = "     Best prediction",legende = TRUE,plotly = TRUE)



```


<p align="center">
  <img src="Results/graphe_AB_tot.png">
</p>


**The best algorithm for total abundance is: RF**

<!-- <p align="center"> -->
<!--   <img src="Results/RF_AB_tot_fig.png"> -->
<!-- </p> -->


## Prediction of total biomass
```{r}
subtitle <- sprintf("Biomass : %.2f ± %.2f g/m²", mean(data_deep$BM_tot, na.rm = TRUE), sd(data_deep$BM_tot, na.rm = TRUE))
```


```{r predit BM_tot , fig.align='center' ,}
# Prediction avec GLM -----------------------------------------
GLM_result_BM_tot = GLM(var_rep ="BM_tot", 
                             df_app=BM_tot_train, 
                             df_valid = BM_tot_test,
                             family = 'gaussian')
# GLM_result_BM_tot$RMSE
# GLM_result_BM_tot$MAE
# GLM_result_BM_tot$R_adj_train
# GLM_result_BM_tot$R_adj_test
# GLM_result_BM_tot$predit
# GLM_result_BM_tot$model



GLM_BM_tot_pred <- GLM_result_BM_tot$predit^2

GLM_df_BM_tot = data.frame(Observed=BM_tot_test[,1]^2,Predicted = GLM_BM_tot_pred)

cor_GLM_BM_tot <- cor(GLM_df_BM_tot$Observed, GLM_df_BM_tot$Predicted)

  # graphique avec ggplot
GLM_BM_tot <- ggplot(GLM_df_BM_tot, aes(x = Observed, y = Predicted)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = TRUE, color = "red") + 
      labs(subtitle =paste0(" GLM: R² adj (train) = ", round(GLM_result_BM_tot$R_adj_train,2),
                           "; \n R² adj (test) = ", round(GLM_result_BM_tot$R_adj_test,2),
                           "; \n RMSE (test) = ",  round(GLM_result_BM_tot$RMSE^2,2)),
                            x = "Observed valuess", 
                            y = "Predicted values") +
      theme_classic() 

# Prediction avec GAM -----------------------------------------
GAM_result_BM_tot = GAM(var_rep ="BM_tot", 
                             df_app=BM_tot_train, 
                             df_valid = BM_tot_test,
                             family = 'gaussian',
                             method = "REML")
# GAM_result_BM_tot$RMSE
# GAM_result_BM_tot$MAE
# GAM_result_BM_tot$R_adj_train
# GAM_result_BM_tot$R_adj_test
# GAM_result_BM_tot$predit
# GAM_result_BM_tot$model

# mod_gam1_ab = GAM_result_BM_tot$model
# cv <- gam.check(GAM_result_BM_tot$model)
# print(cv)
# plot(mod_gam1_ab, pages = 1, seWithMean = TRUE)
# plot(mod_gam1_ab, residuals = TRUE, pch = 1)
# plot(ggeffects::ggpredict(mod_gam1_ab), facets = TRUE)
# gratia::draw(mod_gam1_ab, residuals = TRUE)
# # Verification
# par(mfrow = c(2, 2))
# gam.check(mod_gam1_ab)
# shapiro.test(mod_gam1_ab$res)
# concurvity(mod_gam1_ab,full = TRUE)
# concurvity(mod_gam1_ab,full = FALSE)





GAM_BM_tot_pred <- GAM_result_BM_tot$predit^2

GAM_df_BM_tot = data.frame(Observed=BM_tot_test[,1]^2,Predicted = GAM_BM_tot_pred)

cor_GAM_BM_tot <- cor(GAM_df_BM_tot$Observed, GAM_df_BM_tot$Predicted)

  # graphique avec ggplot
GAM_BM_tot <- ggplot(GAM_df_BM_tot, aes(x = Observed, y = Predicted)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = TRUE, color = "red") + 
      labs(subtitle =paste0(" GAM: R² adj (train) = ", round(GAM_result_BM_tot$R_adj_train,2),
                           "; \n R² adj (test) = ", round(GAM_result_BM_tot$R_adj_test,2),
                           "; \n RMSE (test) = ",  round(GAM_result_BM_tot$RMSE^2,2)),
                            x = "Observed valuess", 
                            y = "Predicted values") +
      theme_classic() 

# Prediction avec RF -----------------------------------------
BM_tot_RF_tuning = read.csv2("results_tuning/BM_tot_RF_tuning.csv")


BM_tot_RF_tuning = as.data.frame(BM_tot_RF_tuning)
BM_tot_RF_tuning = BM_tot_RF_tuning %>% arrange(mae)
# head(BM_tot_RF_tuning)

BM_tot_best_param = BM_tot_RF_tuning[1,]

# plot(seq(1:nrow(BM_tot_RF_tuning)), BM_tot_RF_tuning$r_squared)
# Best hyperparameter values
BM_tot_best_mtry = BM_tot_best_param$mtry
BM_tot_best_ntree = BM_tot_best_param$ntree
BM_tot_best_maxnodes = BM_tot_best_param$maxnode


RF_result_BM_tot = ForetAlea(var_rep ="BM_tot", 
                             df_app=BM_tot_train, 
                             df_valid = BM_tot_test,
                             mtry = BM_tot_best_mtry,
                             ntree= BM_tot_best_ntree,
                             maxnodes = NULL)

# RF_result_BM_tot$RMSE
# RF_result_BM_tot$MAE
# RF_result_BM_tot$R_adj_train
# RF_result_BM_tot$R_adj_test
# RF_result_BM_tot$predit
# RF_result_BM_tot$model


RF_BM_tot_pred <- RF_result_BM_tot$predit^2

RF_df_BM_tot = data.frame(Observed=BM_tot_test[,1]^2,Predicted = RF_BM_tot_pred)

cor_RF_BM_tot <- cor(RF_df_BM_tot$Observed, RF_df_BM_tot$Predicted)

  # graphique avec ggplot
RF_BM_tot <- ggplot(RF_df_BM_tot, aes(x = Observed, y = Predicted)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = TRUE, color = "red") + 
      labs(subtitle =paste0(" RF: R² adj (train) = ", round(RF_result_BM_tot$R_adj_train,2), 
                           "; \n R² adj (test) = ", round(RF_result_BM_tot$R_adj_test,2),
                           "; \n RMSE (test) = ",  round(RF_result_BM_tot$RMSE^2,2)),
                            x = "Observed valuess", 
                            y = "Predicted values") + 
      theme_classic() 
best_algo_BM_tot = RF_BM_tot
# Prediction avec GBM -----------------------------------------
BM_tot_GBM_tuning = read.csv2("results_tuning/BM_tot_GBM_tuning.csv")


BM_tot_GBM_tuning = as.data.frame(BM_tot_GBM_tuning)
BM_tot_GBM_tuning = BM_tot_GBM_tuning %>% arrange(mae)
# head(BM_tot_GBM_tuning)
BM_tot_best_param = BM_tot_GBM_tuning[1,]


# Best hyperparameter values
BM_tot_best_n.trees = BM_tot_best_param$n.trees
BM_tot_best_shrinkage = BM_tot_best_param$shrinkage
BM_tot_best_interaction.depth = BM_tot_best_param$interaction.depth
BM_tot_best_n.minobsinnode = BM_tot_best_param$n.minobsinnode


GBM_result_BM_tot =  GBM(var_rep ="BM_tot", 
                         df_app=BM_tot_train, 
                         df_valid = BM_tot_test,
                         distribution = 'gaussian',
                         n.trees = BM_tot_best_n.trees,
                         shrinkage = BM_tot_best_shrinkage,
                         interaction.depth = BM_tot_best_interaction.depth,
                         n.minobsinnode = BM_tot_best_n.minobsinnode)

# GBM_result_BM_tot$RMSE
# GBM_result_BM_tot$MAE
# GBM_result_BM_tot$R_adj_train
# GBM_result_BM_tot$R_adj_test
# GBM_result_BM_tot$predit
# GBM_result_BM_tot$model


# summary(GBM_result_BM_tot$model)
# best.iter <- gbm.perf(GBM_result_BM_tot$model, method = "cv")
# summary(GBM_result_BM_tot$model, n.trees = best.iter)




GBM_BM_tot_pred = GBM_result_BM_tot$predit^2

GBM_df_BM_tot = data.frame(Observed=BM_tot_test[,1]^2,Predicted = GBM_BM_tot_pred)

cor_GBM_BM_tot<- cor(GBM_df_BM_tot$Observed, GBM_df_BM_tot$Predicted)

# graphique avec ggplot
GBM_BM_tot <- ggplot(GBM_df_BM_tot, aes(x = Observed, y = Predicted)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = TRUE, color = "red") + 
      labs(subtitle =paste0(" GBM: R² adj (train) = ", round(GBM_result_BM_tot$R_adj_train,2),
                           "; \n R² adj (test) = ", round(GBM_result_BM_tot$R_adj_test,2),
                           "; \n RMSE (test) = ",  round(GBM_result_BM_tot$RMSE^2,2)),
                            x = "Observed valuess", 
                            y = "Predicted values") +
      theme_classic() 





# Prediction avec ANN -----------------------------------------
# ANN_tune_BM_tot_results$mse
# ANN_tune_BM_tot_results$mae
# ANN_tune_BM_tot_results$rmse
# ANN_tune_BM_tot_results$r_adj_train
# ANN_tune_BM_tot_results$r_adj_test


ANN_BM_tot_pred = ANN_tune_BM_tot_pred^2

ANN_df_BM_tot = data.frame(Observed=BM_tot_test[,1]^2,Predicted = ANN_BM_tot_pred)

cor_ANN_BM_tot <- cor(ANN_df_BM_tot$Observed, ANN_df_BM_tot$Predicted)

  # graphique avec ggplot
ANN_BM_tot <- ggplot(ANN_df_BM_tot, aes(x = Observed, y = Predicted)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = TRUE, color = "red") +
labs(subtitle =paste0(" ANN: R² adj (train) = ", round(ANN_tune_BM_tot_results$R_adj_train,2),
                           "; \n R² adj (test) = ", round(ANN_tune_BM_tot_results$R_adj_test,2),
                           "; \n RMSE (test) = ",  round(ANN_tune_BM_tot_results$rmse^2,2)),
                            x = "Observed valuess", 
                            y = "Predicted values") +
      theme_classic()

# best algo --------------------------------------
graphe_BM_tot = ggarrange(GLM_BM_tot, GAM_BM_tot, RF_BM_tot, GBM_BM_tot, ANN_BM_tot,
  labels = c('(a)', '(b)','(c)', '(d)','(e)'),widths = 10,
  common.legend = TRUE,
  legend = 'right'
)
ggsave("Results/graphe_BM_tot.png", plot = graphe_BM_tot, dpi = 300, height = 6,width = 7)




df_tot = RF_df_BM_tot
df_tot$observation = seq(1,nrow(df_tot))

# Calcul des quartiles
q1 <- quantile(df_tot$Observed, 0.25)
median <- quantile(df_tot$Observed, 0.50)
q3 <- quantile(df_tot$Observed, 0.75)
max_value <- max(df_tot$Observed)

# Création des DataFrames en fonction des quartiles
df1 <- df_tot[df_tot$Observed <= q1,]
df2 <- df_tot[df_tot$Observed > q1 & df_tot$Observed <= median,]
df3 <- df_tot[df_tot$Observed > median & df_tot$Observed <= q3,]
df4 <- df_tot[df_tot$Observed > q3,]




BM_tot_p1 = plot_comp(df = df1,ylabel = "",title_class = "  min to Q1",legende = TRUE,xlabel = "",title = "RF: Biomass predicted and Observed valuesss \n for different quartiles")

BM_tot_p2 = plot_comp(df = df2,ylabel = "" ,title_class = "Q1 to median",legende = FALSE,xlabel = "")
BM_tot_p3 = plot_comp(df = df3,ylabel = "" ,title_class = "median to Q3" ,legende = FALSE,xlabel = "")
BM_tot_p4 = plot_comp(df = df4,ylabel = "" ,title_class = " Q3 to max" ,legende = FALSE)


RF_BM_tot_fig = ggarrange(BM_tot_p1, BM_tot_p2, BM_tot_p3, BM_tot_p4,
  # labels = c('(a)', '(b)','(c)', '(d)'),
  ncol = 1,vjust = 0.5,
  common.legend = TRUE,
  legend = 'right'
)


ggsave("Results/RF_BM_tot_fig.png", plot = RF_BM_tot_fig, dpi = 300,height = 8)



# df_tot$diff = abs(df_tot$Observed - df_tot$Predicted)
# df_best = df_tot[df_tot$diff<=15,]
# 
# plot_comp(df = df_best,ylabel = "Biomass" ,title_class = "     Best prediction",legende = TRUE,plotly = TRUE)
```


<p align="center">
  <img src="Results/graphe_BM_tot.png">
</p>


**The best algorithm for total Biomass is: RF**

<!-- <p align="center"> -->
<!--   <img src="Results/RF_BM_tot_fig.png"> -->
<!-- </p> -->


## Prediction of total taxonomic richness

```{r}
subtitle <- sprintf("Richness : %.2f ± %.2f", round(mean(data_deep$Richesse_tot, na.rm = TRUE)), round(sd(data_deep$Richesse_tot, na.rm = TRUE)))
```


```{r predit Richesse_tot , fig.align='center', }
# Prediction avec GLM -----------------------------------------
GLM_result_Richesse_tot = GLM(var_rep ="Richesse_tot", 
                             df_app=Richesse_tot_train, 
                             df_valid = Richesse_tot_test,
                             family = 'gaussian')
# GLM_result_Richesse_tot$RMSE
# GLM_result_Richesse_tot$MAE
# GLM_result_Richesse_tot$R_adj_train
# GLM_result_Richesse_tot$R_adj_test
# GLM_result_Richesse_tot$predit
# GLM_result_Richesse_tot$model



GLM_Richesse_tot_pred <- GLM_result_Richesse_tot$predit

GLM_df_Richesse_tot = data.frame(Observed=Richesse_tot_test[,1],Predicted = GLM_Richesse_tot_pred)

cor_GLM_Richesse_tot <- cor(GLM_df_Richesse_tot$Observed, GLM_df_Richesse_tot$Predicted)

  # graphique avec ggplot
GLM_Richesse_tot <- ggplot(GLM_df_Richesse_tot, aes(x = Observed, y = Predicted)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = TRUE, color = "red") + 
      labs(subtitle =paste0(" GLM: R² adj (train) = ", round(GLM_result_Richesse_tot$R_adj_train,2),
                           "; \n R² adj (test) = ", round(GLM_result_Richesse_tot$R_adj_test,2),
                           "; \n RMSE (test) = ",  round(GLM_result_Richesse_tot$RMSE,2)),
                            x = "Observed valuess", 
                            y = "Predicted values") +
      theme_classic() 

# Prediction avec GAM -----------------------------------------
GAM_result_Richesse_tot = GAM(var_rep ="Richesse_tot", 
                             df_app=Richesse_tot_train, 
                             df_valid = Richesse_tot_test,
                             family = 'gaussian',
                             method = "REML")
# GAM_result_Richesse_tot$RMSE
# GAM_result_Richesse_tot$MAE
# GAM_result_Richesse_tot$R_adj_train
# GAM_result_Richesse_tot$R_adj_test
# GAM_result_Richesse_tot$predit
# GAM_result_Richesse_tot$model

# mod_gam1_ab = GAM_result_Richesse_tot$model
# cv <- gam.check(GAM_result_Richesse_tot$model)
# print(cv)
# plot(mod_gam1_ab, pages = 1, seWithMean = TRUE)
# plot(mod_gam1_ab, residuals = TRUE, pch = 1)
# plot(ggeffects::ggpredict(mod_gam1_ab), facets = TRUE)
# gratia::draw(mod_gam1_ab, residuals = TRUE)
# # Verification
# par(mfrow = c(2, 2))
# gam.check(mod_gam1_ab)
# shapiro.test(mod_gam1_ab$res)
# concurvity(mod_gam1_ab,full = TRUE)
# concurvity(mod_gam1_ab,full = FALSE)





GAM_Richesse_tot_pred <- GAM_result_Richesse_tot$predit

GAM_df_Richesse_tot = data.frame(Observed=Richesse_tot_test[,1],Predicted = GAM_Richesse_tot_pred)

cor_GAM_Richesse_tot <- cor(GAM_df_Richesse_tot$Observed, GAM_df_Richesse_tot$Predicted)

  # graphique avec ggplot
GAM_Richesse_tot <- ggplot(GAM_df_Richesse_tot, aes(x = Observed, y = Predicted)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = TRUE, color = "red") + 
      labs(subtitle =paste0(" GAM: R² adj (train) = ", round(GAM_result_Richesse_tot$R_adj_train,2),
                           "; \n R² adj (test) = ", round(GAM_result_Richesse_tot$R_adj_test,2),
                           "; \n RMSE (test) = ",  round(GAM_result_Richesse_tot$RMSE,2)),
                            x = "Observed valuess", 
                            y = "Predicted values") +
      theme_classic() 

# Prediction avec RF -----------------------------------------
Richesse_tot_RF_tuning = read.csv2("results_tuning/Richesse_tot_RF_tuning.csv")


Richesse_tot_RF_tuning = as.data.frame(Richesse_tot_RF_tuning)
Richesse_tot_RF_tuning = Richesse_tot_RF_tuning %>% arrange(mae)
# head(Richesse_tot_RF_tuning)

Richesse_tot_best_param = Richesse_tot_RF_tuning[1,]

# plot(seq(1:nrow(Richesse_tot_RF_tuning)), Richesse_tot_RF_tuning$r_squared)
# Best hyperparameter values
Richesse_tot_best_mtry = Richesse_tot_best_param$mtry
Richesse_tot_best_ntree = Richesse_tot_best_param$ntree
Richesse_tot_best_maxnodes = Richesse_tot_best_param$maxnode


RF_result_Richesse_tot = ForetAlea(var_rep ="Richesse_tot", 
                             df_app=Richesse_tot_train, 
                             df_valid = Richesse_tot_test,
                             mtry = Richesse_tot_best_mtry,
                             ntree= Richesse_tot_best_ntree,
                             maxnodes = Richesse_tot_best_maxnodes)

# RF_result_Richesse_tot$RMSE
# RF_result_Richesse_tot$MAE
# RF_result_Richesse_tot$R_adj_train
# RF_result_Richesse_tot$R_adj_test
# RF_result_Richesse_tot$predit
# RF_result_Richesse_tot$model


RF_Richesse_tot_pred <- RF_result_Richesse_tot$predit

RF_df_Richesse_tot = data.frame(Observed=Richesse_tot_test[,1],Predicted = RF_Richesse_tot_pred)

cor_RF_Richesse_tot <- cor(RF_df_Richesse_tot$Observed, RF_df_Richesse_tot$Predicted)

  # graphique avec ggplot
RF_Richesse_tot <- ggplot(RF_df_Richesse_tot, aes(x = Observed, y = Predicted)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = TRUE, color = "red") + 
      labs(subtitle =paste0(" RF: R² adj (train) = ", round(RF_result_Richesse_tot$R_adj_train,2), 
                           "; \n R² adj (test) = ", round(RF_result_Richesse_tot$R_adj_test,2),
                           "; \n RMSE (test) = ",  round(RF_result_Richesse_tot$RMSE,2)),
                            x = "Observed valuess", 
                            y = "Predicted values") + 
      theme_classic() 
best_algo_Richesse_tot = RF_Richesse_tot

# Prediction avec GBM -----------------------------------------
Richesse_tot_GBM_tuning = read.csv2("results_tuning/Richesse_tot_GBM_tuning.csv")


Richesse_tot_GBM_tuning = as.data.frame(Richesse_tot_GBM_tuning)
Richesse_tot_GBM_tuning = Richesse_tot_GBM_tuning %>% arrange(mae)
# head(Richesse_tot_GBM_tuning)
Richesse_tot_best_param = Richesse_tot_GBM_tuning[1,]


# Best hyperparameter values
Richesse_tot_best_n.trees = Richesse_tot_best_param$n.trees
Richesse_tot_best_shrinkage = Richesse_tot_best_param$shrinkage
Richesse_tot_best_interaction.depth = Richesse_tot_best_param$interaction.depth
Richesse_tot_best_n.minobsinnode = Richesse_tot_best_param$n.minobsinnode


GBM_result_Richesse_tot =  GBM(var_rep ="Richesse_tot", 
                         df_app=Richesse_tot_train, 
                         df_valid = Richesse_tot_test,
                         distribution = 'gaussian',
                         n.trees = Richesse_tot_best_n.trees,
                         shrinkage = Richesse_tot_best_shrinkage,
                         interaction.depth = Richesse_tot_best_interaction.depth,
                         n.minobsinnode = Richesse_tot_best_n.minobsinnode)

# GBM_result_Richesse_tot$RMSE
# GBM_result_Richesse_tot$MAE
# GBM_result_Richesse_tot$R_adj_train
# GBM_result_Richesse_tot$R_adj_test
# GBM_result_Richesse_tot$predit
# GBM_result_Richesse_tot$model


# summary(GBM_result_Richesse_tot$model)
# best.iter <- gbm.perf(GBM_result_Richesse_tot$model, method = "cv")
# summary(GBM_result_Richesse_tot$model, n.trees = best.iter)




GBM_Richesse_tot_pred = GBM_result_Richesse_tot$predit

GBM_df_Richesse_tot = data.frame(Observed=Richesse_tot_test[,1],Predicted = GBM_Richesse_tot_pred)

cor_GBM_Richesse_tot<- cor(GBM_df_Richesse_tot$Observed, GBM_df_Richesse_tot$Predicted)

# graphique avec ggplot
GBM_Richesse_tot <- ggplot(GBM_df_Richesse_tot, aes(x = Observed, y = Predicted)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = TRUE, color = "red") + 
      labs(subtitle =paste0(" GBM: R² adj (train) = ", round(GBM_result_Richesse_tot$R_adj_train,2),
                           "; \n R² adj (test) = ", round(GBM_result_Richesse_tot$R_adj_test,2),
                           "; \n RMSE (test) = ",  round(GBM_result_Richesse_tot$RMSE,2)),
                            x = "Observed valuess", 
                            y = "Predicted values") +
      theme_classic() 





# Prediction avec ANN -----------------------------------------
# ANN_tune_Richesse_tot_results$mse
# ANN_tune_Richesse_tot_results$mae
# ANN_tune_Richesse_tot_results$rmse
# ANN_tune_Richesse_tot_results$r_adj_train
# ANN_tune_Richesse_tot_results$r_adj_test


ANN_Richesse_tot_pred = ANN_tune_Richesse_tot_pred

ANN_df_Richesse_tot = data.frame(Observed=Richesse_tot_test[,1],Predicted = ANN_Richesse_tot_pred)

cor_ANN_Richesse_tot <- cor(ANN_df_Richesse_tot$Observed, ANN_df_Richesse_tot$Predicted)

  # graphique avec ggplot
ANN_Richesse_tot <- ggplot(ANN_df_Richesse_tot, aes(x = Observed, y = Predicted)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = TRUE, color = "red") +
labs(subtitle =paste0(" ANN: R² adj (train) = ", round(ANN_tune_Richesse_tot_results$R_adj_train,2),
                           "; \n R² adj (test) = ", round(ANN_tune_Richesse_tot_results$R_adj_test,2),
                           "; \n RMSE (test) = ",  round(ANN_tune_Richesse_tot_results$rmse,2)),
                            x = "Observed valuess", 
                            y = "Predicted values") +
      theme_classic()

# best algo --------------------------------------
graphe_Richesse_tot = ggarrange(GLM_Richesse_tot, GAM_Richesse_tot, RF_Richesse_tot, GBM_Richesse_tot, ANN_Richesse_tot,
  labels = c('(a)', '(b)','(c)', '(d)','(e)'),widths = 10,
  common.legend = TRUE,
  legend = 'right'
)
ggsave("Results/graphe_Richesse_tot.png", plot = graphe_Richesse_tot, dpi = 300, height = 6,width = 7)




df_tot = RF_df_Richesse_tot
df_tot$observation = seq(1,nrow(df_tot))

df_tot$Predicted = round(df_tot$Predicted)
# Calcul des quartiles
q1 <- quantile(df_tot$Observed, 0.25)
median <- quantile(df_tot$Observed, 0.50)
q3 <- quantile(df_tot$Observed, 0.75)
max_value <- max(df_tot$Observed)

# Création des DataFrames en fonction des quartiles
df1 <- df_tot[df_tot$Observed <= q1,]
df2 <- df_tot[df_tot$Observed > q1 & df_tot$Observed <= median,]
df3 <- df_tot[df_tot$Observed > median & df_tot$Observed <= q3,]
df4 <- df_tot[df_tot$Observed > q3,]




Richesse_tot_p1 = plot_comp(df = df1,ylabel = "",title_class = "  min to Q1",legende = TRUE,xlabel = "",title = "RF: Richness predicted and Observed valuesss \n for different quartiles")

Richesse_tot_p2 = plot_comp(df = df2,ylabel = "" ,title_class = "Q1 to median",legende = FALSE,xlabel = "")
Richesse_tot_p3 = plot_comp(df = df3,ylabel = "" ,title_class = "median to Q3" ,legende = FALSE,xlabel = "")
Richesse_tot_p4 = plot_comp(df = df4,ylabel = "" ,title_class = " Q3 to max" ,legende = FALSE)


GBM_Richesse_tot_fig = ggarrange(Richesse_tot_p1, Richesse_tot_p2, Richesse_tot_p3, Richesse_tot_p4,
  # labels = c('(a)', '(b)','(c)', '(d)'),
  ncol = 1,vjust = 0.5,
  common.legend = TRUE,
  legend = 'right'
)

ggsave("Results/RF_Richesse_tot_fig.png", plot = GBM_Richesse_tot_fig, dpi = 300,height = 8)


# df_tot$diff = abs(df_tot$Observed - df_tot$Predicted)
# df_best = df_tot[df_tot$diff<=0.5,]
# 
# 
# plot_comp(df = df_best,ylabel = "Richness" ,title_class = "     Best prediction",legende = TRUE,plotly = FALSE)
```


<p align="center">
  <img src="Results/graphe_Richesse_tot.png">
</p>


**The best algorithm for total Richness is: RF **

<!-- <p align="center"> -->
<!--   <img src="Results/RF_Richesse_tot_fig.png"> -->
<!-- </p> -->

## Best algo: RF

**-   Summary: ** *Results Case 1 -> repeated data*
<br/> 
```{r saving best algo1,fig.align='center'}
best_algo_RF_1 = ggpubr::ggarrange(best_algo_AB_tot, best_algo_BM_tot, best_algo_Richesse_tot,
                          labels = c('(a)', '(b)','(c)'),ncol = 3,
                          common.legend = TRUE,
                          legend = 'right'
)
ggsave("Results/best_algo_RF_1.png", plot = best_algo_RF_1, dpi = 300,height = 3,width = 7)
```

<p align="center">
  <img src="Results/best_algo_RF_1.png">
</p>



## Transformations effect

- Data without transformations
<p align="center">
  <img src="Results/non_transformation/all_fig_RF.png">
</p>

- Data transformed with *sqrt()*
<p align="center">
  <img src="Results/best_algo_RF_1.png">
</p>






## Comparaison data partition
```{r}
# voir scrip select_var.R
```

**Abundance partition**
<p align="center">
  <img src="Results/kenSton_partition/AB_tot_sample_fig.png">
</p>

- Comparison with kenStone method

<p align="center">
  <img src="Results/kenSton_partition/AB_tot_kenStone_fig.png">
</p>


**CaCO3 partition**
<p align="center">
  <img src="Results/kenSton_partition/CaCO3_sample_fig.png">
</p>

- Comparison with kenStone method

<p align="center">
  <img src="Results/kenSton_partition/CaCO3_kenStone_fig.png">
</p>

**Kolmogorov-Smirnov test**

-   sample methode
```{r}
### model **R² adj RF abundance (train, test)**
AB_tot_train = read.csv2("datas/AB_tot_train.csv")
AB_tot_test = read.csv2("datas/AB_tot_test.csv")
RF_result_AB_tot = ForetAlea(var_rep ="AB_tot", 
                             df_app=AB_tot_train, 
                             df_valid = AB_tot_test,
                             mtry = AB_tot_best_mtry,
                             ntree= AB_tot_best_ntree,
                             maxnodes = AB_tot_best_maxnodes)
# RF_result_AB_tot$R_adj_train
# RF_result_AB_tot$R_adj_test


# Test de Kolmogorov-Smirnov: # Do x and y come from the same distribution?
ks.test(AB_tot_train$AB_tot, AB_tot_test$AB_tot)

```

-   kenStone methode
```{r}
#   **R² adj RF kenStone abundance (train, test)**
AB_tot_train = read.csv2("datas/AB_tot_train.csv")
AB_tot_test = read.csv2("datas/AB_tot_test.csv")
df_AB_tot = rbind(AB_tot_train,AB_tot_test)
partition = kenStone(df_AB_tot,k= round (nrow(AB_tot_train)))
AB_tot_train = df_AB_tot[partition$model,]
AB_tot_test = df_AB_tot[partition$test,]
RF_result_AB_tot = ForetAlea(var_rep ="AB_tot", 
                             df_app=AB_tot_train, 
                             df_valid = AB_tot_test,
                             mtry = AB_tot_best_mtry,
                             ntree= AB_tot_best_ntree,
                             maxnodes = AB_tot_best_maxnodes)
# RF_result_AB_tot$R_adj_train
# RF_result_AB_tot$R_adj_test
# plot(RF_result_AB_tot$predit)
# 
# kable(data.frame(Observed = AB_tot_test$AB_tot, Predicted =RF_result_AB_tot$predit))


# Test de Kolmogorov-Smirnov: # Do x and y come from the same distribution?
ks.test(AB_tot_train$AB_tot, AB_tot_test$AB_tot)
```



## Polynomial model results

<p align="center">
  <img src="Results/POLY/all_graphe_poly.png">
</p>

- Comparison with the best algorithms

<p align="center">
  <img src="Results/best_algo_RF_1.png">
</p>



## Improved prediction of extreme values

```{r increase prediction}
### model AB_tot
AB_tot_train = read.csv2("datas/AB_tot_train.csv")
AB_tot_test = read.csv2("datas/AB_tot_test.csv")
RF_result_AB_tot = ForetAlea(var_rep ="AB_tot", 
                             df_app=AB_tot_train, 
                             df_valid = AB_tot_test,
                             mtry = AB_tot_best_mtry,
                             ntree= AB_tot_best_ntree,
                             maxnodes = NULL)
# RF_result_AB_tot$RMSE
# RF_result_AB_tot$MAE
# RF_result_AB_tot$R_adj_train
# RF_result_AB_tot$R_adj_test
# # RF_result_AB_tot$predit
# RF_result_AB_tot$model

RF_AB_tot_pred <- RF_result_AB_tot$predit^2
RF_df_AB_tot = data.frame(Observed=AB_tot_test[,1]^2,Predicted = RF_AB_tot_pred)
cor_RF_AB_tot <- cor(RF_df_AB_tot$Observed, RF_df_AB_tot$Predicted)

# summary(RF_df_AB_tot)
  # graphique avec ggplot
RF_AB_tot <- ggplot(RF_df_AB_tot, aes(x = Observed, y = Predicted)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = TRUE, color = "red") + 
      labs(subtitle =paste0(" RF: R² adj (train) = ", round(RF_result_AB_tot$R_adj_train,2), 
                           "; \n R² adj (test) = ", round(RF_result_AB_tot$R_adj_test,2),
                           "; \n RMSE (test) = ",  round(RF_result_AB_tot$RMSE^2,2)),
                            x = "Observed valuess", 
                            y = "Predicted values") + 
      theme_classic()+ 
    scale_x_continuous(limits = c(0,1000),breaks = seq(0, 800, by = 200)) +
  scale_y_continuous(limits = c(0,800),breaks = seq(0, 800, by = 200)) 

RF_AB_tot_best2 = RF_AB_tot


### model BM_tot
BM_tot_train = read.csv2("datas/BM_tot_train.csv")
BM_tot_test = read.csv2("datas/BM_tot_test.csv")
RF_result_BM_tot = ForetAlea(var_rep ="BM_tot", 
                             df_app=BM_tot_train, 
                             df_valid = BM_tot_test,
                             mtry = BM_tot_best_mtry,
                             ntree= BM_tot_best_ntree,
                             maxnodes = NULL)
# RF_result_BM_tot$RMSE
# RF_result_BM_tot$MAE
# RF_result_BM_tot$R_adj_train
# RF_result_BM_tot$R_adj_test
# # RF_result_BM_tot$predit
# RF_result_BM_tot$model

RF_BM_tot_pred <- RF_result_BM_tot$predit^2
RF_df_BM_tot = data.frame(Observed=BM_tot_test[,1]^2,Predicted = RF_BM_tot_pred)
cor_RF_BM_tot <- cor(RF_df_BM_tot$Observed, RF_df_BM_tot$Predicted)

# summary(RF_df_BM_tot)
  # graphique avec ggplot
RF_BM_tot <- ggplot(RF_df_BM_tot, aes(x = Observed, y = Predicted)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = TRUE, color = "red") + 
      labs(subtitle =paste0(" RF: R² adj (train) = ", round(RF_result_BM_tot$R_adj_train,2), 
                           "; \n R² adj (test) = ", round(RF_result_BM_tot$R_adj_test,2),
                           "; \n RMSE (test) = ",  round(RF_result_BM_tot$RMSE^2,2)),
                            x = "Observed valuess", 
                            y = "Predicted values") + 
      theme_classic()

RF_BM_tot_best2 =RF_BM_tot

### model Richesse_tot
Richesse_tot_train = read.csv2("datas/Richesse_tot_train.csv")
Richesse_tot_test = read.csv2("datas/Richesse_tot_test.csv")
RF_result_Richesse_tot = ForetAlea(var_rep ="Richesse_tot", 
                             df_app=Richesse_tot_train, 
                             df_valid = Richesse_tot_test,
                             mtry = Richesse_tot_best_mtry,
                             ntree= Richesse_tot_best_ntree,
                             maxnodes = NULL)
# RF_result_Richesse_tot$RMSE
# RF_result_Richesse_tot$MAE
# RF_result_Richesse_tot$R_adj_train
# RF_result_Richesse_tot$R_adj_test
# # RF_result_Richesse_tot$predit
# RF_result_Richesse_tot$model

RF_Richesse_tot_pred <- RF_result_Richesse_tot$predit
RF_df_Richesse_tot = data.frame(Observed=Richesse_tot_test[,1],Predicted = RF_Richesse_tot_pred)
cor_RF_Richesse_tot <- cor(RF_df_Richesse_tot$Observed, RF_df_Richesse_tot$Predicted)

# summary(RF_df_Richesse_tot)
  # graphique avec ggplot
RF_Richesse_tot <- ggplot(RF_df_Richesse_tot, aes(x = Observed, y = Predicted)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = TRUE, color = "red") + 
      labs(subtitle =paste0(" RF: R² adj (train) = ", round(RF_result_Richesse_tot$R_adj_train,2), 
                           "; \n R² adj (test) = ", round(RF_result_Richesse_tot$R_adj_test,2),
                           "; \n RMSE (test) = ",  round(RF_result_Richesse_tot$RMSE,2)),
                            x = "Observed valuess", 
                            y = "Predicted values") + 
      theme_classic() + 
  scale_x_continuous(breaks = seq(0, 12, by = 3)) +
  scale_y_continuous(breaks = seq(0, 12, by = 3))

RF_Richesse_tot_best2 = RF_Richesse_tot

best_algo_RF_2 = ggarrange(RF_AB_tot, RF_BM_tot, RF_Richesse_tot,
                          labels = c('(a)', '(b)','(c)'),ncol = 3,
                          common.legend = TRUE,
                          legend = 'right'
)
ggsave("Results/best_algo_RF_2.png", plot = best_algo_RF_2, dpi = 300,height = 3,width = 7)

```

- best 1

<p align="center">
  <img src="Results/best_algo_RF_1.png">
</p>


- best 2

<p align="center">
  <img src="Results/best_algo_RF_2.png">
</p>


## Model without Elevation
<!-- -   Suppression Elevation -->
<!-- -   Remplacement de CaCO3 par le pH -->

```{r delete vars}
### model AB_tot
AB_tot_train = read.csv2("datas/AB_tot_train.csv")
AB_tot_test = read.csv2("datas/AB_tot_test.csv")
AB_tot_train$elevation = NULL
# AB_tot_train <- AB_tot_train %>%
#   dplyr::select(-elevation) 
# %>%  
#   rename(pH = CaCO3)
AB_tot_test$elevation = NULL
# AB_tot_test <- AB_tot_test %>%
#   dplyr::select( -elevation) 
# %>%
#   rename(pH = CaCO3)
RF_result_AB_tot = ForetAlea(var_rep ="AB_tot", 
                             df_app=AB_tot_train, 
                             df_valid = AB_tot_test,
                             mtry = AB_tot_best_mtry,
                             ntree= AB_tot_best_ntree,
                             maxnodes = NULL)
# RF_result_AB_tot$RMSE
# RF_result_AB_tot$MAE
# RF_result_AB_tot$R_adj_train
# RF_result_AB_tot$R_adj_test
# # RF_result_AB_tot$predit
# RF_result_AB_tot$model

RF_AB_tot_pred <- RF_result_AB_tot$predit^2
RF_df_AB_tot = data.frame(Observed=AB_tot_test[,1]^2,Predicted = RF_AB_tot_pred)
cor_RF_AB_tot <- cor(RF_df_AB_tot$Observed, RF_df_AB_tot$Predicted)

# summary(RF_df_AB_tot)
  # graphique avec ggplot
RF_AB_tot <- ggplot(RF_df_AB_tot, aes(x = Observed, y = Predicted)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = TRUE, color = "red") + 
      labs(subtitle =paste0(" RF: R² adj (train) = ", round(RF_result_AB_tot$R_adj_train,2), 
                           "; \n R² adj (test) = ", round(RF_result_AB_tot$R_adj_test,2),
                           "; \n RMSE (test) = ",  round(RF_result_AB_tot$RMSE^2,2)),
                            x = "Observed valuess", 
                            y = "Predicted values") + 
      theme_classic() 



### model BM_tot
BM_tot_train = read.csv2("datas/BM_tot_train.csv")
BM_tot_test = read.csv2("datas/BM_tot_test.csv")
# BM_tot_train <- BM_tot_train %>%
#   dplyr::select(-CEC, -pH) %>%  
#   rename(pH = CaCO3)  
# BM_tot_test <- BM_tot_test %>%
#   dplyr::select(-CEC, -pH) %>%  
#   rename(pH = CaCO3)
RF_result_BM_tot = ForetAlea(var_rep ="BM_tot", 
                             df_app=BM_tot_train, 
                             df_valid = BM_tot_test,
                             mtry = BM_tot_best_mtry,
                             ntree= BM_tot_best_ntree,
                             maxnodes = NULL)
# RF_result_BM_tot$RMSE
# RF_result_BM_tot$MAE
# RF_result_BM_tot$R_adj_train
# RF_result_BM_tot$R_adj_test
# # RF_result_BM_tot$predit
# RF_result_BM_tot$model

RF_BM_tot_pred <- RF_result_BM_tot$predit^2
RF_df_BM_tot = data.frame(Observed=BM_tot_test[,1]^2,Predicted = RF_BM_tot_pred)
cor_RF_BM_tot <- cor(RF_df_BM_tot$Observed, RF_df_BM_tot$Predicted)

# summary(RF_df_BM_tot)
  # graphique avec ggplot
RF_BM_tot <- ggplot(RF_df_BM_tot, aes(x = Observed, y = Predicted)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = TRUE, color = "red") + 
      labs(subtitle =paste0(" RF: R² adj (train) = ", round(RF_result_BM_tot$R_adj_train,2), 
                           "; \n R² adj (test) = ", round(RF_result_BM_tot$R_adj_test,2),
                           "; \n RMSE (test) = ",  round(RF_result_BM_tot$RMSE^2,2)),
                            x = "Observed valuess", 
                            y = "Predicted values") + 
      theme_classic()



### model Richesse_tot
Richesse_tot_train = read.csv2("datas/Richesse_tot_train.csv")
Richesse_tot_test = read.csv2("datas/Richesse_tot_test.csv")
Richesse_tot_train$elevation=NULL
# Richesse_tot_train <- Richesse_tot_train %>%
#   dplyr::select(-elevation) 
# %>%  
#   rename(pH = CaCO3) 
Richesse_tot_test$elevation=NULL
# Richesse_tot_test <- Richesse_tot_test %>%
#   dplyr::select(-elevation) 
# %>%  
#   rename(pH = CaCO3)
RF_result_Richesse_tot = ForetAlea(var_rep ="Richesse_tot", 
                             df_app=Richesse_tot_train, 
                             df_valid = Richesse_tot_test,
                             mtry = Richesse_tot_best_mtry,
                             ntree= Richesse_tot_best_ntree,
                             maxnodes = NULL)
# RF_result_Richesse_tot$RMSE
# RF_result_Richesse_tot$MAE
# RF_result_Richesse_tot$R_adj_train
# RF_result_Richesse_tot$R_adj_test
# # RF_result_Richesse_tot$predit
# RF_result_Richesse_tot$model

RF_Richesse_tot_pred <- RF_result_Richesse_tot$predit
RF_df_Richesse_tot = data.frame(Observed=Richesse_tot_test[,1],Predicted = RF_Richesse_tot_pred)
cor_RF_Richesse_tot <- cor(RF_df_Richesse_tot$Observed, RF_df_Richesse_tot$Predicted)

# summary(RF_df_Richesse_tot)
  # graphique avec ggplot
RF_Richesse_tot <- ggplot(RF_df_Richesse_tot, aes(x = Observed, y = Predicted)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = TRUE, color = "red") + 
      labs(subtitle =paste0(" RF: R² adj (train) = ", round(RF_result_Richesse_tot$R_adj_train,2), 
                           "; \n R² adj (test) = ", round(RF_result_Richesse_tot$R_adj_test,2),
                           "; \n RMSE (test) = ",  round(RF_result_Richesse_tot$RMSE,2)),
                            x = "Observed valuess", 
                            y = "Predicted values") + 
      theme_classic()


# delete_var -> delete CEC, elevation and pH, pH = CaCO3
RF_fig_delete_var = ggarrange(RF_AB_tot, RF_BM_tot, RF_Richesse_tot,
                          labels = c('(a)', '(b)','(c)'),ncol = 3,
                          common.legend = TRUE,
                          legend = 'right'
)
ggsave("Results/RF_fig_delete_var.png", plot = RF_fig_delete_var, dpi = 300,height = 3,width = 7)

```



<p align="center">
  <img src="Results/RF_fig_delete_var.png">
</p>


- best 2

<p align="center">
  <img src="Results/best_algo_RF_2.png">
</p>

# Réduction et effets des variables

```{r}
Predictors_f = c("CaCO3" ,"gps_x" ,"N" ,"bio3" ,"gps_y" ,"clay" ,
                 "silt" ,"clcm_lvl3" ,"bio4" ,"bio16" ,"PET" ,"P" ,"bio12" ,"bio1")
```

## Abundance

[Plan]

```{r}
AB_tot_train = read.csv2("datas/AB_tot_train.csv")
AB_tot_test = read.csv2("datas/AB_tot_test.csv")

# on supprime: 
AB_tot_var_sup=c("bio7","bio6", "bio4", "elevation","sand","hurs_mean", "bio15", "CEC","P", "gdd0")

RF_result_AB_tot = ForetAlea(var_rep ="AB_tot", 
  df_app=AB_tot_train[,!colnames(AB_tot_train) %in% AB_tot_var_sup ], 
  df_valid = AB_tot_test [,!colnames(AB_tot_train) %in% AB_tot_var_sup],
                             mtry = 3,
                             ntree= AB_tot_best_ntree,
                             maxnodes = NULL)
# RF_result_AB_tot$RMSE^2 
# RF_result_AB_tot$R_adj_train 
# RF_result_AB_tot$R_adj_test 
# RF_result_AB_tot$model

# var_importance <- data.frame(RF_result_AB_tot$model$importance)
# row.names(var_importance)[which.min(var_importance$IncNodePurity)]
# var_importance$nom=rownames(var_importance)
# var_importance <- var_importance[order(var_importance$IncNodePurity,decreasing = TRUE), ]
# var_importance

# on supprime: 
AB_tot_var_sup=c("bio7","bio6", "bio4", "elevation","sand","hurs_mean", "bio15", "CEC","P", "gdd0","silt")

RF_AB_tot_pred <- RF_result_AB_tot$predit^2
RF_df_AB_tot = data.frame(Observed=AB_tot_test[,1]^2,Predicted = RF_AB_tot_pred)
cor_RF_AB_tot <- cor(RF_df_AB_tot$Observed, RF_df_AB_tot$Predicted)

# summary(RF_df_AB_tot)
  # graphique avec ggplot
RF_AB_tot <- ggplot(RF_df_AB_tot, aes(x = Observed, y = Predicted)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = TRUE, color = "red") + 
      labs(title = "After",subtitle =paste0(" RF: R² adj (train) = ", round(RF_result_AB_tot$R_adj_train,2), 
                           "; \n R² adj (test) = ", round(RF_result_AB_tot$R_adj_test,2),
                           "; \n RMSE (test) = ",  round(RF_result_AB_tot$RMSE^2,2)),
                            x = "Observed valuess", 
                            y = "Predicted values") + 
      theme_classic()+ 
    scale_x_continuous(limits = c(0,1000),breaks = seq(0, 800, by = 200)) +
  scale_y_continuous(limits = c(0,800),breaks = seq(0, 800, by = 200)) 

# RF_AB_tot_best2
# RF_AB_tot
graphe = ggarrange(RF_AB_tot_best2, RF_AB_tot,
                          labels = c('(a)', '(b)'),ncol = 2,widths = 4.666667, heights = 3,common.legend = TRUE,legend = 'right')
graphe


# mod_AB_tot = RF_result_AB_tot$model
# varImpPlot(mod_AB_tot, main = "Abundance") #produce variable importance plot
# importance_rf <- as.data.frame(importance(mod_AB_tot))
# importance_rf$nom=rownames(importance_rf)
# importance_rf <- importance_rf[order(importance_rf$IncNodePurity,decreasing = TRUE), ]
# row.names(importance_rf)=NULL
# importance_rf$percent <- importance_rf$IncNodePurity / sum(importance_rf$IncNodePurity)*100
# os = c("clcm_lvl3v","clcm_lvl3nial","clcm_lvl3p","clcm_lvl3gua","clcm_lvl3mf","clcm_lvl3ng")
# 
# barplot(importance_rf$percent, main = "Importance of variables for total abundance", names.arg = importance_rf$nom, ylab = "Importance (%)", las = 2)
```

```{r abundance response, fig.align='center'}
#_______________
AB_tot_train = read.csv2("datas/AB_tot_train.csv")
AB_tot_var = names(AB_tot_train)

AB_tot_var_sup=c("bio7","bio6", "bio4", "elevation","sand","hurs_mean", "bio15", "CEC","P", "gdd0")

# AB_tot_var[!AB_tot_var %in% AB_tot_var_sup]

AB_tot_predictors = c("AB_tot","CaCO3","gps_x","N","bio3","gps_y","bio18","clay",
                      "silt","CN","clcm_lvl3")

AB_tot_predictors = c("AB_tot","CaCO3","gps_x","N","bio3","gps_y","bio18","argile.0_30",
                      "limon.0_30","CN","clcm_lvl3")

AB_tot_df = bdd[,AB_tot_predictors]
AB_tot_df <- AB_tot_df %>% 
  dplyr::rename(clay = argile.0_30)
AB_tot_df <- AB_tot_df %>% 
  dplyr::rename(silt =limon.0_30 )
# colSums(is.na(AB_tot_df))
AB_tot_df = drop_na(AB_tot_df)
AB_tot_df$clcm_lvl3 = as.factor(AB_tot_df$clcm_lvl3)
# summary(AB_tot_df$clcm_lvl3)
levels(AB_tot_df$clcm_lvl3)[levels(AB_tot_df$clcm_lvl3) == "Broad-leaved forest"] <- "Forest"
levels(AB_tot_df$clcm_lvl3)[levels(AB_tot_df$clcm_lvl3) == "Coniferous forest"] <- "Forest"
levels(AB_tot_df$clcm_lvl3)[levels(AB_tot_df$clcm_lvl3) == "Mixed forest"] <- "Forest"
AB_tot_df$clcm_lvl3= as.factor(AB_tot_df$clcm_lvl3)
cl_original <- levels(AB_tot_df$clcm_lvl3)
new_cl <- c("f","gua", "ng", "nial", "p", "v")
AB_tot_df$clcm_lvl3 <- factor(AB_tot_df$clcm_lvl3, levels = cl_original, labels = new_cl)



rf <- randomForest(AB_tot ~ ., data = AB_tot_df, ntree = 500)

# Utilisation du conteneur iml Predictor()
X <- AB_tot_df[which(names(AB_tot_df) != "AB_tot")]
predictor <- Predictor$new(rf, data = X, y = AB_tot_df$AB_tot)




cat("Importance of predictors")
# Importance des fonctionnalités
# On calcule l'importance de chaque caractéristique pour les prédictions avec FeatureImp. La mesure de l'importance des fonctionnalités fonctionne en mélangeant chaque fonctionnalité et en mesurant l'ampleur de la baisse des performances. Pour cette tâche de régression, nous choisissons de mesurer la perte de performance avec l'erreur absolue moyenne (« mae »), un autre choix serait l'erreur quadratique moyenne (« mse »).

imp <- FeatureImp$new(predictor, loss = "mae") # mean absolute error
# imp <- FeatureImp$new(predictor, loss = "mse") # mean squared error  
plot(imp)
# imp$results
# importance_rf <- as.data.frame(importance(rf))
# importance_rf$nom=rownames(importance_rf)
# importance_rf <- importance_rf[order(importance_rf$IncNodePurity,decreasing = TRUE), ]
# row.names(importance_rf)=NULL
# importance_rf$percent <- importance_rf$IncNodePurity / sum(importance_rf$IncNodePurity)*100
# barplot(importance_rf$percent, main = "Importance of variables for total abundance", names.arg = importance_rf$nom, ylab = "Importance (%)", las = 2)





cat("Predictor effects")
# Effets de fonctionnalités
# Les effets locaux accumulés décrivent comment les predicteurs influencent en moyenne la prédiction d'un modèle d'apprentissage automatique: ALE montre comment la prédiction change localement, lorsque les predicteurs varie. Les marques sur l'axe des x indiquent la distribution des predicteurs, montrant la pertinence d'une région pour l'interprétation (peu ou pas de points signifie que nous ne devons pas surinterpréter cette région).

# ale <- FeatureEffect$new(predictor, feature = "gps_x") # uniquement lstat
# ale$plot()
# ale$set.feature("rm")
# ale$plot()
effs <- FeatureEffects$new(predictor) # toutes les variables
plot(effs)






cat("Predictor interactions")
# Mesurer les interactions
# Nous pouvons également mesurer la force avec laquelle les fonctionnalités interagissent les unes avec les autres. La mesure d'interaction concerne la part de la variance de F(X) s’explique par l’interaction. La mesure est comprise entre 0 (pas d'interaction) et 1 (= 100 % de variance deF(X) en raison des interactions). Pour chaque fonctionnalité, nous mesurons dans quelle mesure elles interagissent avec toute autre fonctionnalité.


interact <- Interaction$new(predictor)
plot(interact)
cat("Predictor interactions: Land use")
interact <- Interaction$new(predictor, feature = "clcm_lvl3")
plot(interact)

```

[Plan]





## Biomass

[Plan]

```{r}
BM_tot_train = read.csv2("datas/BM_tot_train.csv")
BM_tot_test = read.csv2("datas/BM_tot_test.csv")
# names(BM_tot_train)

BM_tot_var_sup=c("bio7","pH","hurs_mean","bio3","bio4","sand","N","CN","gdd10","PET","K")

RF_result_BM_tot = ForetAlea(var_rep ="BM_tot", 
                             df_app=BM_tot_train[, !colnames(BM_tot_train) %in% BM_tot_var_sup ], 
                             df_valid = BM_tot_test [, !colnames(BM_tot_train) %in% BM_tot_var_sup],
                             mtry = 3,
                             ntree= BM_tot_best_ntree,
                             maxnodes = NULL)
# RF_result_BM_tot$RMSE^2 
# RF_result_BM_tot$R_adj_train 
# RF_result_BM_tot$R_adj_test 
# RF_result_BM_tot$model
var_importance <- data.frame(RF_result_BM_tot$model$importance)
var_importance$nom=rownames(var_importance)
var_importance <- var_importance[order(var_importance$IncNodePurity,decreasing = TRUE), ]
# var_importance

# on supprime: 
BM_tot_var_sup=c("bio7","pH","hurs_mean","bio3","bio4","sand","N","CN","gdd10","PET","K")



RF_BM_tot_pred <- RF_result_BM_tot$predit^2
RF_df_BM_tot = data.frame(Observed=BM_tot_test[,1]^2,Predicted = RF_BM_tot_pred)
cor_RF_BM_tot <- cor(RF_df_BM_tot$Observed, RF_df_BM_tot$Predicted)

# summary(RF_df_BM_tot)
  # graphique avec ggplot
RF_BM_tot <- ggplot(RF_df_BM_tot, aes(x = Observed, y = Predicted)) +
  geom_point() + # Ajout des points
  geom_smooth(method = "lm", se = TRUE, color = "red") +
  labs(title = "After",subtitle =paste0(" RF: R² adj (train) = ", round(RF_result_BM_tot$R_adj_train,2), 
                           "; \n R² adj (test) = ", round(RF_result_BM_tot$R_adj_test,2),
                           "; \n RMSE (test) = ",  round(RF_result_BM_tot$RMSE^2,2)),
                            x = "Observed valuess", 
                            y = "Predicted values") + 
      theme_classic()  
  # scale_x_continuous(breaks = seq(0, 800, by = 200)) +
  # scale_y_continuous(breaks = seq(0, 750, by = 100)) 

# RF_BM_tot_best2
# RF_BM_tot
graphe = ggarrange(RF_BM_tot_best2, RF_BM_tot,
                          labels = c('(a)', '(b)'),ncol = 2,widths = 4.666667, heights = 3,common.legend = TRUE,legend = 'right')
graphe


# mod_BM_tot = RF_result_BM_tot$model
# varImpPlot(mod_BM_tot, main = "Biomass") #produce variable importance plot
# importance_rf <- as.data.frame(importance(mod_BM_tot))
# importance_rf$nom=rownames(importance_rf)
# importance_rf <- importance_rf[order(importance_rf$IncNodePurity,decreasing = TRUE), ]
# row.names(importance_rf)=NULL
# importance_rf$percent <- importance_rf$IncNodePurity / sum(importance_rf$IncNodePurity)*100
# os = c("clcm_lvl3v","clcm_lvl3nial","clcm_lvl3p","clcm_lvl3gua","clcm_lvl3mf","clcm_lvl3ng")
# 
# barplot(importance_rf$percent, main = "Importance of variables for total Biomass", names.arg = importance_rf$nom, ylab = "Importance (%)", las = 2)
```

```{r Biomass response, fig.align='center'}
#_______________
BM_tot_train = read.csv2("datas/BM_tot_train.csv")
BM_tot_var = names(BM_tot_train)


BM_tot_var_sup=c("bio7","pH","hurs_mean","bio3","bio4","sand","N","CN","gdd10","PET","K")

# BM_tot_var[!BM_tot_var %in% BM_tot_var_sup]


BM_tot_predictors = c("BM_tot","gps_x","CaCO3","P","argile.0_30","gps_y","CEC",
                      "limon.0_30","bio12","clcm_lvl3")

BM_tot_df = bdd[,BM_tot_predictors]
BM_tot_df <- BM_tot_df %>% 
  dplyr::rename(clay = argile.0_30)
BM_tot_df <- BM_tot_df %>% 
  dplyr::rename(silt =limon.0_30 )
# colSums(is.na(BM_tot_df))
BM_tot_df = drop_na(BM_tot_df)
BM_tot_df$clcm_lvl3 = as.factor(BM_tot_df$clcm_lvl3)
# summary(BM_tot_df$clcm_lvl3)
levels(BM_tot_df$clcm_lvl3)[levels(BM_tot_df$clcm_lvl3) == "Broad-leaved forest"] <- "Forest"
levels(BM_tot_df$clcm_lvl3)[levels(BM_tot_df$clcm_lvl3) == "Coniferous forest"] <- "Forest"
levels(BM_tot_df$clcm_lvl3)[levels(BM_tot_df$clcm_lvl3) == "Mixed forest"] <- "Forest"
BM_tot_df$clcm_lvl3= as.factor(BM_tot_df$clcm_lvl3)
cl_original <- levels(BM_tot_df$clcm_lvl3)
new_cl <- c("f","gua", "ng", "nial", "p", "v")
BM_tot_df$clcm_lvl3 <- factor(BM_tot_df$clcm_lvl3, levels = cl_original, labels = new_cl)



rf <- randomForest(BM_tot ~ ., data = BM_tot_df, ntree = 500)

# Utilisation du conteneur iml Predictor()
X <- BM_tot_df[which(names(BM_tot_df) != "BM_tot")]
predictor <- Predictor$new(rf, data = X, y = BM_tot_df$BM_tot)




cat("Importance of predictors")
# Importance des fonctionnalités
# On calcule l'importance de chaque caractéristique pour les prédictions avec FeatureImp. La mesure de l'importance des fonctionnalités fonctionne en mélangeant chaque fonctionnalité et en mesurant l'ampleur de la baisse des performances. Pour cette tâche de régression, nous choisissons de mesurer la perte de performance avec l'erreur absolue moyenne (« mae »), un autre choix serait l'erreur quadratique moyenne (« mse »).

imp <- FeatureImp$new(predictor, loss = "mae") # mean absolute error
# imp <- FeatureImp$new(predictor, loss = "mse") # mean squared error  
plot(imp)
# imp$results
# importance_rf <- as.data.frame(importance(rf))
# importance_rf$nom=rownames(importance_rf)
# importance_rf <- importance_rf[order(importance_rf$IncNodePurity,decreasing = TRUE), ]
# row.names(importance_rf)=NULL
# importance_rf$percent <- importance_rf$IncNodePurity / sum(importance_rf$IncNodePurity)*100
# barplot(importance_rf$percent, main = "Importance of variables for total Biomass", names.arg = importance_rf$nom, ylab = "Importance (%)", las = 2)





cat("Predictor effects")
# Effets de fonctionnalités
# Les effets locaux accumulés décrivent comment les predicteurs influencent en moyenne la prédiction d'un modèle d'apprentissage automatique: ALE montre comment la prédiction change localement, lorsque les predicteurs varie. Les marques sur l'axe des x indiquent la distribution des predicteurs, montrant la pertinence d'une région pour l'interprétation (peu ou pas de points signifie que nous ne devons pas surinterpréter cette région).

# ale <- FeatureEffect$new(predictor, feature = "gps_x") # uniquement lstat
# ale$plot()
# ale$set.feature("rm")
# ale$plot()
effs <- FeatureEffects$new(predictor) # toutes les variables
plot(effs)






cat("Predictor interactions")
# Mesurer les interactions
# Nous pouvons également mesurer la force avec laquelle les fonctionnalités interagissent les unes avec les autres. La mesure d'interaction concerne la part de la variance de F(X) s’explique par l’interaction. La mesure est comprise entre 0 (pas d'interaction) et 1 (= 100 % de variance deF(X) en raison des interactions). Pour chaque fonctionnalité, nous mesurons dans quelle mesure elles interagissent avec toute autre fonctionnalité.


interact <- Interaction$new(predictor)
plot(interact)
cat("Predictor interactions: Land use")
interact <- Interaction$new(predictor, feature = "clcm_lvl3")
plot(interact)

```

[Plan]


## Richness

[Plan]

```{r}
Richesse_tot_train = read.csv2("datas/Richesse_tot_train.csv")
Richesse_tot_test = read.csv2("datas/Richesse_tot_test.csv")

Richesse_tot_var_sup=c("bio10","bio9","elevation","hurs_mean","bio15","bio7","gdd10","CEC","bio13","bio18","pH")

RF_result_Richesse_tot = ForetAlea(var_rep ="Richesse_tot", 
                             df_app=Richesse_tot_train[, !colnames(Richesse_tot_train) %in% Richesse_tot_var_sup ], 
                             df_valid = Richesse_tot_test [, !colnames(Richesse_tot_train) %in% Richesse_tot_var_sup],
                             mtry = 3,
                             ntree= Richesse_tot_best_ntree,
                             maxnodes = NULL)
# RF_result_Richesse_tot$RMSE 
# RF_result_Richesse_tot$R_adj_train 
# RF_result_Richesse_tot$R_adj_test 
# RF_result_Richesse_tot$model

# var_importance <- data.frame(RF_result_Richesse_tot$model$importance)
# row.names(var_importance)[which.min(var_importance$IncNodePurity)]
# var_importance$nom=rownames(var_importance)
# var_importance <- var_importance[order(var_importance$IncNodePurity,decreasing = TRUE), ]
# var_importance

# on supprime: 
Richesse_tot_var_sup=c("bio10","bio9","elevation","hurs_mean","bio15","bio7","gdd10","CEC","bio13","bio18","pH")


RF_Richesse_tot_pred <- RF_result_Richesse_tot$predit
RF_df_Richesse_tot = data.frame(Observed=Richesse_tot_test[,1],Predicted = RF_Richesse_tot_pred)
cor_RF_Richesse_tot <- cor(RF_df_Richesse_tot$Observed, RF_df_Richesse_tot$Predicted)

# summary(RF_df_Richesse_tot)
  # graphique avec ggplot
RF_Richesse_tot <- ggplot(RF_df_Richesse_tot, aes(x = Observed, y = Predicted)) +
  geom_point() + # Ajout des points
  geom_smooth(method = "lm", se = TRUE, color = "red") +
  labs(title = "After",subtitle =paste0(" RF: R² adj (train) = ", round(RF_result_Richesse_tot$R_adj_train,2), 
                           "; \n R² adj (test) = ", round(RF_result_Richesse_tot$R_adj_test,2),
                           "; \n RMSE (test) = ",  round(RF_result_Richesse_tot$RMSE,2)),
                            x = "Observed valuess", 
                            y = "Predicted values") + 
      theme_classic() + 
  scale_x_continuous(breaks = seq(0, 12, by = 3)) +
  scale_y_continuous(breaks = seq(0, 12, by = 3))

# RF_Richesse_tot_best2
# RF_Richesse_tot
graphe = ggarrange(RF_Richesse_tot_best2, RF_Richesse_tot,
                          labels = c('(a)', '(b)'),ncol = 2,widths = 4.666667, heights = 3,common.legend = TRUE,legend = 'right')
graphe


# mod_Richesse_tot = RF_result_Richesse_tot$model
# varImpPlot(mod_Richesse_tot, main = "Richness") #produce variable importance plot
# importance_rf <- as.data.frame(importance(mod_Richesse_tot))
# importance_rf$nom=rownames(importance_rf)
# importance_rf <- importance_rf[order(importance_rf$IncNodePurity,decreasing = TRUE), ]
# row.names(importance_rf)=NULL
# importance_rf$percent <- importance_rf$IncNodePurity / sum(importance_rf$IncNodePurity)*100
# os = c("clcm_lvl3v","clcm_lvl3nial","clcm_lvl3p","clcm_lvl3gua","clcm_lvl3mf","clcm_lvl3ng")
# 
# barplot(importance_rf$percent, main = "Importance of variables for total Richness", names.arg = importance_rf$nom, ylab = "Importance (%)", las = 2)
```

```{r Richness response, fig.align='center'}
#_______________
Richesse_tot_train = read.csv2("datas/Richesse_tot_train.csv")
Richesse_tot_var = names(Richesse_tot_train)

Richesse_tot_var_sup=c("bio10","bio9","elevation","hurs_mean","bio15","bio7","gdd10","CEC","bio13","bio18","pH")

Richesse_tot_var[!Richesse_tot_var %in% Richesse_tot_var_sup]




Richesse_tot_predictors = c("Richesse_tot","bio4", "CaCO3","gps_x","N","bio16","gps_y","PET", "P",
                      "clcm_lvl3")

Richesse_tot_df = bdd[,Richesse_tot_predictors]
Richesse_tot_df <- Richesse_tot_df %>% 
  dplyr::rename(clay = argile.0_30)
Richesse_tot_df <- Richesse_tot_df %>% 
  dplyr::rename(silt =limon.0_30 )
# colSums(is.na(Richesse_tot_df))
Richesse_tot_df = drop_na(Richesse_tot_df)
Richesse_tot_df$clcm_lvl3 = as.factor(Richesse_tot_df$clcm_lvl3)
# summary(Richesse_tot_df$clcm_lvl3)
levels(Richesse_tot_df$clcm_lvl3)[levels(Richesse_tot_df$clcm_lvl3) == "Broad-leaved forest"] <- "Forest"
levels(Richesse_tot_df$clcm_lvl3)[levels(Richesse_tot_df$clcm_lvl3) == "Coniferous forest"] <- "Forest"
levels(Richesse_tot_df$clcm_lvl3)[levels(Richesse_tot_df$clcm_lvl3) == "Mixed forest"] <- "Forest"
Richesse_tot_df$clcm_lvl3= as.factor(Richesse_tot_df$clcm_lvl3)
cl_original <- levels(Richesse_tot_df$clcm_lvl3)
new_cl <- c("f","gua", "ng", "nial", "p", "v")
Richesse_tot_df$clcm_lvl3 <- factor(Richesse_tot_df$clcm_lvl3, levels = cl_original, labels = new_cl)



rf <- randomForest(Richesse_tot ~ ., data = Richesse_tot_df, ntree = 500)

# Utilisation du conteneur iml Predictor()
X <- Richesse_tot_df[which(names(Richesse_tot_df) != "Richesse_tot")]
predictor <- Predictor$new(rf, data = X, y = Richesse_tot_df$Richesse_tot)




cat("Importance of predictors")
# Importance des fonctionnalités
# On calcule l'importance de chaque caractéristique pour les prédictions avec FeatureImp. La mesure de l'importance des fonctionnalités fonctionne en mélangeant chaque fonctionnalité et en mesurant l'ampleur de la baisse des performances. Pour cette tâche de régression, nous choisissons de mesurer la perte de performance avec l'erreur absolue moyenne (« mae »), un autre choix serait l'erreur quadratique moyenne (« mse »).

imp <- FeatureImp$new(predictor, loss = "mae") # mean absolute error
# imp <- FeatureImp$new(predictor, loss = "mse") # mean squared error  
plot(imp)
# imp$results
# importance_rf <- as.data.frame(importance(rf))
# importance_rf$nom=rownames(importance_rf)
# importance_rf <- importance_rf[order(importance_rf$IncNodePurity,decreasing = TRUE), ]
# row.names(importance_rf)=NULL
# importance_rf$percent <- importance_rf$IncNodePurity / sum(importance_rf$IncNodePurity)*100
# barplot(importance_rf$percent, main = "Importance of variables for total Richness", names.arg = importance_rf$nom, ylab = "Importance (%)", las = 2)





cat("Predictor effects")
# Effets de fonctionnalités
# Les effets locaux accumulés décrivent comment les predicteurs influencent en moyenne la prédiction d'un modèle d'apprentissage automatique: ALE montre comment la prédiction change localement, lorsque les predicteurs varie. Les marques sur l'axe des x indiquent la distribution des predicteurs, montrant la pertinence d'une région pour l'interprétation (peu ou pas de points signifie que nous ne devons pas surinterpréter cette région).

# ale <- FeatureEffect$new(predictor, feature = "gps_x") # uniquement lstat
# ale$plot()
# ale$set.feature("rm")
# ale$plot()
effs <- FeatureEffects$new(predictor) # toutes les variables
plot(effs)






cat("Predictor interactions")
# Mesurer les interactions
# Nous pouvons également mesurer la force avec laquelle les fonctionnalités interagissent les unes avec les autres. La mesure d'interaction concerne la part de la variance de F(X) s’explique par l’interaction. La mesure est comprise entre 0 (pas d'interaction) et 1 (= 100 % de variance deF(X) en raison des interactions). Pour chaque fonctionnalité, nous mesurons dans quelle mesure elles interagissent avec toute autre fonctionnalité.


interact <- Interaction$new(predictor)
plot(interact)
cat("Predictor interactions: Land use")
interact <- Interaction$new(predictor, feature = "clcm_lvl3")
plot(interact)

```

[Plan]

<!-- 
## Explanatory power of variables 

-   Abundance (X1 = LC, soil, climate; X2 = Spatial)
```{r,fig.align='center',fig.dpi=300}
# AB_tot --------------------------------------------------------------
VAR_REP = "AB_tot"
TITRE = "Abundance"

training = read.csv2(paste0("datas/",VAR_REP,"_train.csv"))
test = read.csv2(paste0("datas/",VAR_REP,"_test.csv"))

df= rbind(training,test)
df_=drop_na(df)
df_response = df %>% dplyr::select(all_of(VAR_REP))

df_predictors = df %>% dplyr::select(-AB_tot, -gps_x,-gps_y)

df_space = df %>% dplyr::select(gps_x,gps_y)

#Partition of the Variation of Community with env and space (xy)
mod <- varpart(df_response,df_predictors,df_space)
# mod
plot(mod, bg=2:5)
```


-   Biomass (X1 = LC, soil, climate; X2 = Spatial)
```{r,fig.align='center',fig.dpi=300}
# BM_tot --------------------------------------------------------------
VAR_REP = "BM_tot"
TITRE = "Biomass"

training = read.csv2(paste0("datas/",VAR_REP,"_train.csv"))
test = read.csv2(paste0("datas/",VAR_REP,"_test.csv"))

df= rbind(training,test)
df_=drop_na(df)
df_response = df %>% dplyr::select(all_of(VAR_REP))

df_predictors = df %>% dplyr::select(- BM_tot, - gps_x,- gps_y)

df_space = df %>% dplyr::select(gps_x,gps_y)

#Partition of the Variation of Community with env and space (xy)
mod <- varpart(df_response,df_predictors,df_space)
# mod
plot(mod, bg=2:5)
```

-   Richness (X1 = LC, soil, climate; X2 = Space)
```{r,fig.align='center',fig.dpi=300}
# Richesse_tot --------------------------------------------------------------
VAR_REP = "Richesse_tot"
TITRE = "Richness"

training = read.csv2(paste0("datas/",VAR_REP,"_train.csv"))
test = read.csv2(paste0("datas/",VAR_REP,"_test.csv"))

df= rbind(training,test)
df_=drop_na(df)
df_response = df %>% dplyr::select(all_of(VAR_REP))

df_predictors = df %>% dplyr::select(-Richesse_tot, -gps_x,-gps_y)

df_space = df %>% dplyr::select(gps_x,gps_y)

#Partition of the Variation of Community with env and space (xy)
mod <- varpart(df_response,df_predictors,df_space)
# mod
plot(mod, bg=2:5)

```


[Relationship between variables]


-->



# Results: Case 2 -> non-repeated data

## Comparison

- Zero duplicates in Programme, ID_Site and GPS (see [Script.R](script_non_rep.R){target="_blank"} ) 

<p align="center">
  <img src="Results/100_clean/all_graphe_clean100.png">
</p>

- Dataset clean (see [Script.R](script_non_rep.R){target="_blank"} )

<p align="center">
  <img src="Results/100_clean/all_graphe_clean.png">
</p>

- Comparison with the best algorithms (best 2)

<p align="center">
  <img src="Results/best_algo_RF_2.png">
</p>


# Results: Case 3 -> original data

## Inputs

<br/>

Predicteurs =  c("gps_x", "gps_y", "clay", "fine_sand", "coarse_sand", "fine_silt", "coarse_silt", "ph_eau", "om", "n_tot")

<br/>

OS = c("Arable land", "Artificial, non-agricultural vegetated areas", "Forests"                         "Pastures", "Permanent crops"  )


## Results with measured data (BDD1.9)

<p align="center">
  <img src="Results/original_data/all_graphe_ori.png">
</p>

- Comparison with the best algorithms

<p align="center">
  <img src="Results/best_algo_RF_2.png">
</p>


# Dicussion

## Comparison
```{r comp}
chemin_fichier_excel <- "C:/Users/diall/Downloads/datas/ODMAP.xlsx"
df <- read.xlsx(chemin_fichier_excel, sheet = "comp")
kable(df)
```

-   Fig S3 of phillip et al., 2019
<p align="center">
  <img src="Results/phillips_et_al_2019_results.png">
</p>

<p align="center">
  <img src="Results/best_algo_RF_2.png">
</p>


# Question {.unnumbered}
-   Retransformation des données vdt
-   Overfitting


# To do next {.unnumbered}

-   

    1.  Rédaction, protocol ODMAP;

-   

    2.  Models avec LandWorm

-   

    3.  Réduction des variables
    
-   

    3.  Effets des variables

-   

    4.  Prédiction et cartographie
    

[Plan]
<!-- -   Augmentation des données; -->

<!-- -   Création des models par OS ou equilibree les levels des OS; -->

<!-- -   Tester une classification pour la richesse totale taxonomique -->
    

## Additional information

<br/>
<br/>
<br/>
See [species explorations](https://posit.cloud/content/7997063){target="_blank"}
<br/>
<br/>
All the material from my internship, including scripts and datasets, is available on my [GitHub.](https://github.com/diallo-abdou/stage_abdou_m2){target="_blank"}

# {.unnumbered}

<div style="text-align: center; font-size: larger"><strong>Thank you for your attention</strong></div>



