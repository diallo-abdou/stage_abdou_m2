---
title: "Internship progress"
author: "Abdourahmane Diallo"
date: "2024-02-20"
format: 
  revealjs
#smaller: true
scrollable: true
#theme: sky
editor: visual
number-sections: true
toc: FALSE
#toc-expand: false
#toc_float: 'yes'
code_download: 'yes'
slide-number: true
margin: 0.1
#center: true
code-fold: true
width: 1200
height: 700
toc_depth: 1
execute: 
  cache: true
---



# Setting

```{r setup, include=FALSE,fig.align='center',message=FALSE,warning=FALSE,message=FALSE,echo=TRUE}
# rm(list=ls()) # Properly clear workspace
# source("function_abdou.R")

```
## Packages

```{r,echo=TRUE}
  library(tidyverse)
  library(glme)
  library(lsmeans)
  library(agricolae)
  library(RVAideMemoire)
  library(corrplot)
  library(emmeans)
  library(lme4)
  library(multcomp)
  library(MASS)
  library(R2WinBUGS)
  library(arm)
  library(performance)
  library(AER)
  library(AICcmodavg)
  library(MuMIn)
  library(ade4)
  library(Hmisc)
  library(labdsv)
  library(vegan)
  library(cowplot)
  library(ggpubr)
  library(rstatix)
  library(patchwork)
  library(multcompView)
  library(ggsignif)
  library(grid)
  library(FactoMineR)
  library(factoextra)
  library(explore)
  library(ggrepel)
  library(naniar)
  library(outliers)
  library(leaps)
  library(fastDummies)
  library(caret)
  library(mgcv)
  library(ggeffects)
  library(gratia)
  library(GGally)
  library(caTools)
  library(rpart)
  library(rpart.plot)
  library(openxlsx)
  library(readxl)
  library(leaflet)
  library(quarto)
  library(raster)
  library(knitr)
  library(kableExtra)
  library(stringr)
  library(plotly)


```

## Functions

```{r, echo=TRUE}

## Identification des NA dans un df -----------------------------------------------
taux_completion<-
  function(df, afficher_zero_percent = FALSE, seuil, trie=FALSE) {
    # Calcule du pourcentage de NA dans le dataframe
    pourcentage_total <-
      round(sum(is.na(df)) / (nrow(df) * ncol(df)) * 100, 1)
    
    # Calcule du pourcentage de NA par colonne
    pourcentage_colonnes <- round(colMeans(is.na(df)) * 100, 1)
    
    # Creation d'un dataframe résultat avec deux colonnes
    result <-
      data.frame(
        Variables = names(df),
        CR = pourcentage_colonnes,
        row.names = NULL
      )
    
    if (afficher_zero_percent) {
      result <- result[result$CR == 0, ]
      result$CR = 100 -result$CR
    } else {
      result <- result[result$CR > 0, ]
      result$CR = 100 -result$CR
      
    }
    
    result <- rbind(result, c("Total", pourcentage_total))
    #result <- rbind(result, c("Total", paste0(pourcentage_total, "")))
    
    result <- result[, c("Variables", "CR")]
    result$CR = as.numeric(result$CR)
    result$CR = round(result$CR,1)
    if (trie){
      result = result %>% arrange(desc(CR))
    }
    result$CR = paste0(result$CR,"%")
    
    return(result)
  }
# Converssion des colonne en num ou factor-----------------------------------------------
conv_col <- function (data, columns_to_convert, to_types) {
  if (to_types == "numeric") {
    # Conversion des colonnes en numeric
    for (col in columns_to_convert) {
      data[, col] <- as.numeric(data[, col])
    }
  } else {
    # Conversion des colonnes en facteurs
    for (col in columns_to_convert) {
      data[, col] <- as.factor(data[, col])
    }
  }
  return(data)
}
#data_converted <- conv_col(data, names(data [, c(1, 3)]), "factor")

# exploration graphiques des variables numeriques -----------------------------------------------
explo_num <- function(nom_col, titre, df = bdd, ligne_col = c(2, 2),mini = min(df[[nom_col]]), maxi=max(df[[nom_col]]) ) {
  par(mfrow = ligne_col)
  
  df[complete.cases(df[[nom_col]]), ]
  df <- df %>%filter(!is.na(df[[nom_col]]))
  df[[nom_col]] = as.numeric(df[[nom_col]])
  # Boxplot
  boxplot(df[[nom_col]], col = 'blue', ylab = titre, ylim = c(mini, maxi))
  # Cleveland plot
  dotchart(df[[nom_col]], pch = 16, col = 'blue', xlab = titre)
  # Histogram
  hist(df[[nom_col]], col = 'blue', xlab = titre, main = "")
  # Quantile-Quantile plot
  qqnorm(df[[nom_col]], pch = 16, col = 'blue', xlab = '')
  qqline(df[[nom_col]], col = 'red') 
}

# Extraction des predictors + moyennes -----------------------------------------------

extraction <- function(nom_col, tif_file_path, df = bdd, conv = 1) {
  #df <- df %>%filter(!is.na(gps_x) & !is.na(gps_y))
  raster_data <- raster(tif_file_path)
  
  # Création d'un dataframe pour stocker les valeurs extraites
  df_interne <- data.frame(gps_x = df$gps_x, gps_y = df$gps_y)
  proj4Str <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"
  # Transformer les coordonnées GPS en système de coordonnées du raster
  gps_coords_sp <- SpatialPoints(df_interne, proj4string = CRS(proj4Str))
  gps_coords_proj <- spTransform(gps_coords_sp, crs(raster_data))
  
  # Extraction des valeurs du raster 
  values <- raster::extract(raster_data, gps_coords_proj)
  
  # Ajout des valeurs extraites comme nouvelles colonnes a df
  #df_save = data.frame()
  #df_save[[nom_col]] <- values / conv
  
  df[[nom_col]] <- values / conv
  
  return(df)
}

# la moyenne des predictores -----------------------------------------------
moyenne_val_extrct <- function(nom_col, vec_col, df=bdd) {
  df[[nom_col]] <- rowMeans(as.matrix(df[, vec_col, drop = FALSE]), na.rm = TRUE)
  df[[nom_col]] = round(df[[nom_col]],1)
  return(as.data.frame(df))
}
```

# Plan

-   Setting

-   Database import

-   Database exploration

-   Earthworms data

-   Soil data extraction

-   Climate data extraction

-   To do next

# Database import

-   Import of database **LandWorm_dataset_site_V1.9.xlsx** (february 22, 2024)

```{r,echo=FALSE}
chemin_fichier_excel = "C:/Users/diall/Downloads/datas/LandWorm_dataset_site_V1.9.xlsx"
bdd <- read.xlsx(chemin_fichier_excel, sheet = "Sheet1")
```

-   The database contains **`r nrow(bdd)`** rows and **`r ncol(bdd)`** columns

```{r,echo=FALSE}
col_en_factor = c("Programme","Annee","ID_Site","Code_Parcelle","postal_code","clcm_lvl1",
                  "clcm_lvl2","clcm_lvl3","Modalite","Bloc","Protocole","land_cover_detail","type_tillage","fertilisation","ferti_min_product","ferti_orga_product")
bdd = conv_col(bdd, col_en_factor, "factor")
```

-   Data selection: EcoBioSoil = *"dc "*

```{r,echo=TRUE}
n_line=nrow(bdd)
bdd$owner=as.factor(bdd$owner)
summary_df <- as.data.frame(summary(bdd$owner))
colnames(summary_df) <- c("Numbers")
kable(summary_df)
```


```{r,echo=FALSE}
bdd <- subset(bdd, owner == "dc")
bdd$owner=droplevels(bdd$owner)

```

-   The database therefore changes from **`r n_line`** to **`r nrow(bdd)`** observations.



# Database exploration

-   CR = Completion rate

## Complete columns

```{r, echo=TRUE}
df_col=taux_completion(bdd,TRUE,trie=FALSE)
df_col = df_col[df_col$Variables != "Total",]
#print("table")
kable(df_col, caption = "", col.width = c("75%", "25%"))
# cat(                                                    )
# head(bdd[, "ID"])
```

## Non-complete columns

```{r, scrollable = TRUE}
df_col= taux_completion(bdd,FALSE,trie = TRUE)
df_col = df_col[df_col$Variables != "Total",]
kable(df_col, caption = " ", col.width = c("75%", "25%"))
```

## Focus on GPS coordinates

-   There is **`r sum(is.na(bdd$gps_x))`** NA (CR = `r df_col[df_col$Variable=="gps_x", "CR"]`) in **GPS_X**
-   There is **`r sum(is.na(bdd$gps_y))`** NA (CR = `r df_col[df_col$Variable=="gps_y", "CR"]`) in **GPS_Y**

```{r,echo=TRUE}
n_line= nrow(bdd)
bdd$gps_x <- as.numeric(gsub("[^0-9.-]", "", bdd$gps_x))
bdd$gps_y <- as.numeric(gsub("[^0-9.-]", "", bdd$gps_y))
bdd <- bdd[complete.cases(bdd$gps_x, bdd$gps_y), ]
bdd <- bdd %>%filter(!is.na(gps_x) & !is.na(gps_y))
#sum(is.na(bdd$gps_x))
#sum(is.na(bdd$gps_y))
```

-   We delete the *NA* lines in the GPS coordinates
-   The database therefore changes from **`r n_line`** to **`r nrow(bdd)`** observations.
-   Merging database and climat database
```{r, echo=TRUE}
# Ajout variables climatiques (voir chunk extraction données climatiques)
chemin_fichier <- "C:/Users/diall/OneDrive/Bureau/M2_MODE/stage_abdou_m2/datas/bdd_climat_ok.rds"
# saveRDS(bdd_climat_ok, chemin_fichier)
bdd_climat_ok <- readRDS(chemin_fichier)
df_fusion <- subset(bdd_climat_ok, select = -c(gps_x, gps_y))

rows_not_in_df_fusion <- anti_join(bdd, df_fusion, by = "ID")
merged_df <- merge(bdd, df_fusion, by = "ID")

ids_not_matching <- anti_join( merged_df,bdd, by = "ID")

bdd = merged_df

#bdd <- cbind(bdd, df_fusion) # all = TRUE pour garder toutes les lignes
```

## Cartography

```{r,echo=TRUE}
n_ligne= nrow(bdd)
df_coord <- bdd[, c("gps_x", "gps_y")] %>% mutate(gps_x = as.numeric(gps_x),gps_y = as.numeric(gps_y))

df_coord$num_ligne <- seq(nrow(df_coord))
carte <- leaflet(df_coord) %>%
  addTiles() %>%
  addCircleMarkers(lng = ~gps_x, lat = ~gps_y, radius = 0.8, fillOpacity = 0.8, fillColor = "blue")
carte
```

```{r}
hors_france= read.csv(file = "C:/Users/diall/Downloads/datas/hors_france.csv", header = TRUE)

bdd <- bdd[!(bdd$gps_x %in% hors_france$gps_x & bdd$gps_y %in% hors_france$gps_y), ]
bdd <- droplevels(bdd)
```

-   We delete points outside France (**`r nrow(hors_france)`**)
-   The database therefore changes from **`r n_ligne`** to **`r nrow(bdd)`** observations.

## Focus on years

-   Cleaning the Annee column
<br/>
<!--
```{r , echo=TRUE}
# levels(bdd$Annee) # parfois années et jours et ou mois
# bdd$Annee= as.factor(bdd$Annee)
# bdd$Annee <- gsub("^(\\d{4}).*$", "\\1", bdd$Annee) # on prend uniquement les 04 premier chiffre
# bdd$Annee= as.factor(bdd$Annee)

```
-->

-   CR of Annee = **`r df_col[df_col$Variable=="Annee", "CR"]`** (`r length(levels(bdd$Annee))` levels)

```{r, echo=TRUE, scrollable = TRUE}
bdd$Annee= as.factor(bdd$Annee)
summary_df <- as.data.frame(summary(bdd$Annee))
colnames(summary_df) <- c("Numbers")
kable(summary_df)
```

<!-- 
-   We remove all the years before **1990** and the NA 

```{r, echo=TRUE, scrollable = TRUE}
n_ligne =nrow(bdd)
bdd <- bdd %>%filter(!is.na(Annee))# on enleve les NA
annes_omit= c("1821", "1960", "1978", "1982", "1983", "1984", "1986", "1988", "1989") # annee sup
bdd <- bdd[!bdd$Annee %in% annes_omit, ]
bdd=droplevels(bdd)
#levels (bdd$Annee)
#summary (bdd$Annee)
summary_df <- as.data.frame(summary(bdd$Annee))
colnames(summary_df) <- c("Numbers")
kable(summary_df,padding = 5)
```

-   The database therefore changes from **`r n_ligne`** to **`r nrow(bdd)`** observations. 
-->



<!--
## Table land use & protocol ( start )

-   clcm_lvl1 & protocol

```{r, echo=TRUE}
kable(table(bdd$clcm_lvl1, bdd$Protocole,exclude = NULL),padding = 10,align = "c")

```

\n\n\n

-   clcm_lvl2 & protocol

```{r, echo=TRUE}
kable(table(bdd$clcm_lvl2, bdd$Protocole,exclude = NULL),padding = 10,align = "c")
```

\n\n\n

-   clcm_lvl3 & protocol

```{r, echo=TRUE}
kable(table(bdd$clcm_lvl3, bdd$Protocole,exclude = NULL),padding = 0,align = "c")
```
-->


## Focus on protocols

-   List of protocols available on the database ( `r length(levels(bdd$Protocole))` levels)

```{r,echo=TRUE}
bdd$Protocole = as.factor(bdd$Protocole)
summary_df <- as.data.frame(summary(bdd$Protocole))
colnames(summary_df) <- c("Numbers")
kable(summary_df,padding = 5)
```

<!--
-   Selection of protocols: **F_HS, FHS, hand sorting, HS**
-->
-   Selection of protocols: **F_HS, HS**
```{r,echo=TRUE}
n_ligne = nrow(bdd)
#select_protocole =c("F_HS", "FHS", "hand sorting" ,"HS")
select_protocole =c("F_HS", "HS")
bdd <- bdd[bdd$Protocole %in% select_protocole, ]
bdd=droplevels(bdd)
bdd$Protocole = as.factor(bdd$Protocole)
summary_df <- as.data.frame(summary(bdd$Protocole))
colnames(summary_df) <- c("Numbers")
kable(summary_df,padding = 5)
```

-   The database therefore changes from **`r n_ligne`** to **`r nrow(bdd)`** observations.

<!--
-   Merging levels :

    -   F_HS $=$ F_HS $+$ FHS
    -   HS $=$ HS $+$ hand sorting

    ```{r,echo=TRUE}
    levels(bdd$Protocole)[levels(bdd$Protocole) == "FHS"] <- "F_HS"
    levels(bdd$Protocole)[levels(bdd$Protocole) == "hand sorting"] <- "HS"
    bdd$Protocole = as.factor(bdd$Protocole)
    summary_df <- as.data.frame(summary(bdd$Protocole))
    colnames(summary_df) <- c("Numbers")
    kable(summary_df,padding = 5)
    ```
-->


## Focus on clcm_lvl1

-   CR of clcm_lvl1 = **`r df_col[df_col$Variable=="clcm_lvl1","CR"]`** (`r length(levels(bdd$clcm_lvl1))` levels)


```{r, echo=TRUE}
bdd$clcm_lvl1= as.factor(bdd$clcm_lvl1)
summary_df <- as.data.frame(summary(bdd$clcm_lvl1))
colnames(summary_df) <- c("Numbers")
# kable(summary_df,padding = 5)
```

-   Merging levels
```{r, echo=TRUE}
levels(bdd$clcm_lvl1)[levels(bdd$clcm_lvl1) == "1_Naturel"] <- "Forest and semi natural areas"
levels(bdd$clcm_lvl1)[levels(bdd$clcm_lvl1) == "2_Agricole"] <- "Agricultural areas"

bdd$clcm_lvl1= as.factor(bdd$clcm_lvl1)
summary_df <- as.data.frame(summary(bdd$clcm_lvl1))
colnames(summary_df) <- c("Numbers")
kable(summary_df,padding = 5)
```

-   Update **code_clcm_lvl1**
```{r, echo=TRUE}
#bdd$code_clcm_lvl1 = as.factor(bdd$code_clcm_lvl1)

bdd$code_clcm_lvl1 <- ifelse(bdd$clcm_lvl1 == "Forest and semi natural areas", 3, bdd$code_clcm_lvl1)

bdd$code_clcm_lvl1 <- ifelse(bdd$clcm_lvl1 == "Agricultural areas", 2, bdd$code_clcm_lvl1)
```

-   For the moment, we will keep the NA of **clcm_lvl1**



## Focus on clcm_lvl2

-   CR of clcm_lvl2 = **`r df_col[df_col$Variable=="clcm_lvl2","CR"]`** (`r length(levels(bdd$clcm_lvl2))` levels)

```{r, echo=TRUE}
bdd$clcm_lvl2= as.factor(bdd$clcm_lvl2)
summary_df <- as.data.frame(summary(bdd$clcm_lvl2))
colnames(summary_df) <- c("Numbers")
# kable(summary_df,padding = 8)
```

-   Merging levels
```{r, echo=TRUE}
levels(bdd$clcm_lvl2)[levels(bdd$clcm_lvl2) == "21_Agricole ouvert"] <- "Arable land"

bdd$clcm_lvl2= as.factor(bdd$clcm_lvl2)
summary_df <- as.data.frame(summary(bdd$clcm_lvl2))
colnames(summary_df) <- c("Numbers")
kable(summary_df,padding = 5)
```

-   Update **code_clcm_lvl2**
```{r, echo=TRUE}

bdd$code_clcm_lvl2 <- ifelse(bdd$clcm_lvl2 == "Arable land", 21, bdd$code_clcm_lvl2)

```

## Focus on clcm_lvl3

-   CR of clcm_lvl3 = **`r df_col[df_col$Variable=="clcm_lvl3","CR"]`** (`r length(levels(bdd$clcm_lvl3))` levels)


```{r, echo=TRUE, scrollable = TRUE}
bdd$clcm_lvl3= as.factor(bdd$clcm_lvl3)
summary_df <- as.data.frame(summary(bdd$clcm_lvl3))
colnames(summary_df) <- c("Numbers")
kable(summary_df,padding = 5)

```

## Land use selection (clcm_lvl3)
<!--
-   **Broad-leaved forest** 
-   **Coniferous forest** 
-   **Mixed forest** 

-   **Pastures, meadows and other permanent grasslands under agricultural use** 

-   **Non-irrigated arable land** 

-   **Vineyards**

-   **Green urban areas**

-   **Natural grasslands**
-->

```{r, echo=TRUE}
select_os= c("Broad-leaved forest", "Coniferous forest", "Mixed forest", 
"Pastures, meadows and other permanent grasslands under agricultural use", "Non-irrigated arable land", 
"Vineyards","Green urban areas","Natural grasslands")

bdd <- bdd[bdd$clcm_lvl3 %in% select_os, ]
bdd=droplevels(bdd)
bdd$clcm_lvl3 = as.factor(bdd$clcm_lvl3)
summary_df <- as.data.frame(summary(bdd$clcm_lvl3))
colnames(summary_df) <- c("Numbers")
kable(summary_df)

```
- **Maybe, we can merge the three types of forest ?**

## Land use & protocol overview

```{r, echo=TRUE}
# kable (table(bdd$clcm_lvl1, bdd$Protocole,exclude = NULL), align = "c", format = "pipe", padding = 10)
# kable (table(bdd$clcm_lvl2, bdd$Protocole,exclude = NULL), align = "c", format = "pipe", padding = 10)
kable (table(bdd$clcm_lvl3, bdd$Protocole,exclude = NULL), align = "c", format = "pipe", padding = 10)
```


<!--
# Earthworms data

## Total abundance (CR = 100 % )

```{r,fig.align='center',fig.height=10}
summary(bdd$AB_tot) 
#bdd <- subset(bdd, AB_tot <= 800)
explo_num(nom_col = "AB_tot", titre = "Total abundance")
```

## Total biomass (CR = `r df_col[df_col$Variable=="BM_tot","CR"]`)

```{r,fig.align='center',fig.height=10}
summary(bdd$BM_tot) 
#bdd <- subset(bdd, BM_tot <= 3000)
explo_num(nom_col = "BM_tot", titre = "Total biomass",)
```


-->
<!--
## Total richness calculation method

-   Removal of columns with only NA (**`r length(colnames(bdd)[colSums(is.na(bdd)) == nrow(bdd)])`**) and/or only 0
-   Identify columns beginning with **AB\_**
-   Deletion of **AB\_** columns that are not species
-   Calculate richness by assigning **1** to each column if the value is different from 0 and NA
-   Total richness = **1** if the plot has a value in AB and/or BM
-->

```{r, echo=FALSE,"Total richness calculation method"}
# on supprime tout les colonnes ayant que des NA
colonnes_na <- colnames(bdd)[colSums(is.na(bdd)) == nrow(bdd)]
# summary(bdd[, colonnes_na])
bdd <- bdd[, !colnames(bdd) %in% colonnes_na]



# On supprimme toutes les colonnes ayant que des NA et des 0
colonnes_numeriques <- sapply(bdd, is.numeric)
somme_colonnes_numeriques <- colSums(bdd[, colonnes_numeriques],na.rm=TRUE)
colonnes_zeros <- names(somme_colonnes_numeriques[somme_colonnes_numeriques == 0])
#summary(bdd[, colonnes_zeros])
bdd <- bdd[, !colnames(bdd) %in% colonnes_zeros]



# On récupère toutes les colonnes qui commencent par **AB_**
colonnes_AB <- grep("^AB_", names(bdd), value = TRUE)



# On supprimme les colonnes AB_ qui ne sont pas des espèces dans le calcule
ab_supprimee =  c("AB_AD","AB_JV","AB_SA","AB_STAD_X","AB_indéterminable","AB_Indéterminable","AB_indéterminable_endogeic","AB_tot","AB_Indéterminable_epigeic","AB_indéterminable_endogeic","AB_Ep.X","AB_vide", "AB_Ep.X1","AB_Ep.X2","AB_A.X","AB_Adult","AB_cocon","AB_indéterminé","AB_Juvenile","AB_Sub.adult","AB_Indéterminé","AB_Lumbricidae")
colonnes_AB <- colonnes_AB[!colonnes_AB %in% ab_supprimee]



# On calcule la richesse en attribiant 1 à chaque colonne si la valeur est différent de 0 et de NA
bdd$Richesse_tot <- 0
bdd$Richesse_tot <- rowSums(!is.na(bdd[colonnes_AB]) & bdd[colonnes_AB] != 0)
#sum (is.na(bdd$Richesse_tot) )
#summary(bdd$Richesse_tot)



# Check des lignes ayant des 0 richesse et X AB ou BM : 0 lignes
# vdt_a_checker = bdd[bdd$Richesse_tot == 0 & (bdd$Total_AB !=0 | bdd$BM_to !=0), c("ID_Site","AB_tot","BM_tot","Richesse_tot")]
# vdt_a_checker = subset(vdt_a_checker, Richesse_tot==0)
# View(vdt_a_checker)
# vdt_a_checker$Richesse_tot <- 1
# Mettre à jour les ligne correspondant dans la bdd 
# bdd[rownames(bdd) %in% rownames(vdt_a_checker), "Richesse_tot"] <- 1



# Check si y a des ligne ayant que des NA dans AB, BM et Richesse : nop
resultat <- subset(bdd, is.na(AB_tot) & is.na(BM_tot) & is.na(Richesse_tot))
# View(resultat[, c("AB_tot","BM_tot", "Richesse_tot")])



# Check si y a des ligne ayant que des zéros ou des NA dans AB, BM et Richesse_tot: 66 ligne
vdt <- c("AB_tot", "BM_tot", "Richesse_tot")
lignes_zero <- which(rowSums(bdd[vdt] != 0, na.rm = TRUE) == 0)
# View(bdd[lignes_zero,c("ID_Site","AB_tot", "BM_tot", "Richesse_tot")])



# Check des lignes ayant de BM mais pas de AB
bm_sans_ab <- subset(bdd, AB_tot == 0 & BM_tot != 0)
# bm_sans_ab[, c("ID","ID_Site", "Programme", "Protocole", "AB_tot", "BM_tot")]

ab_sans_bm <- subset(bdd, BM_tot == 0 & AB_tot != 0) # 1 parcelles
# ab_sans_bm[, c("ID","ID_Site", "Programme", "Protocole", "AB_tot", "BM_tot")]


# Check des doublons

#duplicated_rows <- subset(bdd, duplicated(bdd[, c("ID", "AB_tot", "BM_tot")]) | #duplicated(bdd[, c("ID", "AB_tot", "BM_tot")], fromLast = TRUE))

```

<!--
## Total richness (CR = 100 % )

```{r, fig.align='center',fig.height=10}
summary(bdd$Richesse_tot)
#bdd <- subset(bdd, Richesse_tot <= 2000)
explo_num(nom_col = "Richesse_tot", titre = "Total richness",)
```








-->

# Soil data extraction

```{r}
# Calcul des distances euclidiennes entre les sites
distances <- dist(cbind(bdd$gps_x, bdd$gps_y))
distance_moyenne <- mean(distances)
# distance_moyenne

```

<!--
## The source database ([openlandmap](https://openlandmap.org/?center=25,39&zoom=4&opacity=72&base=OpenStreetMap&layer=lc_glc.fcs30d&time=2022){target="_blank"})

```{r,echo=TRUE}
chemin_fichier_excel <- "C:/Users/diall/Downloads/datas/ODMAP.xlsx"
pedo <- read.xlsx(chemin_fichier_excel, sheet = "pedo")

# Fusion des cellules des colonnes avec des éléments dupliqués
for (col in names(pedo)) {
  pedo[[col]] <- ifelse(duplicated(pedo[[col]]), "", pedo[[col]])
}

#tableau avec kableExtra et centrage du contenu des cellules
kableExtra::kable(pedo) %>%
  kableExtra::kable_styling() %>%
  kableExtra::column_spec(1:ncol(pedo))  # Centrer le contenu de toutes les colonnes
```

\n

-   Average values between surface (0 cm) and 30 cm depth




## Changing the resolution ![](https://logowik.com/content/uploads/images/python.jpg){width="200"}

-   Long compilation time in R

-   GDAL module with the resampleAlg = bilinear method

-   Resolution = 0.0083 = 30 arc-second \~ 1km

    ```{r}
    test_resolution = bdd
    tif_file_path_origine = "C:/Users/diall/Downloads/datas/raster_origine/sol_ph.h2o_usda.4c1a2a_m_250m_b10..10cm_1950..2017_v0.2.tif"
    raster_ph_origine <- raster(tif_file_path_origine)
    test_resolution <- extraction(nom_col = "ph_10_origine",df = test_resolution,conv = 10, 
                      tif_file_path = tif_file_path_origine)


    tif_file_path_rech = "C:/Users/diall/Downloads/datas/raster_modif/sol_ph.h2o_usda.4c1a2a_m_250m_b10..10cm_1950..2017_v0.2.tif"
    raster_ph_rech <- raster(tif_file_path_rech)
    test_resolution <- extraction(nom_col = "ph_10_rech",df = test_resolution,conv = 10, 
                      tif_file_path = tif_file_path_rech)

    par(mforw=c(1,2))
    image(raster_ph_origine,main="pH at 10cm: original raster (0.002)")
    image(raster_ph_rech, main = "pH at 10cm: raster modify (0.008)")


    bdd_echan = test_resolution
    bdd_echan <- bdd_echan %>%filter(!is.na(ph_10_origine) & !is.na(ph_10_rech))

    # graphique avec ggplot
        # coefficient de corrélation
    correlation <- cor(as.numeric(bdd_echan$ph_10_origine), bdd_echan$ph_10_rech)
    p <- ggplot(bdd_echan, aes(x = ph_10_origine, y = ph_10_rech)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = FALSE, color = "red") + 
      labs(subtitle = paste("R = ", round(correlation, 2)),
           x = "Original pH", y = "Resampled pH") + 
      theme_classic() 

    p

    ```



-->


## Soil organic carbone (g/kg)

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = "c_orga_0",df = bdd,conv = 5, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_organic.carbon_usda.6a1c_m_250m_b0..0cm_1950..2017_v0.2.tif")
bdd <- extraction(nom_col = "c_orga_10",df = bdd,conv = 5, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_organic.carbon_usda.6a1c_m_250m_b10..10cm_1950..2017_v0.2.tif")
bdd <- extraction(nom_col = "c_orga_30",df = bdd,conv = 5, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_organic.carbon_usda.6a1c_m_250m_b30..30cm_1950..2017_v0.2.tif")
bdd = moyenne_val_extrct(nom_col = "c_orga_0_a_30", vec_col = c("c_orga_0","c_orga_10","c_orga_30"),df=bdd)


indice_max <- which.max(bdd$c_orga_0_a_30)
bdd <- bdd[-indice_max, ]


summary(bdd$c_orga_0_a_30)
explo_num(nom_col = "c_orga_0_a_30", titre = "C. organic")
```

## pH
**Extracted values**
```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = "ph_0",df = bdd,conv = 10, 
                  tif_file_path ="C:/Users/diall/Downloads/datas/raster_modif/sol_ph.h2o_usda.4c1a2a_m_250m_b10..10cm_1950..2017_v0.2.tif")

bdd <- extraction(nom_col = "ph_10" ,df = bdd,conv = 10, 
                  tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_ph.h2o_usda.4c1a2a_m_250m_b10..10cm_1950..2017_v0.2.tif")
bdd <- extraction(nom_col = "ph_30" ,df = bdd,conv = 10, 
                  tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_ph.h2o_usda.4c1a2a_m_250m_b30..30cm_1950..2017_v0.2.tif")
bdd = moyenne_val_extrct(nom_col = "ph_0_a_30", vec_col = c("ph_0","ph_10","ph_30"),df=bdd)
# summary(bdd$ph_0_a_30)

explo_num(nom_col = "ph_0_a_30", titre = "pH (0 - 30 cm)")
```

**Measured values & extracted values**

-   Clean pH column
```{r,echo=TRUE}
# On recupere les deux colonnes du pH
df_comp=bdd[, c("ID", "ID_Site","ph_eau","ph_0_a_30" )]
df_comp =df_comp[complete.cases(df_comp$ph_eau),] 
df_comp =df_comp[complete.cases(df_comp$ph_0_a_30),] 
df_comp <- df_comp[!grepl("[^0-9.]", df_comp$ph_eau), ]
df_comp$ph_eau <- as.numeric(df_comp$ph_eau)
df_comp$ph_0_a_30 <- as.numeric(df_comp$ph_0_a_30)


df_comp = df_comp[!df_comp$ph_eau== 44140.00,]
df_comp = df_comp[!df_comp$ph_eau== "NA",]
df_comp = df_comp[!df_comp$ph_0_a_30== "NA",]
df_comp = droplevels(df_comp)
```


```{r,echo=TRUE}
ID_Site_dupliques <- df_comp$ID_Site[duplicated(df_comp$ID_Site)]
#length(ID_Site_dupliques)

lignes_dupliquees <- subset(df_comp, duplicated(ID_Site) & duplicated(ph_eau))

lignes_unique <- unique(lignes_dupliquees$ID_Site )
#length(lignes_unique)

# nrow(df_comp) - length(ID_Site_dupliques) + length(lignes_unique)


dupliquees <- duplicated(df_comp$ID_Site)
df_comp <- df_comp[!dupliquees, ]
df_comp=droplevels(df_comp)

# correlation <- cor.test(df_comp$ph_eau, df_comp$ph_0_a_30,method = "pearson")
#resultat_test <- t.test(df_comp$ph_eau, df_comp$ph_0_a_30)

df_comp$ph_eau <- as.numeric(df_comp$ph_eau)
df_comp$ph_0_a_30 <- as.numeric(df_comp$ph_0_a_30)

```

<!-- <br/>  -->

::: columns
::: {.column width="60%"}
-   Method ?

-   Depth ?

-   Measured values (CR = `r df_col[df_col$Variable=="ph_eau","CR"]`)

```{r}
  summary(df_comp$ph_eau)
```

-   Extracted values

```{r}
  summary(df_comp$ph_0_a_30)
```
:::

::: {.column width="40%"}
```{r,fig.align='center',fig.height=5,fig.width=4}
    correlation <- cor(as.numeric(df_comp$ph_eau), df_comp$ph_0_a_30)
# graphique avec ggplot
    p <- ggplot(df_comp, aes(x = ph_eau, y = ph_0_a_30)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = FALSE, color = "red") + 
      labs(subtitle =paste("R = ", round(correlation, 2)),x = "pH measured values", y = "pH extracted values") + 
      theme_classic() 
p
```
:::
:::



<!--
## Bulk density (kg / m-cube)

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = "d_ap_0",df = bdd,conv = 10, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_bulkdens.fineearth_usda.4a1h_m_250m_b0..0cm_1950..2017_v0.2.tif")
bdd <- extraction(nom_col = "d_ap_10",df = bdd,conv = 10, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_bulkdens.fineearth_usda.4a1h_m_250m_b10..10cm_1950..2017_v0.2.tif")
bdd <- extraction(nom_col = "d_ap_30",df = bdd,conv = 10, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_bulkdens.fineearth_usda.4a1h_m_250m_b30..30cm_1950..2017_v0.2.tif")
bdd = moyenne_val_extrct(nom_col = "d_ap_0_a_30", vec_col = c("d_ap_0","d_ap_10","d_ap_30"),bdd)
summary(bdd$d_ap_0_a_30)
explo_num(nom_col = "d_ap_0_a_30", titre = "Bulk density (0 - 30 cm)")
```


## Sand content (% kg/kg)

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = "sable_0",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_sand.wfraction_usda.3a1a1a_m_250m_b0..0cm_1950..2017_v0.2.tif")
bdd <- extraction(nom_col = "sable_10",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_sand.wfraction_usda.3a1a1a_m_250m_b10..10cm_1950..2017_v0.2.tif")
bdd <- extraction(nom_col = "sable_30",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_sand.wfraction_usda.3a1a1a_m_250m_b30..30cm_1950..2017_v0.2.tif")
bdd = moyenne_val_extrct(nom_col = "sable_0_a_30", vec_col = c("sable_0","sable_10","sable_30"),df=bdd)
summary(bdd$sable_0_a_30)
explo_num(nom_col = "sable_0_a_30", titre = "Sand (0 - 30 cm)")
```
-->


<!--
# Synthèse du taux de remplissage

## Complete columns

```{r, echo=TRUE}
df_col=taux_completion(bdd,TRUE,trie=FALSE)
df_col = df_col[df_col$Variables != "Total",]
kable(df_col, caption = " ", col.width = c("75%", "25%"))
```

## Non-complete columns

```{r, scrollable = TRUE}
df_col= taux_completion(bdd,FALSE,trie = TRUE)
df_col = df_col[df_col$Variables != "Total",]
kable(df_col, caption = " ", col.width = c("75%", "25%"))
```


-->

<!--
# Climate data extraction
## The source database ([CHELSA V2](https://chelsa-climate.org/bioclim/){target="_blank"})

```{r,echo=TRUE}

# Lire le fichier Excel
chemin_fichier_excel <- "C:/Users/diall/Downloads/datas/ODMAP.xlsx"
climat <- read.xlsx(chemin_fichier_excel, sheet = "climat")

# Fusions des cellules des colonnes avec des éléments dupliqués
for (col in names(climat)) {
  climat[[col]] <- ifelse(duplicated(climat[[col]]), "", climat[[col]])
}

# Affichage du tableau avec kableExtra et centrage du contenu des cellules
kableExtra::kable(climat) %>%
  kableExtra::kable_styling() %>%
  kableExtra::column_spec(1:ncol(climat)) 

```

## Extraction method

-   Link recovery ( see file [link .tif](https://1drv.ms/t/s!Avfm81EzNGBHjIZWw8YePljXaGSpCQ?e=qIPeWR){target="_blank"} )

-   Extracting variable names

-   Uses of the **extraction()** function

-   Convert columns to correct format and unit

-   Adding variables to the LANDWORM database

```{r,echo=TRUE}
liens_tif = read.table(file = "C:/Users/diall/Downloads/datas/envidatS3paths.txt")
liens_tif$shortname <- str_extract(liens_tif$V1, "(?<=CHELSA_).*?(?=_1981)")
liens_tif[liens_tif$shortname=="rsds","shortname"]=c("rsds_max","rsds_mean","rsds_min","rsds_range")

#all(is.na(bdd$gps_x))
#all(is.na(bdd$gps_y))

bdd_climat= bdd[, c("ID","gps_x","gps_y")]

temp_1=Sys.time()
#for( i in 1:nrow(liens_tif)){
  #nom=liens_tif[i,c("shortname")]
  #df_ext <- extraction(nom_col = nom,df = bdd_climat,conv = 1, 
                  #tif_file_path = liens_tif[i,c("V1")] ) 
  #bdd_climat[[nom]] <- df_ext [,nom]
  #rm("df_ext","nom")
  #cat("Extraction: ",i,"/",nrow(liens_tif), "\n")
#}
temp_2=Sys.time()
duree= difftime(temp_2,temp_1)

chemin_fichier <- "C:/Users/diall/OneDrive/Bureau/M2_MODE/stage_abdou_m2/datas/bdd_climat.rds"
# saveRDS(bdd_climat, chemin_fichier)
#bdd_climat <- readRDS(chemin_fichier)

# debut cnversion ------------------------------------------------------------
conv_df_climat= data.frame(shortname =liens_tif$shortname )

# unit = 1
conv_df_climat$unit = rep(1)
# unit = 100
unit_100=c("bio4")
conv_df_climat$unit <- ifelse(conv_df_climat$shortname %in% unit_100, 100, 1)


# scale = 0.1
conv_df_climat$scale = rep(0.1)
# scale = 1
scale_1=c("fcf","fgd","gddlgd0","gddlgd5","gddlgd10","gdgfgd0","gdgfgd5","gdgfgd10","gsl","kg0","kg1" ,"kg2" ,"kg3" ,"kg4" ,"kg5","lgd","ngd0","ngd5","ngd10","scd")

# scale = 0.01
scale_01=c("hurs_max","hurs_mean","hurs_min","hurs_range","pet_penman_max",
       "pet_penman_mean","pet_penman_min","pet_penman_range")

# scale = 0.001
scale_001=c("rsds","sfcWind_max","sfcWind_mean","sfcWind_min","sfcWind_range","pet_penman_max","pet_penman_mean","pet_penman_min","pet_penman_range","rsds_max","rsds_mean","rsds_min","rsds_range")

# Remplacement des valeurs de l'échelle en fonction des conditions
conv_df_climat$scale <- ifelse(conv_df_climat$shortname %in% scale_1, 1,
              ifelse(conv_df_climat$shortname %in% scale_01, 0.01,
                    ifelse(conv_df_climat$shortname %in% scale_001,0.001, 0.1)))

# offset = 0
conv_df_climat$offset = rep(0)
# offset = - 273.15
offset_273=c("bio1","bio5","bio6","bio8","bio9","bio10","bio11","gdgfgd10","gsl","gst")
conv_df_climat$offset = ifelse(conv_df_climat$shortname %in% offset_273, -273.15, 0)

# Pas present dans dans le pdf explicative donc pas de conversion
pas_pdf=c( "ai","swb", "clt_max","clt_mean","clt_min","clt_range")
verif=c(unit_100,scale_1,scale_01,scale_001,offset_273)
pas_pdf_2=setdiff(conv_df_climat$shortname, verif)
conv_df_climat[conv_df_climat$shortname %in% pas_pdf,"scale"] = 1

#bdd_climat_ok=bdd_climat[,c("ID","gps_x","gps_y")]

#for ( i in conv_df_climat$shortname){
  #if (i %in% names(bdd_climat)){
  #unitee= conv_df_climat[conv_df_climat$shortname ==i,"unit"]
  #echelle = conv_df_climat[conv_df_climat$shortname ==i,"scale"]
  #decalage = conv_df_climat[conv_df_climat$shortname ==i,"offset"]
  #bdd_climat_ok[[i]] = ((bdd_climat[[i]] / unitee)* echelle) + decalage
  #}else {
    #cat("Attention ",i, "n'exite pas dans la bdd_climat","\n")
  #}
#}


# chemin_fichier <- "C:/Users/diall/OneDrive/Bureau/M2_MODE/stage_abdou_m2/datas/bdd_climat_ok.rds"
# saveRDS(bdd_climat_ok, chemin_fichier)
# bdd_climat_ok <- readRDS(chemin_fichier)
# fin conversion

#df_fusion <- subset(bdd_climat_ok, select = -c(ID,gps_x, gps_y))
#bdd <- cbind(bdd, df_fusion) # all = TRUE pour garder toutes les lignes
```

## List of variables

[Variable description](https://chelsa-climate.org/wp-admin/download-page/CHELSA_tech_specification_V2.pdf){target="_blank"}

```{r}
summary(bdd_climat_ok)
```

## Temperature

-   Average annual air temperature (°C) = bio1

```{r,fig.align='center',fig.height=8}
summary(bdd$bio1)
explo_num(nom_col = "bio1", titre = "temp°.")
```

## Precipitation

-   Annual precipitation (kg/m²) = bio12

```{r,fig.align='center',fig.height=8}
summary(bdd$bio12)
explo_num(nom_col = "bio12", titre = "Précipitat°.")
```




# Questions

```{r}
ID_Site_dupliques <- bdd$ID_Site[duplicated(bdd$ID_Site)]
#length(ID_Site_dupliques)

lignes_dupliquees <- subset(bdd, duplicated(ID_Site))

lignes_unique <- unique(lignes_dupliquees$ID_Site)
#length(lignes_unique)

# nrow(bdd) - length(ID_Site_dupliques) + length(lignes_unique)
```

-   Comment gérer la répétition temporelle des parcelles ?
    -   Avec répétition : **`r nrow(bdd)`** observations
    -   Sans répétition : **`r nrow(bdd) - length(ID_Site_dupliques) + length(lignes_unique)`** observations
-   Liens des données du sol (sable, argile et limon) de data.gouv.fr ?


# To do next

-   

    1.  Extraction des prédicteurs : CEC, Limon, argile, évapotranspiration, paysage,...

-   

    2.  Analyse exploratoire : test de corrélation, VIF, ACP

-   

    3.  Sélection des variables

-   

    4.  Modélisation (GLM, GAM, RF, ANN)

-   

    5.  Validation croisée

-   

    6.  Rédaction, protocol ODMAP



-->



```{r}
df_col= taux_completion(bdd,FALSE,trie = TRUE)
df_col = df_col[df_col$Variables != "Total",]
```

## Sand
**Extracted values (g/kg, 0 - 30 cm)**

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = "sable.0_5",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sable.0_5.tif")

bdd <- extraction(nom_col = "sable.5_15",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sable.5_15.tif")

bdd <- extraction(nom_col = "sable.15_30",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sable.15_30.tif")

bdd = moyenne_val_extrct(nom_col = "sable.0_30", vec_col = c("sable.0_5","sable.5_15","sable.15_30"),df=bdd)

# summary(bdd$sable.0_30)

explo_num(nom_col = "sable.0_30", titre = "Sand extracted values")
```


**Measured values & extracted values**

-   Clean sand column
```{r,echo=TRUE}
# On recupere les deux colonnes du pH
df_comp=bdd[, c("ID", "ID_Site","sand","sable.0_30" )]
df_comp =df_comp[complete.cases(df_comp$sand),] 
df_comp =df_comp[complete.cases(df_comp$sable.0_30),] 
df_comp <- df_comp[!grepl("[^0-9.]", df_comp$sand), ]
df_comp$sand <- as.numeric(df_comp$sand)
df_comp$sable.0_30 <- as.numeric(df_comp$sable.0_30)
# colSums(is.na(df_comp))

df_comp = df_comp[!df_comp$sand== "NA",]
df_comp = df_comp[!df_comp$sable.0_30== "NaN",]
df_comp = droplevels(df_comp)
```


```{r,echo=TRUE}
# -   Deleting duplicate measured values

ID_Site_dupliques <- df_comp$ID_Site[duplicated(df_comp$ID_Site)]
#length(ID_Site_dupliques)

lignes_dupliquees <- subset(df_comp, duplicated(ID_Site) & duplicated(sand))

lignes_unique <- unique(lignes_dupliquees$ID_Site )
#length(lignes_unique)
# nrow(df_comp) - length(ID_Site_dupliques) + length(lignes_unique)

dupliquees <- duplicated(df_comp$ID_Site)
df_comp <- df_comp[!dupliquees, ]
df_comp=droplevels(df_comp)
df_comp$sand <- as.numeric(df_comp$sand)
df_comp$sable.0_30 <- as.numeric(df_comp$sable.0_30)

```


::: columns
::: {.column width="60%"}
-   Method ?

-   Depth ?

-   Measured values  (CR = `r df_col[df_col$Variable=="sand","CR"]`)

```{r}
  summary(df_comp$sand)
```

-   Extracted values

```{r}
  summary(df_comp$sable.0_30)
```
:::

::: {.column width="40%"}
\n\n\n

```{r,fig.align='center',fig.height=5,fig.width=4}
# graphique avec ggplot
correlation <- cor(as.numeric(df_comp$sand), df_comp$sable.0_30)
    p <- ggplot(df_comp, aes(x = sand, y = sable.0_30)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = FALSE, color = "red") + 
      labs(subtitle =paste("R = ", round(correlation, 2)) ,x = "Sand measured values", y = "Sand extracted values") + 
      theme_classic() 
p
```
:::
:::

## Silt
**Extracted values (g/kg, 0 - 30 cm)**

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = "limon.0_5",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/limon.0_5.tif")

bdd <- extraction(nom_col = "limon.5_15",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/limon.5_15.tif")

bdd <- extraction(nom_col = "limon.15_30",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/limon.15_30.tif")

bdd = moyenne_val_extrct(nom_col = "limon.0_30", vec_col = c("limon.0_5","limon.5_15","limon.15_30"),df=bdd)

# summary(bdd$limon.0_30)

explo_num(nom_col = "limon.0_30", titre = "Silt extracted values")
```

**Measured values & extracted values**

-   Clean silt column

```{r,echo=TRUE}
# On recupere les deux colonnes du pH
df_comp=bdd[, c("ID", "ID_Site","silt","limon.0_30" )]
df_comp =df_comp[complete.cases(df_comp$silt),] 
df_comp =df_comp[complete.cases(df_comp$limon.0_30),] 
df_comp <- df_comp[!grepl("[^0-9.]", df_comp$silt), ]
df_comp$silt <- as.numeric(df_comp$silt)
df_comp$limon.0_30 <- as.numeric(df_comp$limon.0_30)
# colSums(is.na(df_comp))


df_comp = df_comp[!df_comp$silt== "NA",]
df_comp = df_comp[!df_comp$limon.0_30== "NaN",]
df_comp = droplevels(df_comp)
```


```{r,echo=TRUE}
# -   Deleting duplicate measured values

ID_Site_dupliques <- df_comp$ID_Site[duplicated(df_comp$ID_Site)]
#length(ID_Site_dupliques)

lignes_dupliquees <- subset(df_comp, duplicated(ID_Site) & duplicated(silt))

lignes_unique <- unique(lignes_dupliquees$ID_Site )
#length(lignes_unique)
# nrow(df_comp) - length(ID_Site_dupliques) + length(lignes_unique)


dupliquees <- duplicated(df_comp$ID_Site)
df_comp <- df_comp[!dupliquees, ]
df_comp=droplevels(df_comp)
df_comp$silt <- as.numeric(df_comp$silt)
df_comp$limon.0_30 <- as.numeric(df_comp$limon.0_30)

```

::: columns
::: {.column width="60%"}
-   Method ?

-   Depth ?

-   Measured values  CR = `r df_col[df_col$Variable=="silt","CR"]`

```{r}
  summary(df_comp$silt)
```

-   Extracted values

```{r}
  summary(df_comp$limon.0_30)
```
:::

::: {.column width="40%"}
\n\n\n

```{r,fig.align='center',fig.height=5,fig.width=4}
# graphique avec ggplot
correlation <- cor(as.numeric(df_comp$silt), df_comp$limon.0_30)
    p <- ggplot(df_comp, aes(x = silt, y = limon.0_30)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = FALSE, color = "red") + 
      labs(subtitle =paste("R = ", round(correlation, 2)) ,x = "Silt measured values", y = "Silt extracted values") + 
      theme_classic() 
p
```
:::
:::


## Clay

**Extracted values (g/kg, 0 - 30 cm)**

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = "argile.0_5",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/argile.0_5.tif")

bdd <- extraction(nom_col = "argile.5_15",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/argile.5_15.tif")

bdd <- extraction(nom_col = "argile.15_30",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/argile.15_30.tif")

bdd = moyenne_val_extrct(nom_col = "argile.0_30", vec_col = c("argile.0_5","argile.5_15","argile.15_30"),df=bdd)

summary(bdd$argile.0_30)

explo_num(nom_col = "argile.0_30", titre = "Clay extracted values")
```

**Measured values & extracted values**
-   Clean clay column

```{r,echo=TRUE}
# On recupere les deux colonnes du pH
df_comp=bdd[, c("ID", "ID_Site","clay","argile.0_30" )]
df_comp =df_comp[complete.cases(df_comp$clay),] 
df_comp =df_comp[complete.cases(df_comp$argile.0_30),] 
df_comp <- df_comp[!grepl("[^0-9.]", df_comp$clay), ]
df_comp$clay <- as.numeric(df_comp$clay)
df_comp$argile.0_30 <- as.numeric(df_comp$argile.0_30)
# colSums(is.na(df_comp))

df_comp = df_comp[!df_comp$clay== "NA",]
df_comp = df_comp[!df_comp$argile.0_30== "NaN",]
df_comp = droplevels(df_comp)
```


```{r,echo=TRUE}
# -   Deleting duplicate measured values

ID_Site_dupliques <- df_comp$ID_Site[duplicated(df_comp$ID_Site)]
#length(ID_Site_dupliques)

lignes_dupliquees <- subset(df_comp, duplicated(ID_Site) & duplicated(clay))

lignes_unique <- unique(lignes_dupliquees$ID_Site )
#length(lignes_unique)
# nrow(df_comp) - length(ID_Site_dupliques) + length(lignes_unique)

dupliquees <- duplicated(df_comp$ID_Site)
df_comp <- df_comp[!dupliquees, ]
df_comp=droplevels(df_comp)
df_comp$clay <- as.numeric(df_comp$clay)
df_comp$argile.0_30 <- as.numeric(df_comp$argile.0_30)

```

::: columns
::: {.column width="60%"}
-   Method ?

-   Depth ?

-   Measured values  CR = `r df_col[df_col$Variable=="clay","CR"]`

```{r}
  summary(df_comp$clay)
```

-   Extracted values

```{r}
  summary(df_comp$argile.0_30)
```
:::

::: {.column width="40%"}
\n\n\n

```{r,fig.align='center',fig.height=5,fig.width=4}
# graphique avec ggplot
correlation <- cor(as.numeric(df_comp$clay), df_comp$argile.0_30)
    p <- ggplot(df_comp, aes(x = clay, y = argile.0_30)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = FALSE, color = "red") + 
      labs(subtitle =paste("R = ", round(correlation, 2)) ,x = "Clay measured values", y = "Clay extracted values") + 
      theme_classic() 
p
```
:::
:::


## Elevation

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = "elevation",df = bdd,conv = 10, 
                  tif_file_path ="C:/Users/diall/Downloads/datas/raster_modif/GMTED2010_Spatial.tif")

summary(bdd$elevation)

explo_num(nom_col = "elevation", titre = "Elevation")
```
