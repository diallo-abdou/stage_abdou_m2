---
title: "Internship progress"
author: "Abdourahmane Diallo"
date: '`r Sys.Date()`'
format: 
  revealjs
#smaller: true
scrollable: true
#theme: sky
editor: visual
number-sections: true
toc: FALSE
#toc-expand: false
#toc_float: 'yes'
code_download: 'yes'
slide-number: true
margin: 0.1
#center: true
code-fold: true
width: 1300
height: 700
toc_depth: 1
execute: 
  cache: true
---

# Setting

```{r setup, include=FALSE,fig.align='center',message=FALSE,warning=FALSE,message=FALSE,echo=TRUE}
# rm(list=ls()) # Properly clear workspace
# source("function_abdou.R")

```

## Packages

```{r,echo=TRUE}
  library(tidyverse)
  library(glme)
  library(lsmeans)
  library(agricolae)
  library(RVAideMemoire)
  library(corrplot)
  library(emmeans)
  library(lme4)
  library(multcomp)
  library(MASS)
  library(R2WinBUGS)
  library(arm)
  library(performance)
  library(AER)
  library(AICcmodavg)
  library(MuMIn)
  library(ade4)
  library(Hmisc)
  library(labdsv)
  library(vegan)
  library(cowplot)
  library(ggpubr)
  library(rstatix)
  library(patchwork)
  library(multcompView)
  library(ggsignif)
  library(grid)
  library(FactoMineR)
  library(factoextra)
  library(explore)
  library(ggrepel)
  library(naniar)
  library(outliers)
  library(leaps)
  library(fastDummies)
  library(caret) # pour l'entrainement des models
  library(mgcv)
  library(ggeffects)
  library(gratia)
  library(GGally)
  library(caTools)
  library(rpart)
  library(rpart.plot)
  library(openxlsx)
  library(readxl)
  library(leaflet)
  library(quarto)
  library(raster)
  library(knitr)
  library(kableExtra)
  library(stringr)
  library(plotly)
  library(PerformanceAnalytics)
  library(usdm)
  library(vcd) # pour la distribution des var reponse
  library(prospectr)# pour split data avec kenSton()
  library(glmnet)
  library(randomForest)
  library(doParallel)
  library(gbm)
  library(kernlab)
  library(e1071)
  library(ggforce)
  #library(keras)
  #library(tensorflow)
  library(neuralnet)
  library(parallel)
  library(iml) # pour l'interpretabilité des models https://cran.r-project.org/web/packages/iml/vignettes/intro.html
  library(e1071)



```

## Functions

```{r, echo=TRUE}

## Identification des NA dans un df -----------------------------------------------
taux_completion<-
  function(df, afficher_zero_percent = FALSE, seuil, trie=FALSE) {
    # Calcule du pourcentage de NA dans le dataframe
    pourcentage_total <-
      round(sum(is.na(df)) / (nrow(df) * ncol(df)) * 100, 1)
    
    # Calcule du pourcentage de NA par colonne
    pourcentage_colonnes <- round(colMeans(is.na(df)) * 100, 1)
    
    # Creation d'un dataframe résultat avec deux colonnes
    result <-
      data.frame(
        Variables = names(df),
        CR = pourcentage_colonnes,
        row.names = NULL
      )
    
    if (afficher_zero_percent) {
      result <- result[result$CR == 0, ]
      result$CR = 100 -result$CR
    } else {
      result <- result[result$CR > 0, ]
      result$CR = 100 -result$CR
      
    }
    
    result <- rbind(result, c("Total", pourcentage_total))
    #result <- rbind(result, c("Total", paste0(pourcentage_total, "")))
    
    result <- result[, c("Variables", "CR")]
    result$CR = as.numeric(result$CR)
    result$CR = round(result$CR,1)
    if (trie){
      result = result %>% arrange(desc(CR))
    }
    result$CR = paste0(result$CR,"%")
    
    return(result)
  }
# Converssion des colonne en num ou factor-----------------------------------------------
conv_col <- function (data, columns_to_convert, to_types) {
  if (to_types == "numeric") {
    # Conversion des colonnes en numeric
    for (col in columns_to_convert) {
      data[, col] <- as.numeric(data[, col])
    }
  } else {
    # Conversion des colonnes en facteurs
    for (col in columns_to_convert) {
      data[, col] <- as.factor(data[, col])
    }
  }
  return(data)
}
#data_converted <- conv_col(data, names(data [, c(1, 3)]), "factor")

# exploration graphiques des variables numeriques -----------------------------------------------
explo_num <- function(nom_col, titre, df = bdd, ligne_col = c(2, 2),mini = min(df[[nom_col]]), maxi=max(df[[nom_col]]) ) {
  par(mfrow = ligne_col)
  
  df[complete.cases(df[[nom_col]]), ]
  df <- df %>%filter(!is.na(df[[nom_col]]))
  df[[nom_col]] = as.numeric(df[[nom_col]])
  # Boxplot
  boxplot(df[[nom_col]], col = 'blue', ylab = titre, ylim = c(mini, maxi))
  # Cleveland plot
  dotchart(df[[nom_col]], pch = 16, col = 'blue', xlab = titre)
  # Histogram
  hist(df[[nom_col]], col = 'blue', xlab = titre, main = "")
  # Quantile-Quantile plot
  qqnorm(df[[nom_col]], pch = 16, col = 'blue', xlab = '')
  qqline(df[[nom_col]], col = 'red') 
}

# Extraction des predictors + moyennes -----------------------------------------------

extraction <- function(nom_col, tif_file_path, df = bdd, conv = 1) {
  #df <- df %>%filter(!is.na(gps_x) & !is.na(gps_y))
  raster_data <- raster(tif_file_path)
  
  # Création d'un dataframe pour stocker les valeurs extraites
  df_interne <- data.frame(gps_x = df$gps_x, gps_y = df$gps_y)
  proj4Str <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"
  # Transformer les coordonnées GPS en système de coordonnées du raster
  gps_coords_sp <- SpatialPoints(df_interne, proj4string = CRS(proj4Str))
  gps_coords_proj <- spTransform(gps_coords_sp, crs(raster_data))
  
  # Extraction des valeurs du raster 
  values <- raster::extract(raster_data, gps_coords_proj)
  
  # Ajout des valeurs extraites comme nouvelles colonnes a df
  #df_save = data.frame()
  #df_save[[nom_col]] <- values / conv
  
  df[[nom_col]] <- values / conv
  
  return(df)
}

# la moyenne des predictores -----------------------------------------------
moyenne_val_extrct <- function(nom_col, vec_col, df=bdd) {
  df[[nom_col]] <- rowMeans(as.matrix(df[, vec_col, drop = FALSE]), na.rm = TRUE)
  df[[nom_col]] = round(df[[nom_col]],1)
  return(as.data.frame(df))
}


# tests de corrélation avec un seuil -----------------------------------------------
cor_function_seuil <- function(data, seuil,affiche=FALSE) {
  # Création d'un vecteur pour stocker les paires de variables corrélées
  variables_corr <- c()
  
  # Boucle pour tester la corrélation entre chaque paire de variables
  for (i in 1:(ncol(data) - 1)) {
    for (j in (i + 1):ncol(data)) {
      # Calcul de la corrélation entre les variables i et j
      cor_value <- stats::cor(data[, i], data[, j], use = "na.or.complete")
      
      # Stockage du résultat dans le vecteur si supérieur au seuil
      if (cor_value >= seuil | cor_value <= -seuil) {
        if(affiche){
        cat(
          "***",
          colnames(data)[i],
          "  __est correlee a__  ",
          colnames(data)[j],
          "avec un R =",
          cor_value,
          "\n \n \n"
        )
      }
        
        variables_corr <-
          c(variables_corr, colnames(data)[i], colnames(data)[j])
      }
    }
  }
  
  return(variables_corr)
}


# tests de valeurs aberant -----------------------------------------------
verifier_valeurs_aberrantes <- function(data, variable, opposite = FALSE) {
 id_ligne=c()
    test_aberrant <- grubbs.test(data[[variable]], opposite = opposite)
    p.value <- test_aberrant$p.value
    cat(p.value, "\n")
      texte <- test_aberrant$alternative
      chiffres <- regmatches(texte, regexpr("-?\\d+\\.\\d+", texte))
      id_ligne <- data[which(data[[variable]] == as.numeric(chiffres)), "ID"]
  return(id_ligne)
}

```

# Plan

-   Setting

-   Database import

-   Database exploration

-   Earthworms data

-   Soil data extraction

-   Climate data extraction

-   To do next

# Database import

-   Import of database **LandWorm_dataset_site_V1.9.xlsx** (february 22, 2024)

```{r import,echo=FALSE}
chemin_fichier_excel = "C:/Users/diall/Downloads/datas/LandWorm_dataset_site_V1.9.xlsx"
bdd <- read.xlsx(chemin_fichier_excel, sheet = "Sheet1")
```

-   The database contains **`r nrow(bdd)`** rows and **`r ncol(bdd)`** columns

```{r conversion,echo=FALSE}
col_en_factor = c("Programme","Annee","ID_Site","Code_Parcelle","postal_code","clcm_lvl1",
                  "clcm_lvl2","clcm_lvl3","Modalite","Bloc","Protocole","land_cover_detail","type_tillage","fertilisation","ferti_min_product","ferti_orga_product")
bdd = conv_col(bdd, col_en_factor, "factor")
```

## Data selection: EcoBioSoil

```{r selection dc1,echo=FALSE}
n_line=nrow(bdd)
bdd$owner=as.factor(bdd$owner)
summary_df <- as.data.frame(summary(bdd$owner))
colnames(summary_df) <- c("Numbers")
kable(summary_df)
```

```{r selection dc2,echo=FALSE}
bdd <- subset(bdd, owner == "dc")
bdd$owner=droplevels(bdd$owner)

```

-   The database therefore changes from **`r n_line`** to **`r nrow(bdd)`** observations.

# Database exploration

-   CR = Completion rate

## Complete columns

```{r Complete columns, echo=TRUE}
df_col=taux_completion(bdd,TRUE,trie=FALSE)
df_col = df_col[df_col$Variables != "Total",]
#print("table")
kable(df_col, caption = "", col.width = c("75%", "25%"))
# cat(                                                    )
# head(bdd[, "ID"])
```

## Non-complete columns

```{r Non-complete columns, scrollable = TRUE}
df_col= taux_completion(bdd,FALSE,trie = TRUE)
df_col = df_col[df_col$Variables != "Total",]
kable(df_col, caption = " ", col.width = c("75%", "25%"))
```

## Focus on GPS coordinates

-   There is **`r sum(is.na(bdd$gps_x))`** NA (CR = `r df_col[df_col$Variable=="gps_x", "CR"]`) in **GPS_X**
-   There is **`r sum(is.na(bdd$gps_y))`** NA (CR = `r df_col[df_col$Variable=="gps_y", "CR"]`) in **GPS_Y**

```{r GPS,echo=TRUE}
n_line= nrow(bdd)
bdd$gps_x <- as.numeric(gsub("[^0-9.-]", "", bdd$gps_x))
bdd$gps_y <- as.numeric(gsub("[^0-9.-]", "", bdd$gps_y))
bdd <- bdd[complete.cases(bdd$gps_x, bdd$gps_y), ]
bdd <- bdd %>%filter(!is.na(gps_x) & !is.na(gps_y))
#sum(is.na(bdd$gps_x))
#sum(is.na(bdd$gps_y))
```

-   We delete the *NA* lines in the GPS coordinates
-   The database therefore changes from **`r n_line`** to **`r nrow(bdd)`** observations.
-   Merging database and climat database

```{r mergins & climat, echo=TRUE}
# Ajout variables climatiques (voir chunk extraction données climatiques)
chemin_fichier <- "C:/Users/diall/OneDrive/Bureau/M2_MODE/stage_abdou_m2/datas/bdd_climat_ok.rds"
# saveRDS(bdd_climat_ok, chemin_fichier)
bdd_climat_ok <- readRDS(chemin_fichier)
df_fusion <- subset(bdd_climat_ok, select = -c(gps_x, gps_y))

rows_not_in_df_fusion <- anti_join(bdd, df_fusion, by = "ID")
merged_df <- merge(bdd, df_fusion, by = "ID")

ids_not_matching <- anti_join( merged_df,bdd, by = "ID")

bdd = merged_df

#bdd <- cbind(bdd, df_fusion) # all = TRUE pour garder toutes les lignes
```

## Cartography

```{r Cartography,echo=TRUE}
n_ligne= nrow(bdd)
df_coord <- bdd[, c("gps_x", "gps_y")] %>% mutate(gps_x = as.numeric(gps_x),gps_y = as.numeric(gps_y))

df_coord$num_ligne <- seq(nrow(df_coord))
carte <- leaflet(df_coord) %>%
  addTiles() %>%
  addCircleMarkers(lng = ~gps_x, lat = ~gps_y, radius = 0.8, fillOpacity = 0.8, fillColor = "blue")
carte
```

```{r outside France}
hors_france= read.csv(file = "C:/Users/diall/Downloads/datas/hors_france.csv", header = TRUE)

bdd <- bdd[!(bdd$gps_x %in% hors_france$gps_x & bdd$gps_y %in% hors_france$gps_y), ]
bdd <- droplevels(bdd)
```

-   We delete points outside France (**`r nrow(hors_france)`**)
-   The database therefore changes from **`r n_ligne`** to **`r nrow(bdd)`** observations.

## Focus on years

-   Cleaning the Annee column 
<br/> 
<!--
```{r years1, echo=TRUE}
# levels(bdd$Annee) # parfois années et jours et ou mois
# bdd$Annee= as.factor(bdd$Annee)
# bdd$Annee <- gsub("^(\\d{4}).*$", "\\1", bdd$Annee) # on prend uniquement les 04 premier chiffre
# bdd$Annee= as.factor(bdd$Annee)

```
-->

-   CR of Annee = **`r df_col[df_col$Variable=="Annee", "CR"]`** (`r length(levels(bdd$Annee))` levels)

```{r years2, echo=TRUE, scrollable = TRUE}
bdd$Annee= as.factor(bdd$Annee)
summary_df <- as.data.frame(summary(bdd$Annee))
colnames(summary_df) <- c("Numbers")
kable(summary_df)
```


<!-- 
-   We remove all the years before **1990** and the NA 

```{r years3, echo=TRUE, scrollable = TRUE}
n_ligne =nrow(bdd)
bdd <- bdd %>%filter(!is.na(Annee))# on enleve les NA
annes_omit= c("1821", "1960", "1978", "1982", "1983", "1984", "1986", "1988", "1989") # annee sup
bdd <- bdd[!bdd$Annee %in% annes_omit, ]
bdd=droplevels(bdd)
#levels (bdd$Annee)
#summary (bdd$Annee)
summary_df <- as.data.frame(summary(bdd$Annee))
colnames(summary_df) <- c("Numbers")
kable(summary_df,padding = 5)
```

-   The database therefore changes from **`r n_ligne`** to **`r nrow(bdd)`** observations. 
-->



<!--
## Table land use & protocol ( start )

-   clcm_lvl1 & protocol

```{r clcm_lvl1 & protocol, echo=TRUE}
kable(table(bdd$clcm_lvl1, bdd$Protocole,exclude = NULL),padding = 10,align = "c")

```

\n\n\n

-   clcm_lvl2 & protocol

```{r clcm_lvl2 & protocol, echo=TRUE}
kable(table(bdd$clcm_lvl2, bdd$Protocole,exclude = NULL),padding = 10,align = "c")
```

\n\n\n

-   clcm_lvl3 & protocol

```{r clcm_lvl3 & protocol, echo=TRUE}
kable(table(bdd$clcm_lvl3, bdd$Protocole,exclude = NULL),padding = 0,align = "c")
```
-->


## Focus on protocols

-   List of protocols available on the database ( `r length(levels(bdd$Protocole))` levels)

```{r protocols,echo=TRUE}
bdd$Protocole = as.factor(bdd$Protocole)
summary_df <- as.data.frame(summary(bdd$Protocole))
colnames(summary_df) <- c("Numbers")
kable(summary_df,padding = 5)
```


<!--
-   Selection of protocols: **F_HS, FHS, hand sorting, HS**
-->

-   Selection of protocols: **F_HS, HS**

```{r select protocols,echo=TRUE}
n_ligne = nrow(bdd)
#select_protocole =c("F_HS", "FHS", "hand sorting" ,"HS")
select_protocole =c("F_HS", "HS")
bdd <- bdd[bdd$Protocole %in% select_protocole, ]
bdd=droplevels(bdd)
bdd$Protocole = as.factor(bdd$Protocole)
summary_df <- as.data.frame(summary(bdd$Protocole))
colnames(summary_df) <- c("Numbers")
kable(summary_df,padding = 5)
```

-   The database therefore changes from **`r n_ligne`** to **`r nrow(bdd)`** observations.


<!--
-   Merging levels :

    -   F_HS $=$ F_HS $+$ FHS
    -   HS $=$ HS $+$ hand sorting

    ```{r merging protocols,echo=TRUE}
    levels(bdd$Protocole)[levels(bdd$Protocole) == "FHS"] <- "F_HS"
    levels(bdd$Protocole)[levels(bdd$Protocole) == "hand sorting"] <- "HS"
    bdd$Protocole = as.factor(bdd$Protocole)
    summary_df <- as.data.frame(summary(bdd$Protocole))
    colnames(summary_df) <- c("Numbers")
    kable(summary_df,padding = 5)
    ```
-->

## Focus on clcm_lvl1

-   CR of clcm_lvl1 = **`r df_col[df_col$Variable=="clcm_lvl1","CR"]`** (`r length(levels(bdd$clcm_lvl1))` levels)

```{r clcm_lvl1, echo=TRUE}
bdd$clcm_lvl1= as.factor(bdd$clcm_lvl1)
summary_df <- as.data.frame(summary(bdd$clcm_lvl1))
colnames(summary_df) <- c("Numbers")
# kable(summary_df,padding = 5)
```

-   Merging levels

```{r merging clcm_lvl1, echo=TRUE}
levels(bdd$clcm_lvl1)[levels(bdd$clcm_lvl1) == "1_Naturel"] <- "Forest and semi natural areas"
levels(bdd$clcm_lvl1)[levels(bdd$clcm_lvl1) == "2_Agricole"] <- "Agricultural areas"

bdd$clcm_lvl1= as.factor(bdd$clcm_lvl1)
summary_df <- as.data.frame(summary(bdd$clcm_lvl1))
colnames(summary_df) <- c("Numbers")
kable(summary_df,padding = 5)
```

-   Update **code_clcm_lvl1**

```{r code_clcm_lvl1, echo=TRUE}
#bdd$code_clcm_lvl1 = as.factor(bdd$code_clcm_lvl1)

bdd$code_clcm_lvl1 <- ifelse(bdd$clcm_lvl1 == "Forest and semi natural areas", 3, bdd$code_clcm_lvl1)

bdd$code_clcm_lvl1 <- ifelse(bdd$clcm_lvl1 == "Agricultural areas", 2, bdd$code_clcm_lvl1)
```

-   For the moment, we will keep the NA of **clcm_lvl1**

## Focus on clcm_lvl2

-   CR of clcm_lvl2 = **`r df_col[df_col$Variable=="clcm_lvl2","CR"]`** (`r length(levels(bdd$clcm_lvl2))` levels)

```{r clcm_lvl2 , echo=TRUE}
bdd$clcm_lvl2= as.factor(bdd$clcm_lvl2)
summary_df <- as.data.frame(summary(bdd$clcm_lvl2))
colnames(summary_df) <- c("Numbers")
# kable(summary_df,padding = 8)
```

-   Merging levels

```{r merging clcm_lvl2, echo=TRUE}
levels(bdd$clcm_lvl2)[levels(bdd$clcm_lvl2) == "21_Agricole ouvert"] <- "Arable land"

bdd$clcm_lvl2= as.factor(bdd$clcm_lvl2)
summary_df <- as.data.frame(summary(bdd$clcm_lvl2))
colnames(summary_df) <- c("Numbers")
kable(summary_df,padding = 5)
```

-   Update **code_clcm_lvl2**

```{r code_clcm_lvl2, echo=TRUE}

bdd$code_clcm_lvl2 <- ifelse(bdd$clcm_lvl2 == "Arable land", 21, bdd$code_clcm_lvl2)

```

## Focus on clcm_lvl3

-   CR of clcm_lvl3 = **`r df_col[df_col$Variable=="clcm_lvl3","CR"]`** (`r length(levels(bdd$clcm_lvl3))` levels)

```{r clcm_lvl3, echo=TRUE, scrollable = TRUE}
bdd$clcm_lvl3= as.factor(bdd$clcm_lvl3)
summary_df <- as.data.frame(summary(bdd$clcm_lvl3))
colnames(summary_df) <- c("Numbers")
kable(summary_df,padding = 5)

```

## Land use selection (clcm_lvl3)


<!--
-   **Broad-leaved forest** 
-   **Coniferous forest** 
-   **Mixed forest** 

-   **Pastures, meadows and other permanent grasslands under agricultural use** 

-   **Non-irrigated arable land** 

-   **Vineyards**

-   **Green urban areas**

-   **Natural grasslands**
-->

```{r select clcm_lvl3, echo=TRUE}
select_os= c("Broad-leaved forest", "Coniferous forest", "Mixed forest", 
"Pastures, meadows and other permanent grasslands under agricultural use", "Non-irrigated arable land", 
"Vineyards","Green urban areas","Natural grasslands")

bdd <- bdd[bdd$clcm_lvl3 %in% select_os, ]
bdd=droplevels(bdd)
bdd$clcm_lvl3 = as.factor(bdd$clcm_lvl3)
summary_df <- as.data.frame(summary(bdd$clcm_lvl3))
colnames(summary_df) <- c("Numbers")
kable(summary_df)

```

-   **Maybe, we can merge the three types of forest ?**

## Land use & protocol overview

```{r LU & protocol overview, echo=TRUE}
# kable (table(bdd$clcm_lvl1, bdd$Protocole,exclude = NULL), align = "c", format = "pipe", padding = 10)
# kable (table(bdd$clcm_lvl2, bdd$Protocole,exclude = NULL), align = "c", format = "pipe", padding = 10)
kable (table(bdd$clcm_lvl3, bdd$Protocole,exclude = NULL), align = "c", format = "pipe", padding = 10)
```


<!--
# Earthworms data

## Total abundance (CR = 100 % )

```{r fig AB_tot,fig.align='center',fig.height=10}
# summary(bdd$AB_tot) 
# explo_num(nom_col = "AB_tot", titre = "Total abundance")
# On supprime les parcelles dont l'AB_tot sont sup a 1500 ind/m²
id_ligne <- bdd[which(bdd$AB_tot >=1500), "ID"] 
bdd <- bdd[!bdd$ID %in% id_ligne, ]
bdd=droplevels(bdd)


summary(bdd$AB_tot) 
explo_num(nom_col = "AB_tot", titre = "Total abundance")
```


## Total biomass (CR = `r df_col[df_col$Variable=="BM_tot","CR"]`)

```{r fig BM_tot,fig.align='center',fig.height=10}
# summary(bdd$BM_tot) 
# explo_num(nom_col = "BM_tot", titre = "Total biomass")
# On supprime les parcelles dont la BM_tot sont sup a 500g/m²

id_ligne <- bdd[which(bdd$BM_tot >=500), "ID"] 
bdd <- bdd[!bdd$ID %in% id_ligne, ]
bdd=droplevels(bdd)

summary(bdd$BM_tot) 
explo_num(nom_col = "BM_tot", titre = "Total biomass")
```


## Total richness calculation method

-   Removal of columns with only NA (**`r length(colnames(bdd)[colSums(is.na(bdd)) == nrow(bdd)])`**) and/or only 0
-   Identify columns beginning with **AB\_**
-   Deletion of **AB\_** columns that are not species
-   Calculate richness by assigning **1** to each column if the value is different from 0 and NA
-   Total richness = **1** if the plot has a value in AB and/or BM
-->

```{r calcul richness, echo=FALSE}
# on supprime tout les colonnes ayant que des NA
colonnes_na <- colnames(bdd)[colSums(is.na(bdd)) == nrow(bdd)]
# summary(bdd[, colonnes_na])
bdd <- bdd[, !colnames(bdd) %in% colonnes_na]



# On supprimme toutes les colonnes ayant que des NA et des 0
colonnes_numeriques <- sapply(bdd, is.numeric)
somme_colonnes_numeriques <- colSums(bdd[, colonnes_numeriques],na.rm=TRUE)
colonnes_zeros <- names(somme_colonnes_numeriques[somme_colonnes_numeriques == 0])
#summary(bdd[, colonnes_zeros])
bdd <- bdd[, !colnames(bdd) %in% colonnes_zeros]



# On récupère toutes les colonnes qui commencent par **AB_**
colonnes_AB <- grep("^AB_", names(bdd), value = TRUE)



# On supprimme les colonnes AB_ qui ne sont pas des espèces dans le calcule
ab_supprimee =  c("AB_AD","AB_JV","AB_SA","AB_STAD_X","AB_indéterminable","AB_Indéterminable","AB_indéterminable_endogeic","AB_tot","AB_Indéterminable_epigeic","AB_indéterminable_endogeic","AB_Ep.X","AB_vide", "AB_Ep.X1","AB_Ep.X2","AB_A.X","AB_Adult","AB_cocon","AB_indéterminé","AB_Juvenile","AB_Sub.adult","AB_Indéterminé","AB_Lumbricidae")
colonnes_AB <- colonnes_AB[!colonnes_AB %in% ab_supprimee]



# On calcule la richesse en attribiant 1 à chaque colonne si la valeur est différent de 0 et de NA
bdd$Richesse_tot <- 0
bdd$Richesse_tot <- rowSums(!is.na(bdd[colonnes_AB]) & bdd[colonnes_AB] != 0)
#sum (is.na(bdd$Richesse_tot) )
#summary(bdd$Richesse_tot)



# Check des lignes ayant des 0 richesse et X AB ou BM : 0 lignes
# vdt_a_checker = bdd[bdd$Richesse_tot == 0 & (bdd$Total_AB !=0 | bdd$BM_to !=0), c("ID_Site","AB_tot","BM_tot","Richesse_tot")]
# vdt_a_checker = subset(vdt_a_checker, Richesse_tot==0)
# View(vdt_a_checker)
# vdt_a_checker$Richesse_tot <- 1
# Mettre à jour les ligne correspondant dans la bdd 
# bdd[rownames(bdd) %in% rownames(vdt_a_checker), "Richesse_tot"] <- 1



# Check si y a des ligne ayant que des NA dans AB, BM et Richesse : nop
resultat <- subset(bdd, is.na(AB_tot) & is.na(BM_tot) & is.na(Richesse_tot))
# View(resultat[, c("AB_tot","BM_tot", "Richesse_tot")])



# Check si y a des ligne ayant que des zéros ou des NA dans AB, BM et Richesse_tot: 66 ligne
vdt <- c("AB_tot", "BM_tot", "Richesse_tot")
lignes_zero <- which(rowSums(bdd[vdt] != 0, na.rm = TRUE) == 0)
# View(bdd[lignes_zero,c("ID_Site","AB_tot", "BM_tot", "Richesse_tot")])



# Check des lignes ayant de BM mais pas de AB
bm_sans_ab <- subset(bdd, AB_tot == 0 & BM_tot != 0)
# bm_sans_ab[, c("ID","ID_Site", "Programme", "Protocole", "AB_tot", "BM_tot")]

ab_sans_bm <- subset(bdd, BM_tot == 0 & AB_tot != 0) # 1 parcelles
# ab_sans_bm[, c("ID","ID_Site", "Programme", "Protocole", "AB_tot", "BM_tot")]


# Check des doublons

#duplicated_rows <- subset(bdd, duplicated(bdd[, c("ID", "AB_tot", "BM_tot")]) | #duplicated(bdd[, c("ID", "AB_tot", "BM_tot")], fromLast = TRUE))

```


<!--
## Total richness (CR = 100 % )

```{r fig richness, fig.align='center',fig.height=10}
summary(bdd$Richesse_tot)
#bdd <- subset(bdd, Richesse_tot <= 2000)
explo_num(nom_col = "Richesse_tot", titre = "Total richness",)
```
-->



<!--
# Synthèse du taux de remplissage

## Complete columns

```{r synt CR, echo=TRUE}
df_col=taux_completion(bdd,TRUE,trie=FALSE)
df_col = df_col[df_col$Variables != "Total",]
kable(df_col, caption = " ", col.width = c("75%", "25%"))
```

## Non-complete columns

```{r, scrollable = TRUE}
df_col= taux_completion(bdd,FALSE,trie = TRUE)
df_col = df_col[df_col$Variables != "Total",]
kable(df_col, caption = " ", col.width = c("75%", "25%"))
```

-->



<!--
# Climate data extraction
## The source database ([CHELSA V2](https://chelsa-climate.org/bioclim/){target="_blank"})

```{r Climate df extraction,echo=TRUE}

# Lire le fichier Excel
chemin_fichier_excel <- "C:/Users/diall/Downloads/datas/ODMAP.xlsx"
climat <- read.xlsx(chemin_fichier_excel, sheet = "climat")

# Fusions des cellules des colonnes avec des éléments dupliqués
for (col in names(climat)) {
  climat[[col]] <- ifelse(duplicated(climat[[col]]), "", climat[[col]])
}

# Affichage du tableau avec kableExtra et centrage du contenu des cellules
kableExtra::kable(climat) %>%
  kableExtra::kable_styling() %>%
  kableExtra::column_spec(1:ncol(climat)) 

```

## Extraction method

-   Link recovery ( see file [link .tif](https://1drv.ms/t/s!Avfm81EzNGBHjIZWw8YePljXaGSpCQ?e=qIPeWR){target="_blank"} )

-   Extracting variable names

-   Uses of the **extraction()** function

-   Convert columns to correct format and unit

-   Adding variables to the LANDWORM database

```{r Extraction method,echo=TRUE}
liens_tif = read.table(file = "C:/Users/diall/Downloads/datas/envidatS3paths.txt")
liens_tif$shortname <- str_extract(liens_tif$V1, "(?<=CHELSA_).*?(?=_1981)")
liens_tif[liens_tif$shortname=="rsds","shortname"]=c("rsds_max","rsds_mean","rsds_min","rsds_range")

#all(is.na(bdd$gps_x))
#all(is.na(bdd$gps_y))

bdd_climat= bdd[, c("ID","gps_x","gps_y")]

temp_1=Sys.time()
#for( i in 1:nrow(liens_tif)){
  #nom=liens_tif[i,c("shortname")]
  #df_ext <- extraction(nom_col = nom,df = bdd_climat,conv = 1, 
                  #tif_file_path = liens_tif[i,c("V1")] ) 
  #bdd_climat[[nom]] <- df_ext [,nom]
  #rm("df_ext","nom")
  #cat("Extraction: ",i,"/",nrow(liens_tif), "\n")
#}
temp_2=Sys.time()
duree= difftime(temp_2,temp_1)

chemin_fichier <- "C:/Users/diall/OneDrive/Bureau/M2_MODE/stage_abdou_m2/datas/bdd_climat.rds"
# saveRDS(bdd_climat, chemin_fichier)
#bdd_climat <- readRDS(chemin_fichier)

# debut cnversion ------------------------------------------------------------
conv_df_climat= data.frame(shortname =liens_tif$shortname )

# unit = 1
conv_df_climat$unit = rep(1)
# unit = 100
unit_100=c("bio4")
conv_df_climat$unit <- ifelse(conv_df_climat$shortname %in% unit_100, 100, 1)


# scale = 0.1
conv_df_climat$scale = rep(0.1)
# scale = 1
scale_1=c("fcf","fgd","gddlgd0","gddlgd5","gddlgd10","gdgfgd0","gdgfgd5","gdgfgd10","gsl","kg0","kg1" ,"kg2" ,"kg3" ,"kg4" ,"kg5","lgd","ngd0","ngd5","ngd10","scd")

# scale = 0.01
scale_01=c("hurs_max","hurs_mean","hurs_min","hurs_range","pet_penman_max",
       "pet_penman_mean","pet_penman_min","pet_penman_range")

# scale = 0.001
scale_001=c("rsds","sfcWind_max","sfcWind_mean","sfcWind_min","sfcWind_range","pet_penman_max","pet_penman_mean","pet_penman_min","pet_penman_range","rsds_max","rsds_mean","rsds_min","rsds_range")

# Remplacement des valeurs de l'échelle en fonction des conditions
conv_df_climat$scale <- ifelse(conv_df_climat$shortname %in% scale_1, 1,
              ifelse(conv_df_climat$shortname %in% scale_01, 0.01,
                    ifelse(conv_df_climat$shortname %in% scale_001,0.001, 0.1)))

# offset = 0
conv_df_climat$offset = rep(0)
# offset = - 273.15
offset_273=c("bio1","bio5","bio6","bio8","bio9","bio10","bio11","gdgfgd10","gsl","gst")
conv_df_climat$offset = ifelse(conv_df_climat$shortname %in% offset_273, -273.15, 0)

# Pas present dans dans le pdf explicative donc pas de conversion
pas_pdf=c( "ai","swb", "clt_max","clt_mean","clt_min","clt_range")
verif=c(unit_100,scale_1,scale_01,scale_001,offset_273)
pas_pdf_2=setdiff(conv_df_climat$shortname, verif)
conv_df_climat[conv_df_climat$shortname %in% pas_pdf,"scale"] = 1

#bdd_climat_ok=bdd_climat[,c("ID","gps_x","gps_y")]

#for ( i in conv_df_climat$shortname){
  #if (i %in% names(bdd_climat)){
  #unitee= conv_df_climat[conv_df_climat$shortname ==i,"unit"]
  #echelle = conv_df_climat[conv_df_climat$shortname ==i,"scale"]
  #decalage = conv_df_climat[conv_df_climat$shortname ==i,"offset"]
  #bdd_climat_ok[[i]] = ((bdd_climat[[i]] / unitee)* echelle) + decalage
  #}else {
    #cat("Attention ",i, "n'exite pas dans la bdd_climat","\n")
  #}
#}


# chemin_fichier <- "C:/Users/diall/OneDrive/Bureau/M2_MODE/stage_abdou_m2/datas/bdd_climat_ok.rds"
# saveRDS(bdd_climat_ok, chemin_fichier)
# bdd_climat_ok <- readRDS(chemin_fichier)
# fin conversion

#df_fusion <- subset(bdd_climat_ok, select = -c(ID,gps_x, gps_y))
#bdd <- cbind(bdd, df_fusion) # all = TRUE pour garder toutes les lignes
```

## List of variables

[Variable description](https://chelsa-climate.org/wp-admin/download-page/CHELSA_tech_specification_V2.pdf){target="_blank"}

```{r}
summary(bdd_climat_ok)
```
-->


<!--
## Temperature

-   Average annual air temperature (°C) = bio1

```{r Temperature,fig.align='center',fig.height=8}
summary(bdd$bio1)
explo_num(nom_col = "bio1", titre = "temp°.")
```

## Precipitation

-   Annual precipitation (kg/m²) = bio12

```{r Precipitation,fig.align='center',fig.height=8}
summary(bdd$bio12)
explo_num(nom_col = "bio12", titre = "Précipitat°.")
```

-->



<!--
# Questions

```{r Questions}
ID_Site_dupliques <- bdd$ID_Site[duplicated(bdd$ID_Site)]
#length(ID_Site_dupliques)

lignes_dupliquees <- subset(bdd, duplicated(ID_Site))

lignes_unique <- unique(lignes_dupliquees$ID_Site)
#length(lignes_unique)

# nrow(bdd) - length(ID_Site_dupliques) + length(lignes_unique)
```

-   Comment gérer la répétition temporelle des parcelles ?
    -   Avec répétition : **`r nrow(bdd)`** observations
    -   Sans répétition : **`r nrow(bdd) - length(ID_Site_dupliques) + length(lignes_unique)`** observations
-   Liens des données du sol (sable, argile et limon) de data.gouv.fr ?


# To do next

-   

    1.  Extraction des prédicteurs : CEC, Limon, argile, évapotranspiration, paysage,...

-   

    2.  Analyse exploratoire : test de corrélation, VIF, ACP

-   

    3.  Sélection des variables

-   

    4.  Modélisation (GLM, GAM, RF, ANN)

-   

    5.  Validation croisée

-   

    6.  Rédaction, protocol ODMAP






-->

# Soil data extraction

```{r Soil data extraction}
# Calcul des distances euclidiennes entre les sites
distances <- dist(cbind(bdd$gps_x, bdd$gps_y))
distance_moyenne <- mean(distances)
# distance_moyenne

df_col= taux_completion(bdd,FALSE,trie = TRUE)
df_col = df_col[df_col$Variables != "Total",]
```

<!--
## The source database ([openlandmap](https://openlandmap.org/?center=25,39&zoom=4&opacity=72&base=OpenStreetMap&layer=lc_glc.fcs30d&time=2022){target="_blank"})

```{r soil source database,echo=TRUE}
chemin_fichier_excel <- "C:/Users/diall/Downloads/datas/ODMAP.xlsx"
pedo <- read.xlsx(chemin_fichier_excel, sheet = "pedo")

# Fusion des cellules des colonnes avec des éléments dupliqués
for (col in names(pedo)) {
  pedo[[col]] <- ifelse(duplicated(pedo[[col]]), "", pedo[[col]])
}

#tableau avec kableExtra et centrage du contenu des cellules
kableExtra::kable(pedo) %>%
  kableExtra::kable_styling() %>%
  kableExtra::column_spec(1:ncol(pedo))  # Centrer le contenu de toutes les colonnes
```

\n
-   Average values between surface (0 cm) and 30 cm depth


## Changing the resolution ![](https://logowik.com/content/uploads/images/python.jpg){width="200"}

-   Long compilation time in R

-   GDAL module with the resampleAlg = bilinear method

-   Resolution = 0.0083 = 30 arc-second \~ 1km

```{r changing resolution}
    test_resolution = bdd
    tif_file_path_origine = "C:/Users/diall/Downloads/datas/raster_origine/sol_ph.h2o_usda.4c1a2a_m_250m_b10..10cm_1950..2017_v0.2.tif"
    raster_ph_origine <- raster(tif_file_path_origine)
    test_resolution <- extraction(nom_col = "ph_10_origine",df = test_resolution,conv = 10, 
                      tif_file_path = tif_file_path_origine)


    tif_file_path_rech = "C:/Users/diall/Downloads/datas/raster_modif/sol_ph.h2o_usda.4c1a2a_m_250m_b10..10cm_1950..2017_v0.2.tif"
    raster_ph_rech <- raster(tif_file_path_rech)
    test_resolution <- extraction(nom_col = "ph_10_rech",df = test_resolution,conv = 10, 
                      tif_file_path = tif_file_path_rech)

    par(mforw=c(1,2))
    image(raster_ph_origine,main="pH at 10cm: original raster (0.002)")
    image(raster_ph_rech, main = "pH at 10cm: raster modify (0.008)")


    bdd_echan = test_resolution
    bdd_echan <- bdd_echan %>%filter(!is.na(ph_10_origine) & !is.na(ph_10_rech))

    # graphique avec ggplot
        # coefficient de corrélation
    correlation <- cor(as.numeric(bdd_echan$ph_10_origine), bdd_echan$ph_10_rech)
    p <- ggplot(bdd_echan, aes(x = ph_10_origine, y = ph_10_rech)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = FALSE, color = "red") + 
      labs(subtitle = paste("r = ", round(correlation, 2)),
           x = "Original pH", y = "Resampled pH") + 
      theme_classic() 

    p

    ```



-->

## Soil organic carbone (g/kg)

```{r extract C,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = "c_orga_0",df = bdd,conv = 5, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_organic.carbon_usda.6a1c_m_250m_b0..0cm_1950..2017_v0.2.tif")
bdd <- extraction(nom_col = "c_orga_10",df = bdd,conv = 5, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_organic.carbon_usda.6a1c_m_250m_b10..10cm_1950..2017_v0.2.tif")
bdd <- extraction(nom_col = "c_orga_30",df = bdd,conv = 5, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_organic.carbon_usda.6a1c_m_250m_b30..30cm_1950..2017_v0.2.tif")
bdd = moyenne_val_extrct(nom_col = "c_orga_0_a_30", vec_col = c("c_orga_0","c_orga_10","c_orga_30"),df=bdd)


# summary(bdd$c_orga_0_a_30)
# explo_num(nom_col = "c_orga_0_a_30", titre = "C. organic")

id_ligne <- bdd[which(bdd$c_orga_0_a_30 >=2.4), "ID"] 
bdd <- bdd[!bdd$ID %in% id_ligne, ]
bdd=droplevels(bdd)



summary(bdd$c_orga_0_a_30)
explo_num(nom_col = "c_orga_0_a_30", titre = "C. organic")

# verifier_valeurs_aberrantes(data=bdd, variable="c_orga_0_a_30", opposite = FALSE)

```

## pH

**Extracted values**

```{r extract pH,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = "ph_0",df = bdd,conv = 10, 
                  tif_file_path ="C:/Users/diall/Downloads/datas/raster_modif/sol_ph.h2o_usda.4c1a2a_m_250m_b10..10cm_1950..2017_v0.2.tif")

bdd <- extraction(nom_col = "ph_10" ,df = bdd,conv = 10, 
                  tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_ph.h2o_usda.4c1a2a_m_250m_b10..10cm_1950..2017_v0.2.tif")
bdd <- extraction(nom_col = "ph_30" ,df = bdd,conv = 10, 
                  tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_ph.h2o_usda.4c1a2a_m_250m_b30..30cm_1950..2017_v0.2.tif")
bdd = moyenne_val_extrct(nom_col = "ph_0_a_30", vec_col = c("ph_0","ph_10","ph_30"),df=bdd)
# summary(bdd$ph_0_a_30)

explo_num(nom_col = "ph_0_a_30", titre = "pH (0 - 30 cm)")
```

**Measured values & extracted values**

-   Clean pH column

```{r clean pH,echo=TRUE}
# On recupere les deux colonnes du pH
df_comp=bdd[, c("ID", "ID_Site","ph_eau","ph_0_a_30" )]
df_comp =df_comp[complete.cases(df_comp$ph_eau),] 
df_comp =df_comp[complete.cases(df_comp$ph_0_a_30),] 
df_comp <- df_comp[!grepl("[^0-9.]", df_comp$ph_eau), ]
df_comp$ph_eau <- as.numeric(df_comp$ph_eau)
df_comp$ph_0_a_30 <- as.numeric(df_comp$ph_0_a_30)


df_comp = df_comp[!df_comp$ph_eau== 44140.00,]
df_comp = df_comp[!df_comp$ph_eau== "NA",]
df_comp = df_comp[!df_comp$ph_0_a_30== "NA",]
df_comp = droplevels(df_comp)
```

```{r,echo=TRUE}
ID_Site_dupliques <- df_comp$ID_Site[duplicated(df_comp$ID_Site)]
#length(ID_Site_dupliques)

lignes_dupliquees <- subset(df_comp, duplicated(ID_Site) & duplicated(ph_eau))

lignes_unique <- unique(lignes_dupliquees$ID_Site )
#length(lignes_unique)

# nrow(df_comp) - length(ID_Site_dupliques) + length(lignes_unique)


dupliquees <- duplicated(df_comp$ID_Site)
df_comp <- df_comp[!dupliquees, ]
df_comp=droplevels(df_comp)

# correlation <- cor.test(df_comp$ph_eau, df_comp$ph_0_a_30,method = "pearson")
#resultat_test <- t.test(df_comp$ph_eau, df_comp$ph_0_a_30)

df_comp$ph_eau <- as.numeric(df_comp$ph_eau)
df_comp$ph_0_a_30 <- as.numeric(df_comp$ph_0_a_30)

```

<!-- <br/>  -->

::: columns
::: {.column width="60%"}
-   Method ?

-   Depth ?

-   Measured values (CR = `r df_col[df_col$Variable=="ph_eau","CR"]`)

```{r}
  summary(df_comp$ph_eau)
```

-   Extracted values

```{r}
  summary(df_comp$ph_0_a_30)
```
:::

::: {.column width="40%"}
```{r fig cor pH,fig.align='center',fig.height=5,fig.width=4}
    correlation <- cor(as.numeric(df_comp$ph_eau), df_comp$ph_0_a_30)
# graphique avec ggplot
    p <- ggplot(df_comp, aes(x = ph_eau, y = ph_0_a_30)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = FALSE, color = "red") + 
      labs(subtitle =paste("r = ", round(correlation, 2)),x = "pH measured values", y = "pH extracted values") + 
      theme_classic() 
p
# plot(as.numeric(df_comp$ph_eau))
```
:::
:::

<!--
## Bulk density (kg / m-cube)

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = "d_ap_0",df = bdd,conv = 10, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_bulkdens.fineearth_usda.4a1h_m_250m_b0..0cm_1950..2017_v0.2.tif")
bdd <- extraction(nom_col = "d_ap_10",df = bdd,conv = 10, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_bulkdens.fineearth_usda.4a1h_m_250m_b10..10cm_1950..2017_v0.2.tif")
bdd <- extraction(nom_col = "d_ap_30",df = bdd,conv = 10, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_bulkdens.fineearth_usda.4a1h_m_250m_b30..30cm_1950..2017_v0.2.tif")
bdd = moyenne_val_extrct(nom_col = "d_ap_0_a_30", vec_col = c("d_ap_0","d_ap_10","d_ap_30"),bdd)
summary(bdd$d_ap_0_a_30)
explo_num(nom_col = "d_ap_0_a_30", titre = "Bulk density (0 - 30 cm)")
```


## Sand content (% kg/kg)

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = "sable_0",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_sand.wfraction_usda.3a1a1a_m_250m_b0..0cm_1950..2017_v0.2.tif")
bdd <- extraction(nom_col = "sable_10",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_sand.wfraction_usda.3a1a1a_m_250m_b10..10cm_1950..2017_v0.2.tif")
bdd <- extraction(nom_col = "sable_30",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_sand.wfraction_usda.3a1a1a_m_250m_b30..30cm_1950..2017_v0.2.tif")
bdd = moyenne_val_extrct(nom_col = "sable_0_a_30", vec_col = c("sable_0","sable_10","sable_30"),df=bdd)
summary(bdd$sable_0_a_30)
explo_num(nom_col = "sable_0_a_30", titre = "Sand (0 - 30 cm)")
```



-->

## Sand

**Extracted values (g/kg, 0 - 30 cm)**

```{r extract sand,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = "sable.0_5",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sable.0_5.tif")

bdd <- extraction(nom_col = "sable.5_15",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sable.5_15.tif")

bdd <- extraction(nom_col = "sable.15_30",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sable.15_30.tif")

bdd = moyenne_val_extrct(nom_col = "sable.0_30", vec_col = c("sable.0_5","sable.5_15","sable.15_30"),df=bdd)

# summary(bdd$sable.0_30)

explo_num(nom_col = "sable.0_30", titre = "Sand extracted values")
```

**Measured values & extracted values**

-   Clean sand column

```{r,echo=TRUE}
# On recupere les deux colonnes du pH
df_comp=bdd[, c("ID", "ID_Site","sand","sable.0_30" )]
df_comp =df_comp[complete.cases(df_comp$sand),] 
df_comp =df_comp[complete.cases(df_comp$sable.0_30),] 
df_comp <- df_comp[!grepl("[^0-9.]", df_comp$sand), ]
df_comp$sand <- as.numeric(df_comp$sand)
df_comp$sable.0_30 <- as.numeric(df_comp$sable.0_30)
# colSums(is.na(df_comp))

df_comp = df_comp[!df_comp$sand== "NA",]
df_comp = df_comp[!df_comp$sable.0_30== "NaN",]
df_comp = droplevels(df_comp)
```

```{r,echo=TRUE}
# -   Deleting duplicate measured values

ID_Site_dupliques <- df_comp$ID_Site[duplicated(df_comp$ID_Site)]
#length(ID_Site_dupliques)

lignes_dupliquees <- subset(df_comp, duplicated(ID_Site) & duplicated(sand))

lignes_unique <- unique(lignes_dupliquees$ID_Site )
#length(lignes_unique)
# nrow(df_comp) - length(ID_Site_dupliques) + length(lignes_unique)

dupliquees <- duplicated(df_comp$ID_Site)
df_comp <- df_comp[!dupliquees, ]
df_comp=droplevels(df_comp)
df_comp$sand <- as.numeric(df_comp$sand)
df_comp$sable.0_30 <- as.numeric(df_comp$sable.0_30)

# summary(df_comp$sand)
# explo_num(nom_col = "sand", titre = "Sand extracted values",df = df_comp)
id_ligne <- df_comp[which(df_comp$sand >=83), "ID"] 
df_comp <- df_comp[!df_comp$ID %in% id_ligne, ]
df_comp=droplevels(df_comp)


# 
# summary(df_comp$sable.0_30)
# explo_num(nom_col = "sable.0_30", titre = "Sand extracted values",df = df_comp)
id_ligne <- df_comp[which(df_comp$sable.0_30 >=60), "ID"] 
df_comp <- df_comp[!df_comp$ID %in% id_ligne, ]
df_comp=droplevels(df_comp)

```

::: columns
::: {.column width="60%"}
-   Method ?

-   Depth ?

-   Measured values (CR = `r df_col[df_col$Variable=="sand","CR"]`)

```{r}
  summary(df_comp$sand)
```

-   Extracted values

```{r}
  summary(df_comp$sable.0_30)
```
:::

::: {.column width="40%"}
\n\n\n

```{r,fig.align='center',fig.height=5,fig.width=4}
# graphique avec ggplot
correlation <- cor(as.numeric(df_comp$sand), df_comp$sable.0_30)
    p <- ggplot(df_comp, aes(x = sand, y = sable.0_30)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = FALSE, color = "red") + 
      labs(subtitle =paste("r = ", round(correlation, 2)) ,x = "Sand measured values", y = "Sand extracted values") + 
      theme_classic() 
p
# plot(as.numeric(df_comp$sand))
# plot(df_comp$sable.0_30)
```

:::
:::

## Silt

**Extracted values (g/kg, 0 - 30 cm)**

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = "limon.0_5",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/limon.0_5.tif")

bdd <- extraction(nom_col = "limon.5_15",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/limon.5_15.tif")

bdd <- extraction(nom_col = "limon.15_30",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/limon.15_30.tif")

bdd = moyenne_val_extrct(nom_col = "limon.0_30", vec_col = c("limon.0_5","limon.5_15","limon.15_30"),df=bdd)



# summary(bdd$limon.0_30)
# explo_num(nom_col = "limon.0_30", titre = "limon.0_30",df = bdd)
id_ligne <- bdd[which(bdd$limon.0_30 >=80), "ID"] 
bdd <- bdd[!bdd$ID %in% id_ligne, ]
bdd=droplevels(bdd)


# summary(bdd$limon.0_30)
explo_num(nom_col = "limon.0_30", titre = "Silt extracted values")
```

**Measured values & extracted values**

-   Clean silt column

```{r,echo=TRUE}
# On recupere les deux colonnes du pH
df_comp=bdd[, c("ID", "ID_Site","silt","limon.0_30" )]
df_comp =df_comp[complete.cases(df_comp$silt),] 
df_comp =df_comp[complete.cases(df_comp$limon.0_30),] 
df_comp <- df_comp[!grepl("[^0-9.]", df_comp$silt), ]
df_comp$silt <- as.numeric(df_comp$silt)
df_comp$limon.0_30 <- as.numeric(df_comp$limon.0_30)
# colSums(is.na(df_comp))


df_comp = df_comp[!df_comp$silt== "NA",]
df_comp = df_comp[!df_comp$limon.0_30== "NaN",]
df_comp = droplevels(df_comp)
```

```{r,echo=TRUE}
# -   Deleting duplicate measured values

ID_Site_dupliques <- df_comp$ID_Site[duplicated(df_comp$ID_Site)]
#length(ID_Site_dupliques)

lignes_dupliquees <- subset(df_comp, duplicated(ID_Site) & duplicated(silt))

lignes_unique <- unique(lignes_dupliquees$ID_Site )
#length(lignes_unique)
# nrow(df_comp) - length(ID_Site_dupliques) + length(lignes_unique)


dupliquees <- duplicated(df_comp$ID_Site)
df_comp <- df_comp[!dupliquees, ]
df_comp=droplevels(df_comp)
df_comp$silt <- as.numeric(df_comp$silt)
df_comp$limon.0_30 <- as.numeric(df_comp$limon.0_30)




# summary(df_comp$silt)
# explo_num(nom_col = "silt", titre = "Silt",df = df_comp)
id_ligne <- df_comp[which(df_comp$silt <=7.3), "ID"] 
df_comp <- df_comp[!df_comp$ID %in% id_ligne, ]
df_comp=droplevels(df_comp)



# summary(df_comp$limon.0_30)
# explo_num(nom_col = "limon.0_30", titre = "limon.0_30",df = df_comp)
id_ligne <- df_comp[which(df_comp$limon.0_30 >=80), "ID"] 
df_comp <- df_comp[!df_comp$ID %in% id_ligne, ]
df_comp=droplevels(df_comp)


```

::: columns
::: {.column width="60%"}
-   Method ?

-   Depth ?

-   Measured values (CR = `r df_col[df_col$Variable=="silt","CR"]`)

```{r}
  summary(df_comp$silt)
```

-   Extracted values

```{r}
  summary(df_comp$limon.0_30)
```
:::

::: {.column width="40%"}
\n\n\n

```{r,fig.align='center',fig.height=5,fig.width=4}
# graphique avec ggplot
correlation <- cor(as.numeric(df_comp$silt), df_comp$limon.0_30)
    p <- ggplot(df_comp, aes(x = silt, y = limon.0_30)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = FALSE, color = "red") + 
      labs(subtitle =paste("r = ", round(correlation, 2)) ,x = "Silt measured values", y = "Silt extracted values") + 
      theme_classic() 
p

plot(as.numeric(df_comp$limon.0_30))
```

```{r,fig.align='center',fig.height=5,fig.width=4}
# graphique avec ggplot
correlation <- cor(as.numeric(df_comp$silt), df_comp$limon.0_30)
    p <- ggplot(df_comp, aes(x = silt, y = limon.0_30)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = FALSE, color = "red") + 
      labs(subtitle =paste("r = ", round(correlation, 2)) ,x = "Silt measured values", y = "Silt extracted values") + 
      theme_classic() 
p

plot(as.numeric(df_comp$limon.0_30))
```
:::
:::

## Clay

**Extracted values (g/kg, 0 - 30 cm)**

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = "argile.0_5",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/argile.0_5.tif")

bdd <- extraction(nom_col = "argile.5_15",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/argile.5_15.tif")

bdd <- extraction(nom_col = "argile.15_30",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/argile.15_30.tif")

bdd = moyenne_val_extrct(nom_col = "argile.0_30", vec_col = c("argile.0_5","argile.5_15","argile.15_30"),df=bdd)



# summary(bdd$argile.0_30)
# explo_num(nom_col = "argile.0_30", titre = "argile.0_30",df = bdd)
id_ligne <- bdd[which(bdd$argile.0_30 >=52), "ID"] 
bdd <- bdd[!bdd$ID %in% id_ligne, ]
bdd=droplevels(bdd)

summary(bdd$argile.0_30)
explo_num(nom_col = "argile.0_30", titre = "Clay extracted values")
```

**Measured values & extracted values** - Clean clay column

```{r,echo=TRUE}
# On recupere les deux colonnes du pH
df_comp=bdd[, c("ID", "ID_Site","clay","argile.0_30" )]
df_comp =df_comp[complete.cases(df_comp$clay),] 
df_comp =df_comp[complete.cases(df_comp$argile.0_30),] 
df_comp <- df_comp[!grepl("[^0-9.]", df_comp$clay), ]
df_comp$clay <- as.numeric(df_comp$clay)
df_comp$argile.0_30 <- as.numeric(df_comp$argile.0_30)
# colSums(is.na(df_comp))

df_comp = df_comp[!df_comp$clay== "NA",]
df_comp = df_comp[!df_comp$argile.0_30== "NaN",]
df_comp = droplevels(df_comp)
```

```{r,echo=TRUE}
# -   Deleting duplicate measured values

ID_Site_dupliques <- df_comp$ID_Site[duplicated(df_comp$ID_Site)]
#length(ID_Site_dupliques)

lignes_dupliquees <- subset(df_comp, duplicated(ID_Site) & duplicated(clay))

lignes_unique <- unique(lignes_dupliquees$ID_Site )
#length(lignes_unique)
# nrow(df_comp) - length(ID_Site_dupliques) + length(lignes_unique)

dupliquees <- duplicated(df_comp$ID_Site)
df_comp <- df_comp[!dupliquees, ]
df_comp=droplevels(df_comp)
df_comp$clay <- as.numeric(df_comp$clay)
df_comp$argile.0_30 <- as.numeric(df_comp$argile.0_30)


df_comp$clay = as.numeric(df_comp$clay)/10 # pour conv en %

```

::: columns
::: {.column width="60%"}
-   Method ?

-   Depth ?

-   Measured values (CR = `r df_col[df_col$Variable=="clay","CR"]`)

```{r}
  summary(df_comp$clay)
```

-   Extracted values

```{r}
  summary(df_comp$argile.0_30)
```
:::

::: {.column width="40%"}
\n\n\n

```{r,fig.align='center',fig.height=5,fig.width=4}
# graphique avec ggplot
correlation <- cor(as.numeric(df_comp$clay), df_comp$argile.0_30)
    p <- ggplot(df_comp, aes(x = clay, y = argile.0_30)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = FALSE, color = "red") + 
      labs(subtitle =paste("r = ", round(correlation, 2)) ,x = "Clay measured values", y = "Clay extracted values") + 
      theme_classic() 
p
```
:::
:::

## Elevation

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = "elevation",df = bdd,conv = 1, 
                  tif_file_path ="C:/Users/diall/Downloads/datas/raster_modif/GMTED2010_Spatial.tif")


# summary(bdd$elevation)
# explo_num(nom_col = "elevation", titre = "elevation",df = bdd)
id_ligne <- bdd[which(bdd$elevation >=800), "ID"] 
bdd <- bdd[!bdd$ID %in% id_ligne, ]
bdd=droplevels(bdd)


summary(bdd$elevation)
explo_num(nom_col = "elevation", titre = "elevation")
```


<!--
## pH_H2O_CaCl 

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = 'jrc_pH_H2O_CaCl', df = bdd, conv = 1, tif_file_path = 'C:/Users/diall/Downloads/datas/raster_modif/pH_H2O_CaCl.tif')

summary(bdd$jrc_pH_H2O_CaCl)

explo_num(nom_col = 'jrc_pH_H2O_CaCl', titre = 'jrc_pH_H2O_CaCl')
```

## pH_H2O 

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = 'jrc_pH_H2O', df = bdd, conv = 1, tif_file_path = 'C:/Users/diall/Downloads/datas/raster_modif/pH_H2O.tif')

summary(bdd$jrc_pH_H2O)

explo_num(nom_col = 'jrc_pH_H2O', titre = 'jrc_pH_H2O')

df_comp=bdd[, c("ID", "ID_Site","ph_eau","jrc_pH_H2O" )]
df_comp =df_comp[complete.cases(df_comp$ph_eau),] 
df_comp =df_comp[complete.cases(df_comp$jrc_pH_H2O),] 
df_comp <- df_comp[!grepl("[^0-9.]", df_comp$ph_eau), ]
df_comp$ph_eau <- as.numeric(df_comp$ph_eau)
df_comp$jrc_pH_H2O <- as.numeric(df_comp$jrc_pH_H2O)


df_comp = df_comp[!df_comp$ph_eau== 44140.00,]
df_comp = df_comp[!df_comp$ph_eau== "NA",]
df_comp = df_comp[!df_comp$jrc_pH_H2O== "NA",]
df_comp = droplevels(df_comp)

correlation <- cor(as.numeric(df_comp$ph_eau), df_comp$jrc_pH_H2O)
```

## pH_CaCl 

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = 'jrc_pH_CaCl', df = bdd, conv = 1, tif_file_path = 'C:/Users/diall/Downloads/datas/raster_modif/pH_CaCl.tif')

summary(bdd$jrc_pH_CaCl)

explo_num(nom_col = 'jrc_pH_CaCl', titre = 'jrc_pH_CaCl')
```

-->


## Phosphore (P, mg/kg)

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = 'P', df = bdd, conv = 1, tif_file_path = 'C:/Users/diall/Downloads/datas/raster_modif/P.tif')

summary(bdd$P)

explo_num(nom_col = 'P', titre = 'P')
```

## Azote (N, g/kg)

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = 'N', df = bdd, conv = 1, tif_file_path = 'C:/Users/diall/Downloads/datas/raster_modif/N.tif')


# summary(bdd$N)
# explo_num(nom_col = "N", titre = "N",df = bdd)
id_ligne <- bdd[which(bdd$N >=4), "ID"] 
bdd <- bdd[!bdd$ID %in% id_ligne, ]
bdd=droplevels(bdd)


summary(bdd$N)
explo_num(nom_col = 'N', titre = 'N')
```

## Potassium (K, mg/kg)

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = 'K', df = bdd, conv = 1, tif_file_path = 'C:/Users/diall/Downloads/datas/raster_modif/K.tif')


# summary(bdd$K)
# explo_num(nom_col = "K", titre = "K",df = bdd)
id_ligne <- bdd[which(bdd$K >=500), "ID"] 
bdd <- bdd[!bdd$ID %in% id_ligne, ]
bdd=droplevels(bdd)


summary(bdd$K)
explo_num(nom_col = 'K', titre = 'K')
```

## C/N

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = 'CN', df = bdd, conv = 1, tif_file_path = 'C:/Users/diall/Downloads/datas/raster_modif/CN.tif')


# summary(bdd$N)
# explo_num(nom_col = "CN", titre = "CN",df = bdd)
id_ligne <- bdd[which(bdd$CN >=18), "ID"] 
bdd <- bdd[!bdd$ID %in% id_ligne, ]
bdd=droplevels(bdd)


summary(bdd$CN)
explo_num(nom_col = 'CN', titre = 'CN')
```

## Capacité d'échange de cations (CEC, cmol/kg)

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = 'CEC', df = bdd, conv = 1, tif_file_path = 'C:/Users/diall/Downloads/datas/raster_modif/CEC.tif')

summary(bdd$CEC)

explo_num(nom_col = 'CEC', titre = 'CEC')
```

## Carbonates de calcium (CaCO3, g/kg)

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = 'CaCO3', df = bdd, conv = 1, tif_file_path = 'C:/Users/diall/Downloads/datas/raster_modif/CaCO3.tif')


# summary(bdd$N)
# explo_num(nom_col = "CaCO3", titre = "CaCO3",df = bdd)
id_ligne <- bdd[which(bdd$CaCO3 >=350), "ID"] 
bdd <- bdd[!bdd$ID %in% id_ligne, ]
bdd=droplevels(bdd)


summary(bdd$CaCO3)
explo_num(nom_col = 'CaCO3', titre = 'CaCO3')
```

# Analyses explorations

**Réduction du jeu de donnée**

```{r ana explo,echo=TRUE,fig.height=8,fig.show='animate',fig.align='center'}
id_col=c("ID","Programme","Annee","ID_Site","Protocole")

vdt_col=c("AB_tot", "BM_tot", "Richesse_tot")

land_cover_col=c("clcm_lvl3")

topo_col=c("elevation","gps_x","gps_y")


soil_col=c("ph_0_a_30","sable.0_30","limon.0_30","argile.0_30","c_orga_0_a_30","P","N","K","CN","CEC","CaCO3")


climate_col=c()
for (i in 1:19){
  climate_col=c(climate_col, paste0("bio",i) )
}
climate_col=c(climate_col,"cmi_mean","gdd0","gdd10","hurs_mean","pet_penman_mean")

bdd_explo= bdd[,c(id_col,vdt_col,land_cover_col,topo_col,soil_col,climate_col)]
# str(bdd_explo)
bdd_explo$ID = as.factor(bdd_explo$ID)


# Renome
new_soil_col=c("pH","sand","silt","clay","C","P","N","K","CN","CEC","CaCO3")
bdd_explo <- rename(bdd_explo, !!setNames(soil_col, new_soil_col))

bdd_explo <- bdd_explo %>% 
  rename(PET = pet_penman_mean)
climate_col=c()
for (i in 1:19){
  climate_col=c(climate_col, paste0("bio",i) )
}
climate_col=c(climate_col,"cmi_mean","gdd0","gdd10","hurs_mean","PET")


col_graph=c(vdt_col,land_cover_col,topo_col,new_soil_col,climate_col)
# for (i in names(bdd_explo[,col_graph])){
#   par(mfrow=c(2,2))
#   plot(bdd_explo[[i]], main=i)
# }


# STANDARIZATION
# names(bdd_explo)
var_quanti = c(topo_col,new_soil_col,climate_col)
for (col in var_quanti) {
  bdd_explo[[col]] <- scale(bdd_explo[[col]])
}
# summary(bdd_explo[, var_quanti])
# str(bdd_explo[, var_quanti])


levels(bdd_explo$clcm_lvl3)[levels(bdd_explo$clcm_lvl3) == "Broad-leaved forest"] <- "Mixed forest"
levels(bdd_explo$clcm_lvl3)[levels(bdd_explo$clcm_lvl3) == "Coniferous forest"] <- "Mixed forest"
bdd_explo$clcm_lvl3= as.factor(bdd_explo$clcm_lvl3)

cl_original <- levels(bdd_explo$clcm_lvl3)
new_cl <- c("mf","gua", "ng", "nial", "p", "v")
bdd_explo$clcm_lvl3 <- factor(bdd_explo$clcm_lvl3, levels = cl_original, labels = new_cl)


data_mod = bdd_explo


```

<!--
## Test de correlation : intra catégories

-   **Topographie**

Colonnes supprimée : *gps_x*

```{r,echo=FALSE,fig.align='center'}
# ggpairs(bdd_explo[,topo_col])
correlation_matrix <- cor(bdd_explo[,topo_col],use = "na.or.complete")
# 
corrplot::corrplot(correlation_matrix, method = "color", type = "lower", order = "hclust", addCoef.col = "black",diag = FALSE,cl.cex = 1,number.cex = 1)
topo_sup=c("gps_x")
bdd_explo <- bdd_explo[, setdiff(names(bdd_explo), topo_sup)]

```


<br/>
-   **Soil data**

```{r,echo=FALSE}
# ggpairs(bdd_explo[,new_soil_col])
soil_sup=c("sand")
```

::: columns
::: {.column width="25%"}
<br/>
Colonnes supprimée : *`r soil_sup`*
:::

::: {.column width="75%"}

```{r,echo=FALSE,fig.height=6,fig.width=9,fig.align='right'}
correlation_matrix <- cor(bdd_explo[,new_soil_col],use = "na.or.complete")
corrplot::corrplot(correlation_matrix, method = "color", type = "lower", order = "hclust", addCoef.col = "black",diag = FALSE,cl.cex = 1,number.cex = 0.7)

bdd_explo <- bdd_explo[, setdiff(names(bdd_explo), soil_sup)]
```
:::
:::



<br/>
-   **Climat data**

```{r,echo=FALSE,out.height="100%",out.width="100%"}
# correlation_matrix <- cor(bdd_explo[,climate_col],use = "na.or.complete")
# 
# chemin="C:/Users/diall/OneDrive/Bureau/M2_MODE/stage_abdou_m2/R_Stage_M2/graphes/climat_corrplot.png"
# png(chemin, width = 2000, height = 1000,res = 110)
# 
# 
# corrplot::corrplot(correlation_matrix, method = "color", type = "lower", order = "hclust", addCoef.col = "black",diag = FALSE,cl.cex = 1,number.cex = 0.7,mar = c(0,0,0,0))
# 
# dev.off()
# ![](graphes/climat_corrplot.png)


# cor_function_seuil(data = bdd_explo[,climate_col], seuil = 0.7)
climat_sup=c("bio2","bio4","bio5","bio6","bio7", "bio9", "bio10","bio11","bio13","bio16","bio17","bio18","bio19","gdd0","gdd10", "cmi_mean","PET")
```

::: columns
::: {.column width="25%"}
<br/>
Colonnes supprimmées :*`r climat_sup`*
:::

::: {.column width="75%"}
```{r,echo=FALSE,fig.height=7,fig.width=9,fig.align='right'}
climat_selec = climate_col[!climate_col %in% climat_sup]
# ggpairs(bdd_explo[,climat_selec])
correlation_matrix <- cor(bdd_explo[,climat_selec],use = "na.or.complete")
corrplot::corrplot(correlation_matrix, method = "color", type = "lower", order = "hclust", addCoef.col = "black",diag = FALSE,cl.cex = 1,number.cex = 0.9,mar = c(0,0,0,0))

bdd_explo <- bdd_explo[, setdiff(names(bdd_explo), climat_sup)]
```
:::
:::



## Test de correlation : inter catégories

Colonnes supprimée : *gps_y*
```{r,echo=FALSE}
# correlation_matrix <- cor(bdd_explo[,10:31],use = "na.or.complete")
# corrplot::corrplot(correlation_matrix, method = "color", type = "lower", order = "hclust", addCoef.col = "black",diag = FALSE,cl.cex = 0.5,number.cex = 0.5,mar = c(0,0,0,0))

# var=c("elevation","pH","silt","clay","C","P","N","K","CN","CEC","CaCO3","bio1","bio3","bio8","bio12","bio14","bio15","hurs_mean","gps_y")
# 
# correlation_matrix <- cor(bdd_explo[,var],use = "na.or.complete")
# 
# chemin="C:/Users/diall/OneDrive/Bureau/M2_MODE/stage_abdou_m2/R_Stage_M2/graphes/interC_corrplot.png"
# png(chemin, width = 1200, height = 1000,res = 110)
# 
# 
# corrplot::corrplot(correlation_matrix, method = "color", type = "lower", order = "hclust", addCoef.col = "black",diag = FALSE,cl.cex = 1,number.cex = 0.7,mar = c(0,0,0,0))
# 
# dev.off()


bdd_explo <- bdd_explo[, setdiff(names(bdd_explo), c("gps_y"))]
```

![](graphes/interC_corrplot.png) {width="1200",aligne="center"}



## VIF
```{r VIF var}
var_avant=c("elevation","pH","silt","clay","C","P","N","K","CN","CEC","CaCO3","bio1","bio3",
      "bio8","bio12","bio14","bio15","hurs_mean")
#usdm::vif(bdd_explo[,var_avant])
#usdm::vifcor(bdd_explo[,var_avant], th = 0.9, keep = NULL, method = 'pearson')
usdm::vifstep(bdd_explo[,var_avant], th = 10, keep = NULL, method = 'pearson')
# -   On enleve "bio14" car la plus forte VIF
bdd_explo <- bdd_explo[, setdiff(names(bdd_explo), c("bio14"))]

var=c("elevation","pH","silt","clay","C","P","N","K","CN","CEC","CaCO3","bio1","bio3","bio8","bio12","bio15","hurs_mean")
```

**Suppression de la variable bio14**

## Création des variables factices

```{r factices var ,echo=TRUE}
bdd_explo$clcm_lvl3= as.factor(bdd_explo$clcm_lvl3)
summary_df <- as.data.frame(summary(bdd_explo$clcm_lvl3))
colnames(summary_df) <- c("Numbers")
kable(summary_df,padding = 5)
```

-   Merging:
    *Mixed forest = Broad\_leaved forest +* 
                    *Coniferous forest +* 
                    *Mixed forest*

```{r clc3 levels, echo=FALSE}
levels(bdd_explo$clcm_lvl3)[levels(bdd_explo$clcm_lvl3) == "Broad-leaved forest"] <- "Mixed forest"
levels(bdd_explo$clcm_lvl3)[levels(bdd_explo$clcm_lvl3) == "Coniferous forest"] <- "Mixed forest"
bdd_explo$clcm_lvl3= as.factor(bdd_explo$clcm_lvl3)
# summary_df <- as.data.frame(summary(bdd_explo$clcm_lvl3))
# colnames(summary_df) <- c("Numbers")
# kable(summary_df,padding = 5)
```

-   Abréviation des levels

```{r clc3 names, echo=FALSE}
cl_original <- levels(bdd_explo$clcm_lvl3)
new_cl <- c("mf","gua", "ng", "nial", "p", "v")
bdd_explo$clcm_lvl3 <- factor(bdd_explo$clcm_lvl3, levels = cl_original, labels = new_cl)

bdd_explo <- bdd_explo %>% 
  rename(clc3 = clcm_lvl3)
```


-   Variables

```{r head clc var ,echo=FALSE}
bdd_explo <- dummy_cols(bdd_explo, select_columns = c("clc3"))
clc3_col= c("clc3_mf","clc3_gua", "clc3_ng", "clc3_nial", "clc3_p", "clc3_v")
head(bdd_explo[,clc3_col])
# str(bdd_explo[,clc3_col])
```


```{r VIF and clc3}
# usdm::vif(bdd_explo[,c(var,clc3_col)])
# usdm::vifcor(bdd_explo[,c(var,clc3_col)], th = 0.9, keep = NULL, method = 'pearson')
# usdm::vifstep(bdd_explo[,c(var,clc3_col)], th = 10, keep = NULL, method = 'pearson')
# summary(bdd_explo$clc3)
```



## ACP
```{r ACP, echo=TRUE, fig.align='center'}
# On supprime les lignes avec des NA dans toutes les colonnes sauf BM_tot
# colSums(is.na(bdd_explo))
bdd_explo <- bdd_explo[apply(bdd_explo[, !colnames(bdd_explo) %in% "BM_tot"], 1, function(x) all(!is.na(x))), ]
# colSums(is.na(bdd_explo))


var=c("elevation","pH","silt","clay","C","P","N","K","CN","CEC","CaCO3","bio1","bio3",
      "bio8","bio12","bio15","hurs_mean")

data_acp = bdd_explo[, var]
acp0 <- PCA(data_acp, graph = FALSE)


## Choix du nombre d'axes
# acp0$eig
fviz_eig(acp0, addlabels = TRUE) # on prend les trois premiers axes
contrib_axes <- acp0$var$contrib[, 1:3]  # 3 premiers axes
contrib_axes <- round(contrib_axes, 3)   # Plus facile a lire
fviz_contrib(acp0, choice = "var", axes = 1)
fviz_contrib(acp0, choice = "var", axes = 2)
fviz_contrib(acp0, choice = "var", axes = 3)

seuil <- 1 / ncol(data_acp) * 100
lignes_superieures <- rownames(contrib_axes)[apply(contrib_axes, 1,
                                                   function(x)
                                                     any(x >= seuil))]
var [! var %in% lignes_superieures]

#  **** En consderant les Trois premiers axes:
# seul K n'est pas trés influant





# coloree les variables selon leurs contributions aux axes
fviz_pca_var(
  axes = c(1, 2),
  acp0,
  col.var = "contrib",
  gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
  repel = TRUE
) # evite le chevauchement de texte




# coul = c("yellow", "green", "violet", "blue", "black", "red")
# fviz_pca_ind(
#   axes = c(1, 2),
#   acp0,
#   geom.ind = "text",
#   pointshape = 21,
#   pointsize = 2,
#   #palette = coul,
#   palette = "viridis",
#   addEllipses = TRUE,
#   legend.title = "CLC",
#   fill.ind = bdd_explo$clc3
# )


fviz_pca_ind(
  axes = c(1, 2),
  acp0,
  geom.ind = "point",
  pointshape = 21,
  pointsize = 2,
  palette = "viridis",
  addEllipses = TRUE,
  legend.title = "CLC",
  fill.ind = bdd_explo$clc3
)
```


```{r}
# Classification
# class1 = HCPC(acp0)
# plot(class1$call$t$inert.gain, type = "s")
# #Je choisis finalement 4 groupes
# class1 = HCPC(acp0, nb.clust = 4)
# fviz_dend(class1)
# fviz_cluster(class1)
# fviz_cluster(class1, ellipse.type = "norm", ellipse.level = 0.8)    #Groupes
```


# Selecting variables with regsubsets()
```{r select var, echo=TRUE}
# colSums(is.na(bdd_explo))
bdd_explo <- bdd_explo[apply(bdd_explo[, !colnames(bdd_explo) %in% "BM_tot"], 1, function(x) all(!is.na(x))), ]
# colSums(is.na(bdd_explo))
dim(bdd_explo)
```

## Séléction pour AB_tot
```{r abundance selection, echo=TRUE, fig.align='center'}
# names(bdd_explo)
supp = c("ID","Programme","Annee","ID_Site", "Protocole","BM_tot", "Richesse_tot","clc3")
df_AB_tot= bdd_explo[, setdiff(names(bdd_explo), supp)]
df_AB_tot= df_AB_tot[, setdiff(names(df_AB_tot), clc3_col)]
# str(df_AB_tot)
# colSums(is.na(df_AB_tot))
results_AB_tot <- regsubsets(AB_tot ~ ., data = df_AB_tot,method = "exhaustive",nvmax =17 )
# summary(results_AB_tot)
rsq_AB_tot= round (summary(results_AB_tot)$rsq,2)
adjr2_AB_tot= round(summary(results_AB_tot)$adjr2,2)
cp_AB_tot=round (summary(results_AB_tot)$cp,2)
bic_AB_tot=round(summary(results_AB_tot)$bic,2)
```


-   Selection by R² : stable from `r which.max(rsq_AB_tot)` variables
```{r, echo=FALSE, fig.align='center'}
plot(results_AB_tot, scale = "r2", main='R² criteria')
```

-   Selection by R² adj : stable from `r which.max(adjr2_AB_tot)` variables
```{r, echo=FALSE, fig.align='center'}
plot(results_AB_tot, scale = "adjr2",main='R² adj criteria')
```

-   Selection by Cp : stable from `r which.min(cp_AB_tot)` variables
```{r, echo=FALSE, fig.align='center'}
plot(results_AB_tot, scale = "Cp",main='Mallows Cp criteria')
```

-   Selection by BIC : stable from `r which.min(bic_AB_tot)` variables
```{r, echo=FALSE, fig.align='center'}
plot(results_AB_tot, scale = "bic",main="BIC criteria")
```

```{r}
nbr_AB_tot= max(which.max(rsq_AB_tot),which.max(adjr2_AB_tot),which.min(cp_AB_tot),which.min(bic_AB_tot))
best_var_AB_tot=coefficients(results_AB_tot, id = nbr_AB_tot)
best_var_AB_tot=names(best_var_AB_tot)[-1]
```
-   Les `r length(best_var_AB_tot)` meilleurs variables sont: **`r best_var_AB_tot`**



## Séléction pour BM_tot

```{r biomass selection, echo=TRUE, fig.align='center'}
# names(bdd_explo)
supp = c("ID","Programme","Annee","ID_Site", "Protocole","AB_tot", "Richesse_tot","clc3")
df_BM_tot= bdd_explo[, setdiff(names(bdd_explo), supp)]
df_BM_tot= df_BM_tot[, setdiff(names(df_BM_tot), clc3_col)]
df_BM_tot=drop_na(df_BM_tot)
# str(df_BM_tot)
# colSums(is.na(df_BM_tot))
results_BM_tot <- regsubsets(BM_tot ~ ., data = df_BM_tot,method = "exhaustive",nvmax =17 )
# summary(results_BM_tot)
rsq_BM_tot= round (summary(results_BM_tot)$rsq,2)
adjr2_BM_tot= round(summary(results_BM_tot)$adjr2,2)
cp_BM_tot=round (summary(results_BM_tot)$cp,2)
bic_BM_tot=round(summary(results_BM_tot)$bic,2)
```

-   Suppression de `r sum(is.na(bdd_explo$BM_tot))` lignes de NA de BM_tot (nrow = `r nrow(df_BM_tot)`)

-   Selection by R² : stable from `r which.max(rsq_BM_tot)` variables
```{r, echo=FALSE, fig.align='center'}
plot(results_BM_tot, scale = "r2", main='R² criteria')
```

-   Selection by R² adj : stable from `r which.max(adjr2_BM_tot)` variables
```{r, echo=FALSE, fig.align='center'}
plot(results_BM_tot, scale = "adjr2",main='R² adj criteria')
```

-   Selection by Cp : stable from `r which.min(cp_BM_tot)` variables
```{r, echo=FALSE, fig.align='center'}
plot(results_BM_tot, scale = "Cp",main='Mallows Cp criteria')
```

-   Selection by BIC : stable from `r which.min(bic_BM_tot)` variables
```{r, echo=FALSE, fig.align='center'}
plot(results_BM_tot, scale = "bic",main="BIC criteria")
```

```{r}
nbr_BM_tot= max(which.max(rsq_BM_tot),which.max(adjr2_BM_tot),which.min(cp_BM_tot),which.min(bic_BM_tot))
best_var_BM_tot=coefficients(results_BM_tot, id = nbr_BM_tot)
best_var_BM_tot=names(best_var_BM_tot)[-1]
```
-   Les `r length(best_var_BM_tot)` meilleurs variables sont: **`r best_var_BM_tot`**



## Séléction pour Richesse_tot
```{r richness selection, echo=TRUE, fig.align='center'}
# names(bdd_explo)
supp = c("ID","Programme","Annee","ID_Site", "Protocole","BM_tot", "AB_tot","clc3")
df_Richesse_tot= bdd_explo[, setdiff(names(bdd_explo), supp)]
df_Richesse_tot= df_Richesse_tot[, setdiff(names(df_Richesse_tot), clc3_col)]
# str(df_Richesse_tot)
# colSums(is.na(df_Richesse_tot))
results_Richesse_tot <- regsubsets(Richesse_tot ~ ., data = df_Richesse_tot,method = "exhaustive",nvmax =17 )
# summary(results_Richesse_tot)
rsq_Richesse_tot= round (summary(results_Richesse_tot)$rsq,2)
adjr2_Richesse_tot= round(summary(results_Richesse_tot)$adjr2,2)
cp_Richesse_tot=round (summary(results_Richesse_tot)$cp,2)
bic_Richesse_tot=round(summary(results_Richesse_tot)$bic,2)
```


-   Selection by R² : stable from `r which.max(rsq_Richesse_tot)` variables
```{r, echo=FALSE, fig.align='center'}
plot(results_Richesse_tot, scale = "r2", main='R² criteria')
```

-   Selection by R² adj : stable from `r which.max(adjr2_Richesse_tot)` variables
```{r, echo=FALSE, fig.align='center'}
plot(results_Richesse_tot, scale = "adjr2",main='R² adj criteria')
```

-   Selection by Cp : stable from `r which.min(cp_Richesse_tot)` variables
```{r, echo=FALSE, fig.align='center'}
plot(results_Richesse_tot, scale = "Cp",main='Mallows Cp criteria')
```

-   Selection by BIC : stable from `r which.min(bic_Richesse_tot)` variables
```{r, echo=FALSE, fig.align='center'}
plot(results_Richesse_tot, scale = "bic",main="BIC criteria")
```

```{r}
nbr_Richesse_tot= max(which.max(rsq_Richesse_tot),which.max(adjr2_Richesse_tot),which.min(cp_Richesse_tot),which.min(bic_Richesse_tot))
best_var_Richesse_tot=coefficients(results_Richesse_tot, id = nbr_Richesse_tot)
best_var_Richesse_tot=names(best_var_Richesse_tot)[-1]
```
-   Les `r length(best_var_Richesse_tot)` meilleurs variables sont: **`r best_var_Richesse_tot`**


## Synthèses
```{r synth selection}
all_var=unique(c(best_var_AB_tot,best_var_BM_tot,best_var_Richesse_tot))
col_max=max(c(length(best_var_AB_tot),length(best_var_BM_tot),length(best_var_Richesse_tot)))


# Colonnes communes
colonnes_communes <- intersect(intersect(best_var_AB_tot, best_var_BM_tot), best_var_Richesse_tot)

resultat_tableau <- data.frame(
  Var_AB_tot =c(best_var_AB_tot, rep(" ", col_max - length(best_var_AB_tot))),
  
  Var_BM_tot =  c(best_var_BM_tot, rep(" ", col_max - length(best_var_BM_tot))),
  
  Var_Richesse_tot =  c(best_var_Richesse_tot, rep(" ", col_max -    length(best_var_Richesse_tot))),
  
  Communes =c(colonnes_communes, rep(" ", col_max - length(colonnes_communes)))
)
kable(resultat_tableau,padding=10)


```
**bio1** = mean annual air temperature

**bio3** = isothermality

**bio8** = mean air temperatures of the wettest quarter

**bio8** = mean air temperatures of the wettest quarter

**bio12** = annual precipitation amount

**bio15** = precipitation seasonality

**hurs_mean** = Mean monthly near-surface relative humidity







-->


# Distributions des variables de réponse
## Distributions: AB_tot

```{r abundace dist}
# Noraml
set.seed(123)
par(mfrow = c(2,2))
donnees_normales <- rnorm(nrow(data_mod), mean = mean(data_mod$AB_tot), sd = 2)
hist(donnees_normales, freq = FALSE, main = "Distribution Normale", xlab = "Valeurs", ylab = "Densité")
curve(dnorm(x, mean = mean(donnees_normales), sd = sd(donnees_normales)), add = TRUE, col = "blue", lwd = 2)


# Poisson
set.seed(123)
donnees_poisson <- rpois(nrow(data_mod), lambda = mean(data_mod$AB_tot))
hist(donnees_poisson, freq = FALSE, main = "Distribution de Poisson", xlab = "Valeurs", ylab = "Densité")
x <- 0:max(donnees_poisson)
lines(x, dpois(x, lambda = mean(data_mod$AB_tot)), col = "blue", lwd = 2)


hist(data_mod$AB_tot,xlab = "AB_tot",main = "Distribution des données")


qqnorm(data_mod$AB_tot, pch= 16, col = 'blue',xlab='')
qqline(data_mod$AB_tot, col= 'red')
```


-   Transformation sqrt
```{r}
# Noraml
set.seed(123)
test = sqrt(data_mod$AB_tot)
par(mfrow = c(2,2))
donnees_normales <- rnorm(nrow(data_mod), mean = mean(test), sd = 2)
hist(donnees_normales, freq = FALSE, main = "Distribution Normale", xlab = "Valeurs", ylab = "Densité")
curve(dnorm(x, mean = mean(donnees_normales), sd = sd(donnees_normales)), add = TRUE, col = "blue", lwd = 2)


# Poisson
set.seed(123)
donnees_poisson <- rpois(nrow(data_mod), lambda = mean(test))
hist(donnees_poisson, freq = FALSE, main = "Distribution de Poisson", xlab = "Valeurs", ylab = "Densité")
x <- 0:max(donnees_poisson)
lines(x, dpois(x, lambda = mean(test)), col = "blue", lwd = 2)


hist(test,xlab = "AB_tot",main = "Distribution avec transformation sqrt")


qqnorm(test, pch= 16, col = 'blue',xlab='')
qqline(test, col= 'red')


```


## Distributions: BM_tot
```{r biomass dist}
# Noraml
df_bm=drop_na(data_mod)
set.seed(123)
par(mfrow = c(2,2))
donnees_normales <- rnorm(nrow(df_bm), mean = mean(df_bm$BM_tot), sd = 2)
hist(donnees_normales, freq = FALSE, main = "Distribution Normale", xlab = "Valeurs", ylab = "Densité")
curve(dnorm(x, mean = mean(donnees_normales), sd = sd(donnees_normales)), add = TRUE, col = "blue", lwd = 2)


# Poisson
set.seed(123)
donnees_poisson <- rpois(nrow(df_bm), lambda = mean(df_bm$BM_tot))
hist(donnees_poisson, freq = FALSE, main = "Distribution de Poisson", xlab = "Valeurs", ylab = "Densité")
x <- 0:max(donnees_poisson)
lines(x, dpois(x, lambda = mean(df_bm$BM_tot)), col = "blue", lwd = 2)


hist(df_bm$BM_tot,xlab = "BM_tot",main = "Distribution des données")


qqnorm(df_bm$BM_tot, pch= 16, col = 'blue',xlab='')
qqline(df_bm$BM_tot, col= 'red')
```


-   Transformation sqrt
```{r}
# Noraml
set.seed(123)
test = sqrt(data_mod$BM_tot)
test = drop_na(as.data.frame(test))
par(mfrow = c(2,2))
test =test$test
donnees_normales <- rnorm(length(test), mean = mean(test), sd = sd(test))
hist(donnees_normales, freq = FALSE, main = "Distribution Normale", xlab = "Valeurs", ylab = "Densité")
curve(dnorm(x, mean = mean(donnees_normales), sd = sd(donnees_normales)), add = TRUE, col = "blue", lwd = 2)


# Poisson
set.seed(123)
donnees_poisson <- rpois(length(test), lambda = mean(test))
hist(donnees_poisson, freq = FALSE, main = "Distribution de Poisson", xlab = "Valeurs", ylab = "Densité")
x <- 0:max(donnees_poisson)
lines(x, dpois(x, lambda = mean(test)), col = "blue", lwd = 2)


hist(test,xlab = "BM_tot",main = "Distribution avec transformation sqrt")


qqnorm(test, pch= 16, col = 'blue',xlab='')
qqline(test, col= 'red')


```


## Distributions: Richesse_tot

```{r richness dist}
# Noraml
set.seed(123)
par(mfrow = c(2,2))
donnees_normales <- rnorm(nrow(data_mod), mean = mean(data_mod$Richesse_tot), sd = 2)
hist(donnees_normales, freq = FALSE, main = "Distribution Normale", xlab = "Valeurs", ylab = "Densité")
curve(dnorm(x, mean = mean(donnees_normales), sd = sd(donnees_normales)), add = TRUE, col = "blue", lwd = 2)


# Poisson
set.seed(123)
donnees_poisson <- rpois(nrow(data_mod), lambda = mean(data_mod$Richesse_tot))
hist(donnees_poisson, freq = FALSE, main = "Distribution de Poisson", xlab = "Valeurs", ylab = "Densité")
x <- 0:max(donnees_poisson)
lines(x, dpois(x, lambda = mean(data_mod$Richesse_tot)), col = "blue", lwd = 2)


hist(data_mod$Richesse_tot,xlab = "Richesse_tot",main = "Distribution des données")


qqnorm(data_mod$Richesse_tot, pch= 16, col = 'blue',xlab='')
qqline(data_mod$Richesse_tot, col= 'red')
```


-   Transformation sqrt
```{r}
# Noraml
set.seed(123)
test = sqrt(data_mod$Richesse_tot+1)
test = drop_na(as.data.frame(test))
par(mfrow = c(2,2))
test =test$test
donnees_normales <- rnorm(length(test), mean = mean(test), sd = sd(test))
hist(donnees_normales, freq = FALSE, main = "Distribution Normale", xlab = "Valeurs", ylab = "Densité")
curve(dnorm(x, mean = mean(donnees_normales), sd = sd(donnees_normales)), add = TRUE, col = "blue", lwd = 2)


# Poisson
set.seed(123)
donnees_poisson <- rpois(length(test), lambda = mean(test))
hist(donnees_poisson, freq = FALSE, main = "Distribution de Poisson", xlab = "Valeurs", ylab = "Densité")
x <- 0:max(donnees_poisson)
lines(x, dpois(x, lambda = mean(test)), col = "blue", lwd = 2)


hist(test,xlab = "Richesse_tot",main = "Distribution avec transformation sqrt")


qqnorm(test, pch= 16, col = 'blue',xlab='')
qqline(test, col= 'red')


```
Tranformation non satisfaisante


# Importance des variables
```{r}
# Transformation
data_mod$AB_tot = sqrt(data_mod$AB_tot)
data_mod$BM_tot = sqrt(data_mod$BM_tot)
```

## Importance des variables pour AB_tot
```{r}
ForetAlea <- function(var_rep, df_app, df_valid,mtry =9,ntree= 2000,maxnodes=60){
  
  # Nombre de variables tirées aléatoirement pour la construction des arbre : mtry =    1/nombre de variable explicatives totales # nombre d'arbres : ntree = 250
  
  # var_mod=c(var_rep,predicteurs)
  # df_app = df_app[,var_mod]
  # df_valid = df_valid[,var_mod]
  
  
  col_posi <- which(names(df_app) == var_rep)
  ForeVDT <- randomForest(df_app[-col_posi], df_app[[col_posi]], mtry=mtry, ntree=ntree,maxnodes=maxnodes)
  
  # Prediction sur le jeu de validation
  col_posi <- which(names(df_valid) == var_rep)
  pred.RF<-predict(ForeVDT,newdata=as.data.frame(df_valid[,-col_posi]))
  
  # Calcul du RMSE pour évaluer la qualité du modele
  rmse <- round (sqrt(mean((df_valid[,col_posi] - pred.RF)^2)),2)
  
  r_adj = round (mean(ForeVDT$rsq),2)
  
    results <- list(RMSE = rmse, R_squared= r_adj, model=ForeVDT)
  return(results)
}
```

```{r imp abundance}
# names(data_mod)

df_explo_AB_tot = data_mod[,c(6,9:47)]
df_explo_AB_tot = drop_na(df_explo_AB_tot)


set.seed(1234)  # Pour rendre les résultats reproductibles
index <- createDataPartition(df_explo_AB_tot$clcm_lvl3, p = 0.8, list = FALSE)

# Séparer les données en ensembles d'entraînement et de test
df_train_AB_tot <- df_explo_AB_tot[index, ]  # Données d'entraînement
df_test_AB_tot <- df_explo_AB_tot[-index, ]  # Données de test
df_train_AB_tot = droplevels(df_train_AB_tot)
df_test_AB_tot = droplevels(df_test_AB_tot)


all_var_AB_tot=ForetAlea(var_rep ="AB_tot", df_app=df_train_AB_tot, df_valid = df_test_AB_tot,mtry =9,ntree= 2000)
AB_tot_fit_rf = all_var_AB_tot$model

# # Etape 5 importance des variables
# AB_tot_fit_rf #display fitted model
# which.min(AB_tot_fit_rf$mse) #find number of trees that produce lowest test MSE
# sqrt(AB_tot_fit_rf$mse[which.min(AB_tot_fit_rf$mse)]) #find RMSE of best model
# varImpPlot(AB_tot_fit_rf) #produce variable importance plot

# Obtention de l'importance des variables
importance_rf <- as.data.frame(importance(AB_tot_fit_rf))
importance_rf$nom=rownames(importance_rf)
importance_rf <- importance_rf[order(importance_rf$IncNodePurity,decreasing = TRUE), ]

best_20_AB_tot = c(importance_rf$nom[1:20])
var_a_sup_AB_tot= setdiff(y = best_20_AB_tot,x = importance_rf$nom)

best20_var_AB_tot=ForetAlea(var_rep ="AB_tot", df_app=df_train_AB_tot[,c("AB_tot",best_20_AB_tot)], df_valid = df_test_AB_tot [,c("AB_tot",best_20_AB_tot)],mtry =9,ntree= 2000)

varImpPlot(best20_var_AB_tot$model) #produce variable importance plot

```



## Importance des variables pour BM_tot
```{r imp biomass}
# names(data_mod)
df_explo_BM_tot = data_mod[,c(7,9:47)]
df_explo_BM_tot = drop_na(df_explo_BM_tot)


set.seed(1234)  # Pour rendre les résultats reproductibles
index <- createDataPartition(df_explo_BM_tot$clcm_lvl3, p = 0.8, list = FALSE)

# Séparer les données en ensembles d'entraînement et de test
df_train_BM_tot <- df_explo_BM_tot[index, ]  # Données d'entraînement
df_test_BM_tot <- df_explo_BM_tot[-index, ]  # Données de test
df_train_BM_tot = droplevels(df_train_BM_tot)
df_test_BM_tot = droplevels(df_test_BM_tot)


all_var_BM_tot= ForetAlea(var_rep ="BM_tot", df_app=df_train_BM_tot, df_valid = df_test_BM_tot,mtry =9,ntree= 2000)
BM_tot_fit_rf = all_var_BM_tot$model


# # Etape 5 importance des variables
# BM_tot_fit_rf #display fitted model
# which.min(BM_tot_fit_rf$mse) #find number of trees that produce lowest test MSE
# sqrt(BM_tot_fit_rf$mse[which.min(BM_tot_fit_rf$mse)]) #find RMSE of best model
# sqrt(mean(BM_tot_fit_rf$mse))
# mean(100*BM_tot_fit_rf$rsq)
# varImpPlot(BM_tot_fit_rf) #produce variable importance plot

# Obtention de l'importance des variables
importance_rf <- as.data.frame(importance(BM_tot_fit_rf))
importance_rf$nom=rownames(importance_rf)
importance_rf <- importance_rf[order(importance_rf$IncNodePurity,decreasing = TRUE), ]

best_20_BM_tot = c(importance_rf$nom[1:20])
var_a_sup_BM_tot= setdiff(y = best_20_BM_tot,x = importance_rf$nom)

best20_var_BM_tot= ForetAlea(var_rep ="BM_tot", df_app=df_train_BM_tot[, c("BM_tot",best_20_BM_tot)], df_valid = df_test_BM_tot [, c("BM_tot",best_20_BM_tot)],mtry =9,ntree= 2000)
varImpPlot(best20_var_BM_tot$model) #produce variable importance plot

```


## Importance des variables pour Richesse tot
```{r imp richness}
# names(data_mod)
df_explo_Richesse_tot = data_mod[,c(8,9:47)]
df_explo_Richesse_tot = drop_na(df_explo_Richesse_tot)


set.seed(1234)  # Pour rendre les résultats reproductibles
index <- createDataPartition(df_explo_Richesse_tot$clcm_lvl3, p = 0.8, list = FALSE)

# Séparer les données en ensembles d'entraînement et de test
df_train_Richesse_tot <- df_explo_Richesse_tot[index, ]  # Données d'entraînement
df_test_Richesse_tot <- df_explo_Richesse_tot[-index, ]  # Données de test
df_train_Richesse_tot = droplevels(df_train_Richesse_tot)
df_test_Richesse_tot = droplevels(df_test_Richesse_tot)



all_var_Richesse_tot= ForetAlea(var_rep ="Richesse_tot", df_app=df_train_Richesse_tot, df_valid = df_test_Richesse_tot,mtry =9,ntree= 2000)
Richesse_tot_fit_rf = all_var_Richesse_tot$model


# Etape 5 importance des variables
# Richesse_tot_fit_rf #display fitted model
# which.min(Richesse_tot_fit_rf$mse) #find number of trees that produce lowest test MSE
# sqrt(Richesse_tot_fit_rf$mse[which.min(Richesse_tot_fit_rf$mse)]) #find RMSE of best model
# sqrt(mean(Richesse_tot_fit_rf$mse))
# mean(100*Richesse_tot_fit_rf$rsq)
# varImpPlot(Richesse_tot_fit_rf) #produce variable importance plot

# Obtention de l'importance des variables
importance_rf <- as.data.frame(importance(Richesse_tot_fit_rf))
importance_rf$nom=rownames(importance_rf)
importance_rf <- importance_rf[order(importance_rf$IncNodePurity,decreasing = TRUE), ]

best_20_Richesse_tot = c(importance_rf$nom[1:20])
var_a_sup_Richesse_tot= setdiff(y = best_20_Richesse_tot,x = importance_rf$nom)

best20_var_Richesse_tot= ForetAlea(var_rep ="Richesse_tot", df_app=df_train_Richesse_tot[, c("Richesse_tot",best_20_Richesse_tot)], df_valid = df_test_Richesse_tot [, c("Richesse_tot",best_20_Richesse_tot)],mtry =9,ntree= 2000)


varImpPlot(best20_var_Richesse_tot$model) #produce variable importance plot
```






# Modélisation
```{r modeling, echo=TRUE}
supp = c("ID","Programme","Annee","ID_Site", "Protocole","AB_tot","BM_tot","Richesse_tot", "AB_tot","clc3")

# names(bdd_explo[,10:26])
# df_mod=bdd_explo
# predicteurs=c(names(df_mod)[! names(df_mod) %in% supp])

set.seed(1234)  # Pour rendre les résultats reproductibles
index <- createDataPartition(data_mod$clcm_lvl3, p = 0.8, list = FALSE)

# Séparer les données en ensembles d'entraînement et de test
df_train <- data_mod[index, ]  # Données d'entraînement
df_test <- data_mod[-index, ]  # Données de test
df_train = droplevels(df_train)
df_test = droplevels(df_test)
# summary(df_train$clcm_lvl3)
# summary(df_test$clcm_lvl3)
# summary(data_mod$clcm_lvl3)
# 80*summary(data_mod$clcm_lvl3)/100

n_sim=30 # Nombre de similation



df_mod_AB_tot = data_mod[,c(6,9:47)]
df_mod_AB_tot = drop_na(df_mod_AB_tot)


df_mod_BM_tot = data_mod[,c(7,9:47)]
df_mod_BM_tot = drop_na(df_mod_BM_tot)


df_mod_Richesse_tot = data_mod[,c(8,9:47)]
df_mod_Richesse_tot = drop_na(df_mod_Richesse_tot)

```

-   Data partition (`r dim(data_mod)`):
    -   train data (80 %) = `r dim(df_train)`
    -   test data (20 %) = `r dim(df_test)`
-   Nombre de simulation = `r n_sim`



Création des fonction de chaque modèle 
 

## GLM
```{r function GLM, echo=TRUE}
GLM <- function(var_rep, df_app, df_valid,family = 'gaussian'){
  
  var_predicteurs = c("clcm_lvl3","elevation","pH","silt","clay","C","N","K","CN","CEC","CaCO3","bio1","bio3",
                        "bio8","bio12","bio15","hurs_mean")

  df_app = df_app[,c(var_rep,var_predicteurs)]
  df_valid = df_valid[,c(var_rep,var_predicteurs)]
  
  formula <- substitute(var_rep ~ ., list(var_rep = as.name(var_rep)))
  
  
  # entrainement du modele sur le jeu d'entrainement
  modelglm<-glm(formula,family = family ,data = df_app)
  
  # Prediction sur le jeu de validation
  pred.GLM<-predict(modelglm,newdata=as.data.frame(df_valid[,var_predicteurs]))
  
  # Calcul du RMSE pour évaluer la qualite du modele
  rmse <- round (sqrt(mean((df_valid[,var_rep] - pred.GLM)^2,na.rm=TRUE)),2)
  
  
   # Calcul du R-squared pour évaluer la qualite du modele
  # la somme des carrés résiduels (SSR)
  col_posi <- which(names(df_valid) == var_rep)
  SSR <- sum((pred.GLM - df_valid[,col_posi]) ^ 2,na.rm=TRUE)
  # la somme totale des carrés (SST)
  SST <- sum((df_valid[,col_posi] - mean(df_valid[,col_posi])) ^ 2)
  r_adj <- 1 - (SSR / SST)

  
  MAE <- mean(abs(pred.GLM - df_valid[,col_posi]),na.rm=TRUE)
  
  results <- list(RMSE = rmse, R_squared= r_adj, MAE = MAE, model = modelglm)
  return(results)
}

# GLM(var_rep ="AB_tot" , df_app=df_train, df_valid = df_test,family = 'gaussian')
# GLM(var_rep ="BM_tot" , df_app=drop_na(df_train), df_valid = drop_na(df_test),family = 'gaussian')
# GLM(var_rep ="Richesse_tot" , df_app=df_train, df_valid = df_test,family = 'gaussian')
```

## GAM
```{r function GAM, echo=TRUE}
GAM <- function(var_rep, df_app, df_valid, family = 'gaussian',method = "REML"){
  var_predicteurs = c("clcm_lvl3","elevation","pH","silt","clay","C","N","K","CN","CEC","CaCO3","bio1","bio3","bio8","bio12","bio15","hurs_mean")
  # var_mod=c(var_rep,predicteurs)
  # df_app = df_app[,var_mod]
  # df_valid = df_valid[,var_mod]
  #formula <- substitute(var_rep ~.,   list(var_rep = as.name(var_rep)))
  
  # entrainement du modele sur le jeu d'entrainement
  if(var_rep == "AB_tot"){ 
  modelgam<-gam(AB_tot ~ clcm_lvl3 + s(elevation) + s(pH) + s(silt) + s(clay) + s(C) + s(N) +  s(K) + s(CN) + s(CEC) + s(CaCO3) + s(bio1) + s(bio3) + s(bio8) + s(bio12) + s(bio15) + s(hurs_mean) ,
        family=family,method = method,data = df_app)
  }
    if(var_rep == "BM_tot"){ 
  modelgam<-gam(BM_tot ~ clcm_lvl3 + s(elevation) + s(pH) + s(silt) + s(clay) + s(C) + s(N) +  s(K) + s(CN) + s(CEC) + s(CaCO3) + s(bio1) + s(bio3) + s(bio8) + s(bio12) + s(bio15) + s(hurs_mean) ,
        family=family,method = method,data = df_app)
    }
    if(var_rep == "Richesse_tot"){ 
  modelgam<-gam(Richesse_tot ~ clcm_lvl3 + s(elevation) + s(pH) + s(silt) + s(clay) + s(C) + s(N) +  s(K) + s(CN) +  s(CEC) + s(CaCO3) + s(bio1) + s(bio3) + s(bio8) + s(bio12) + s(bio15) + s(hurs_mean) ,
        family=family,method = method,data = df_app)
    }
  
  
  # Prediction sur le jeu de validation
  pred.GAM<-predict(modelgam,newdata=as.data.frame(df_valid[,var_predicteurs]))
  
  # Calcul du RMSE pour évaluer la qualite du modele
  rmse <- sqrt(mean((df_valid[,var_rep] - pred.GAM)^2,na.rm=TRUE))

  
  # calcule du R-squared
  # la somme des carrés résiduels (SSR)
  col_posi <- which(names(df_valid) == var_rep)
  SSR <- sum((pred.GAM - df_valid[,col_posi]) ^ 2,na.rm=TRUE)
  # la somme totale des carrés (SST)
  SST <- sum((df_valid[,col_posi] - mean(df_valid[,col_posi])) ^ 2)
  r_adj <- 1 - (SSR / SST)

  
  MAE <- mean(abs(pred.GAM - df_valid[,col_posi]))
  results <- list(RMSE = rmse, R_squared= r_adj, MAE = MAE,model = modelgam)
  return(results)
}
# GAM(var_rep ="AB_tot", df_app=df_train, df_valid = df_test,family = 'gaussian',method = "REML")
# GAM(var_rep ="BM_tot", df_app= drop_na(df_train), df_valid = drop_na(df_test),family = 'gaussian',method = "REML")
# GAM(var_rep ="Richesse_tot", df_app=df_train, df_valid = df_test,family = 'gaussian',method = "REML")
```


## RF
-   Boosting the RF model
  -   Évaluer le modèle avec le paramètre par défaut
  -   Trouver le meilleur nombre de mtry
  -   Trouver le meilleur nombre de maxnodes
  -   Trouver le meilleur nombre d'arbres
  -   Évaluer le modèle sur l'ensemble de données de test

```{r function RF, echo=TRUE}
# # Define the control
# trControl <- trainControl(method = "cv",
#     number = 5,
#     search = "grid")
# 
# # Étape 1) default model
# set.seed(1234)
# # Run the model
# rf_default <- train(AB_tot~.,
#     data = df_train_AB_tot,
#     method = "rf",
#     metric = "RMSE",
#     trControl = trControl)
# # Print the results
# print(rf_default)
# 
# 
# 
# # Étape 2) Recherchez le meilleur essai
# set.seed(1234)
# tuneGrid <- expand.grid(.mtry = c(2: 10))
# rf_mtry <- train(AB_tot~.,
#     data = df_train_AB_tot,
#     method = "rf",
#     metric = "RMSE",
#     tuneGrid = tuneGrid,
#     trControl = trControl,
#     importance = TRUE,
#     nodesize = 14,
#     ntree = 300)
# print(rf_mtry)
# 
# ## RMSE was used to select the optimal model using  the largest value.
# ## The final value used for the model was mtry = XX
# rf_mtry$bestTune$mtry
# min(rf_mtry$results$RMSE)
# best_mtry_AB_tot <- rf_mtry$bestTune$mtry
# 
# 
# # Étape 3) Recherchez les meilleurs maxnodes
# 
# store_maxnode <- list()
# tuneGrid <- expand.grid(.mtry = best_mtry_AB_tot)
# for (maxnodes in c(50: 60)) {
#     set.seed(1234)
#     rf_maxnode <- train(AB_tot~.,
#         data = df_train_AB_tot,
#         method = "rf",
#         metric = "RMSE",
#         tuneGrid = tuneGrid,
#         trControl = trControl,
#         importance = TRUE,
#         nodesize = 14,
#         maxnodes = maxnodes,
#         ntree = 300)
#     current_iteration <- toString(maxnodes)
#     store_maxnode[[current_iteration]] <- rf_maxnode
# }
# results_mtry <- resamples(store_maxnode)
# summary(results_mtry)
# best_maxnodes_AB_tot = 60
# 
# # Étape 4) Recherchez les meilleurs ntrees
# store_maxtrees <- list()
# for (ntree in c(250, 300, 350, 400, 450, 500, 550, 600, 800, 1000, 2000)) {
#     set.seed(5678)
#     rf_maxtrees <- train(AB_tot~.,
#         data = df_train_AB_tot,
#         method = "rf",
#         metric = "RMSE",
#         tuneGrid = tuneGrid,
#         trControl = trControl,
#         importance = TRUE,
#         nodesize = 14,
#         maxnodes = 60,
#         ntree = ntree)
#     key <- toString(ntree)
#     store_maxtrees[[key]] <- rf_maxtrees
# }
# results_tree <- resamples(store_maxtrees)
# summary(results_tree)
# best_ntree_AB_tot = 2000
# 
# 
# AB_tot_fit_rf <- randomForest(AB_tot~.,
#     data=df_train_AB_tot,
#     method = "rf",
#     metric = "RMSE",
#     tuneGrid = tuneGrid,
#     trControl = trControl,
#     importance = TRUE,
#     nodesize = 14,
#     ntree = best_ntree_AB_tot,
#     maxnodes = best_maxnodes_AB_tot)


#tune model
# df_AB_tot=df_mod
# names(df_AB_tot)
# df_AB_tot = df_AB_tot[,c(6,10:32)]
# model_tuned <- tuneRF(
#   x=df_AB_tot[,-1],
#   y=df_AB_tot$AB_tot,
#   ntreeTry=500,
#   mtryStart=10,
#   stepFactor=1.5,
#   improve=0.01, doBest =TRUE,
#   trace=FALSE #don't show real-time progress
# )
# plot(model_tuned)

ForetAlea <- function(var_rep, df_app, df_valid,mtry =9,ntree= 2000,maxnodes=60){
  
  # Nombre de variables tirées aléatoirement pour la construction des arbre : mtry =    1/nombre de variable explicatives totales # nombre d'arbres : ntree = 250
  
  # var_mod=c(var_rep,predicteurs)
  # df_app = df_app[,var_mod]
  # df_valid = df_valid[,var_mod]
  
  
  col_posi <- which(names(df_app) == var_rep)
  ForeVDT <- randomForest(df_app[-col_posi], df_app[[col_posi]], mtry=mtry, ntree=ntree,maxnodes=maxnodes)
  
  # Prediction sur le jeu de validation
  col_posi <- which(names(df_valid) == var_rep)
  pred.RF<-predict(ForeVDT,newdata=as.data.frame(df_valid[,-col_posi]))
  
  # Calcul du RMSE pour évaluer la qualité du modele
  rmse <- round (sqrt(mean((df_valid[,col_posi] - pred.RF)^2)),2)
  
  # calcule du R-squared
  # la somme des carrés résiduels (SSR)
SSR <- sum((pred.RF - df_valid[,col_posi]) ^ 2)
# la somme totale des carrés (SST)
SST <- sum((df_valid[,col_posi] - mean(df_valid[,col_posi])) ^ 2)

r_adj <- 1 - (SSR / SST)
  
  
  MAE <- mean(abs(pred.RF - df_valid[,col_posi]))

    results <- list(RMSE = rmse, R_squared= r_adj,MAE = MAE, model=ForeVDT)
    
    
  return(results)
}



# ForetAlea(var_rep ="AB_tot", df_app=df_train_AB_tot, df_valid = df_test_AB_tot,mtry =9,ntree= 2000)
# 
# ForetAlea(var_rep ="AB_tot", df_app=df_train_AB_tot[,c("AB_tot",best_20_AB_tot)], df_valid = df_test_AB_tot[,c("AB_tot",best_20_AB_tot)],mtry =9,ntree= 2000)
# 
# 
# 
# 
# ForetAlea(var_rep ="BM_tot", df_app=df_train_BM_tot, df_valid = df_test_BM_tot,mtry =9,ntree= 2000)
# 
# ForetAlea(var_rep ="BM_tot", df_app=df_train_BM_tot[, c("BM_tot",best_20_BM_tot)], df_valid = df_test_BM_tot [, c("BM_tot",best_20_BM_tot)],mtry =9,ntree= 2000)
# 
# 
# 
# 
# ForetAlea(var_rep ="Richesse_tot", df_app=df_train_Richesse_tot, df_valid = df_test_Richesse_tot,mtry =9,ntree= 2000)
# 
# ForetAlea(var_rep ="Richesse_tot", df_app=df_train_Richesse_tot[, c("Richesse_tot",best_20_Richesse_tot)], df_valid = df_test_Richesse_tot [, c("Richesse_tot",best_20_Richesse_tot)],mtry =9,ntree= 2000)

```


## GBM

-   Choisir un learning rate relativement élevé. Généralement (0.1 mais entre 0.05 et 0.2 peut marché aussi).

-   Trouver le nombre optimal de ntree pour ce learning rate.

-   Corriger les hyperparamètres de l’arborescence, ajuster le learning rate et évaluer la vitesse par rapport aux performances.

-   Ajustez les paramètres spécifiques à l’arbre pour un learning rate déterminé.

-   Réduir le learning rate pour évaluer toute amélioration de la précision.

-   Utiliser les paramètres d'hyperparamètres finaux et augmentez les procédures de CV pour obtenir des estimations plus robustes. 

```{r function GBM, echo=TRUE}
# # see : https://bradleyboehmke.github.io/HOML/gbm.html
# # df_train_AB_tot
# # df_test_AB_tot
# 
# # run a basic GBM model
# set.seed(123)  # for reproducibility
# ames_gbm1 <- gbm(
#   formula = AB_tot ~ .,
#   data = df_train_AB_tot,
#   distribution = "gaussian",  # SSE loss function
#   n.trees = 5000,
#   shrinkage = 0.1,
#   interaction.depth = 3,
#   n.minobsinnode = 10,
#   cv.folds = 10
# )
# 
# # find index for number trees with minimum CV error
# best <- which.min(ames_gbm1$cv.error)
# 
# # get MSE and compute RMSE
# sqrt(ames_gbm1$cv.error[best])
# 
# # plot error curve
# gbm.perf(ames_gbm1, method = "cv")
# 
# 
# 
# # a partir de l'etape 3
# # create grid search
# hyper_grid <- expand.grid(
#   learning_rate = c(0.3, 0.1, 0.05, 0.01, 0.005),
#   RMSE = NA,
#   trees = NA,
#   time = NA
# )
# 
# # execute grid search
# for(i in seq_len(nrow(hyper_grid))) {
# 
#   # fit gbm
#   set.seed(123)  # for reproducibility
#   train_time <- system.time({
#     m <- gbm(
#       formula = AB_tot ~ .,
#       data = df_train_AB_tot,
#       distribution = "gaussian",
#       n.trees = 5000, 
#       shrinkage = hyper_grid$learning_rate[i], 
#       interaction.depth = 3, 
#       n.minobsinnode = 10,
#       cv.folds = 10 
#    )
#   })
#   
#   # add SSE, trees, and training time to results
#   hyper_grid$RMSE[i]  <- sqrt(min(m$cv.error))
#   hyper_grid$trees[i] <- which.min(m$cv.error)
#   hyper_grid$Time[i]  <- train_time[["elapsed"]]
# cat(i)
# }
# 
# # results
# arrange(hyper_grid, RMSE)
# #   learning_rate     RMSE trees time   Time
# # 1         0.010 5.635777  4174   NA  81.22
# # 2         0.050 5.655101   819   NA 107.34
# # 3         0.005 5.658337  4997   NA 103.94
# # 4         0.100 5.703994   274   NA 108.89
# # 5         0.300 5.788693   141   NA 109.50
# 
# 
# 
# 
# 
# # etape 4
# # search grid
# hyper_grid <- expand.grid(
#   n.trees = 6000,
#   shrinkage = 0.01,
#   interaction.depth = c(3, 5, 7),
#   n.minobsinnode = c(5, 10, 15)
# )
# 
# # create model fit function
# model_fit <- function(n.trees, shrinkage, interaction.depth, n.minobsinnode) {
#   set.seed(123)
#   m <- gbm(
#     formula = AB_tot ~ .,
#     data = df_train_AB_tot,
#     distribution = "gaussian",
#     n.trees = n.trees,
#     shrinkage = shrinkage,
#     interaction.depth = interaction.depth,
#     n.minobsinnode = n.minobsinnode,
#     cv.folds = 10
#   )
#   # compute RMSE
#   sqrt(min(m$cv.error))
# }
# 
# # perform search grid with functional programming
# hyper_grid$rmse <- purrr::pmap_dbl(
#   hyper_grid,
#   ~ model_fit(
#     n.trees = ..1,
#     shrinkage = ..2,
#     interaction.depth = ..3,
#     n.minobsinnode = ..4
#     )
# )
# 
# # results
# arrange(hyper_grid, rmse)
# #   n.trees shrinkage interaction.depth n.minobsinnode     rmse
# # 1    6000      0.01                 7             15 5.600324
# # 2    6000      0.01                 7             10 5.607520
# # 3    6000      0.01                 5             15 5.612125
# # 4    6000      0.01                 7              5 5.612956
# # 5    6000      0.01                 5              5 5.625295
# # 6    6000      0.01                 5             10 5.627353
# # 7    6000      0.01                 3             15 5.632210
# # 8    6000      0.01                 3             10 5.635777
# # 9    6000      0.01                 3              5 5.653090
# 
# # nouvelle tantative
# # search grid
# hyper_grid <- expand.grid(
#   n.trees = 8000,
#   shrinkage = 0.001,
#   interaction.depth = c(7),
#   n.minobsinnode = c(15)
# )
# # perform search grid with functional programming
# hyper_grid$rmse <- purrr::pmap_dbl(
#   hyper_grid,
#   ~ model_fit(
#     n.trees = ..1,
#     shrinkage = ..2,
#     interaction.depth = ..3,
#     n.minobsinnode = ..4
#     )
# )
# # results
# arrange(hyper_grid, rmse)

best_n.trees = 6000
best_shrinkage = 0.01
best_interaction.depth = 7
best_n.minobsinnode = 15



GBM <- function(var_rep, df_app, df_valid,distribution = 'gaussian',n.trees = 6000,
shrinkage = 0.01,interaction.depth = 7,n.minobsinnode = 15){
  # var_mod=c(var_rep,predicteurs)
  # df_app = df_app[,var_mod]
  # df_valid = df_valid[,var_mod]
  formula <- substitute(var_rep ~ ., list(var_rep = as.name(var_rep)))
  # entrainement du modele sur le jeu d'entrainement
  Gradboost<-gbm(formula, data = df_app,
    distribution = distribution, 
    n.trees = n.trees,
    shrinkage = shrinkage,
    interaction.depth = interaction.depth,
    n.minobsinnode = n.minobsinnode,
    cv.folds = 10) 
  
  # Prediction sur le jeu de validation :
   col_posi <- which(names(df_valid) == var_rep)
  prev.GBM<-predict(Gradboost,newdata=as.data.frame(df_valid[,-col_posi]))
 
  # Calcul du RMSE pour évaluer la qualité du modele
  rmse <- round (sqrt(mean((df_valid[,var_rep] - prev.GBM)^2)),2)


 # calcule du R-squared
  # la somme des carrés résiduels (SSR)
SSR <- sum((prev.GBM - df_valid[,col_posi]) ^ 2)
# la somme totale des carrés (SST)
SST <- sum((df_valid[,col_posi] - mean(df_valid[,col_posi])) ^ 2)

r_adj <- 1 - (SSR / SST)
  
  MAE <- mean(abs(prev.GBM - df_valid[,col_posi])) 
  results <- list(RMSE = rmse, R_squared= r_adj,MAE = MAE, model = Gradboost)
  return(results)
}


# GBM_test = GBM(var_rep ="AB_tot", df_app=df_train_AB_tot, df_valid = df_test_AB_tot,distribution = 'gaussian',n.trees = 6000,shrinkage = 0.01,interaction.depth = 7,n.minobsinnode = 15)
# summary(GBM_test$model)
# best.iter <- gbm.perf(GBM_test$model, method = "cv")
# summary(GBM_test$model, n.trees = best.iter)



# GBM(var_rep ="AB_tot", df_app=df_train_AB_tot[,c("AB_tot",best_20_AB_tot)], df_valid = df_test_AB_tot[,c("AB_tot",best_20_AB_tot)],distribution = 'gaussian',n.trees = 6000,
# shrinkage = 0.01,interaction.depth = 7,n.minobsinnode = 15)



# GBM(var_rep ="BM_tot", df_app=df_train_BM_tot, df_valid = df_test_BM_tot,distribution = 'gaussian',n.trees = 6000,
# shrinkage = 0.01,interaction.depth = 7,n.minobsinnode = 15)

# GBM(var_rep ="BM_tot", df_app=df_train_BM_tot[, c("BM_tot",best_20_BM_tot)], df_valid = df_test_BM_tot[, c("BM_tot",best_20_BM_tot)],distribution = 'gaussian',n.trees = 6000,
# shrinkage = 0.01,interaction.depth = 7,n.minobsinnode = 15)




# GBM(var_rep ="Richesse_tot", df_app=df_train_Richesse_tot, df_valid = df_test_Richesse_tot,distribution = 'gaussian',n.trees = 6000,shrinkage = 0.01,interaction.depth = 7,n.minobsinnode = 15)

# GBM(var_rep ="Richesse_tot", df_app=df_train_Richesse_tot[, c("Richesse_tot",best_20_Richesse_tot)], df_valid = df_test_Richesse_tot [, c("Richesse_tot",best_20_Richesse_tot)],distribution = 'gaussian',n.trees = 6000,
# shrinkage = 0.01,interaction.depth = 7,n.minobsinnode = 15)

```


## ANN
```{r}
# Pour AB_tot
ANN_df_train_AB_tot = df_train_AB_tot[,c("AB_tot",best_20_AB_tot)]
colnames(ANN_df_train_AB_tot)[colnames(ANN_df_train_AB_tot) == "clcm_lvl3"] <- "clcm_lvl3"

dummy_vars <- model.matrix(~ clcm_lvl3 - 1, data = ANN_df_train_AB_tot)
ANN_df_train_AB_tot <- cbind(ANN_df_train_AB_tot, dummy_vars)
ANN_df_train_AB_tot <- ANN_df_train_AB_tot[, -which(names(ANN_df_train_AB_tot) == "clcm_lvl3")]


ANN_df_test_AB_tot = df_test_AB_tot[,c("AB_tot",best_20_AB_tot)]
colnames(ANN_df_test_AB_tot)[colnames(ANN_df_test_AB_tot) == "clcm_lvl3"] <- "clcm_lvl3"

dummy_vars <- model.matrix(~ clcm_lvl3 - 1, data = ANN_df_test_AB_tot)
ANN_df_test_AB_tot <- cbind(ANN_df_test_AB_tot, dummy_vars)
ANN_df_test_AB_tot <- ANN_df_test_AB_tot[, -which(names(ANN_df_test_AB_tot) == "clcm_lvl3")]

write.csv2(x =ANN_df_train_AB_tot,file = "Results/AB_tot_train.csv", row.names = FALSE)
write.csv2(x =ANN_df_test_AB_tot,file = "Results/AB_tot_test.csv", row.names = FALSE)


# Pour BM_tot
ANN_df_train_BM_tot = df_train_BM_tot[,c("BM_tot",best_20_BM_tot)]
colnames(ANN_df_train_BM_tot)[colnames(ANN_df_train_BM_tot) == "clcm_lvl3"] <- "clcm_lvl3"

dummy_vars <- model.matrix(~ clcm_lvl3 - 1, data = ANN_df_train_BM_tot)
ANN_df_train_BM_tot <- cbind(ANN_df_train_BM_tot, dummy_vars)
ANN_df_train_BM_tot <- ANN_df_train_BM_tot[, -which(names(ANN_df_train_BM_tot) == "clcm_lvl3")]


ANN_df_test_BM_tot = df_test_BM_tot[,c("BM_tot",best_20_BM_tot)]
colnames(ANN_df_test_BM_tot)[colnames(ANN_df_test_BM_tot) == "clcm_lvl3"] <- "clcm_lvl3"

dummy_vars <- model.matrix(~ clcm_lvl3 - 1, data = ANN_df_test_BM_tot)
ANN_df_test_BM_tot <- cbind(ANN_df_test_BM_tot, dummy_vars)
ANN_df_test_BM_tot <- ANN_df_test_BM_tot[, -which(names(ANN_df_test_BM_tot) == "clcm_lvl3")]

write.csv2(x =ANN_df_train_BM_tot,file = "Results/BM_tot_train.csv", row.names = FALSE)
write.csv2(x =ANN_df_test_BM_tot,file = "Results/BM_tot_test.csv", row.names = FALSE)




# Pour Richesse_tot
ANN_df_train_Richesse_tot = df_train_Richesse_tot[,c("Richesse_tot",best_20_Richesse_tot)]
colnames(ANN_df_train_Richesse_tot)[colnames(ANN_df_train_Richesse_tot) == "clcm_lvl3"] <- "clcm_lvl3"

dummy_vars <- model.matrix(~ clcm_lvl3 - 1, data = ANN_df_train_Richesse_tot)
ANN_df_train_Richesse_tot <- cbind(ANN_df_train_Richesse_tot, dummy_vars)
ANN_df_train_Richesse_tot <- ANN_df_train_Richesse_tot[, -which(names(ANN_df_train_Richesse_tot) == "clcm_lvl3")]


ANN_df_test_Richesse_tot = df_test_Richesse_tot[,c("Richesse_tot",best_20_Richesse_tot)]
colnames(ANN_df_test_Richesse_tot)[colnames(ANN_df_test_Richesse_tot) == "clcm_lvl3"] <- "clcm_lvl3"

dummy_vars <- model.matrix(~ clcm_lvl3 - 1, data = ANN_df_test_Richesse_tot)
ANN_df_test_Richesse_tot <- cbind(ANN_df_test_Richesse_tot, dummy_vars)
ANN_df_test_Richesse_tot <- ANN_df_test_Richesse_tot[, -which(names(ANN_df_test_Richesse_tot) == "clcm_lvl3")]

write.csv2(x =ANN_df_train_Richesse_tot,file = "Results/Richesse_tot_train.csv", row.names = FALSE)
write.csv2(x =ANN_df_test_Richesse_tot,file = "Results/Richesse_tot_test.csv", row.names = FALSE)

```


```{r function ANN, echo=TRUE}
# Fonction ANN pour entraîner un modèle de réseau neuronal artificiel
# et évaluer ses performances

set.seed(12321)



# ANN1 <- neuralnet::neuralnet(AB_tot~.,               
#                       data = ANN_df_train_AB_tot,linear.output = FALSE) 
# plot(ANN1, rep = 'best')







ANN = function(var_rep, df_app, df_valid){

  formula <- substitute(var_rep ~ ., list(var_rep = as.name(var_rep)))
  # entrainement du modele sur le jeu d'entrainement
  ResNeu <- neuralnet(formula,               #Modèle à ajuster
                      data = df_app,         #Jeu de données avec variables de la formule
                      hidden = c(11,5,2),#Nombre de neurones sur la couche cachée
                      #err.fct = "sse",  #Erreur estimée par la somme des carrés des erreurs absolues que l'algorithme va chercher à minimiser
                      linear.output = FALSE,   #Application linéaire aux neuronnes de sortie
                      #lifesign = 'full',       #Imprimer ou non le détail du calcul
                      rep = 2,                 #Nombre de répétition pour l'entraînement du réseau
                      #algorithm = "rprop+",    #retropropagation résiliente avec retour arriere de poids
                      #stepmax = 3000
                      )      #Etapes maximales pour la formation du reseau
  
  #Sélection de la répétition la plus efficace
   col_posi <- which(names(df_valid) == var_rep)
   repp1 = ResNeu$result.matrix[1,1]
   repp2 = ResNeu$result.matrix[1,2]
   
  if(repp1  < repp2){
    
    output <- compute(ResNeu, rep = 1, df_valid[,-col_posi])
  }
  if(repp1  > repp2){
    output <- compute(ResNeu, rep = 2, df_valid[,-col_posi])
  }
 


   # Calcul de la RMSE
rmse <-sqrt(mean((df_valid[,col_posi]-(output$net.result*(max(df_app[,var_rep])-min(df_app[,var_rep])+min(df_app[,var_rep]))))^2))
rmse = round(rmse,2)  
  
  
   # Calcul du McFadden's R-squared pour évaluer la qualite du modele
  r_squared = 1 
  r_adj <- round (100*r_squared,2)
  
  
  results <- list(RMSE = rmse,R_squared= r_squared, model = ResNeu)
  return(results)
}

# resl = ANN(var_rep ="AB_tot" , df_app= ANN_df_train_AB_tot, df_valid = ANN_df_test_AB_tot)
# 
# resl$RMSE
# resl$R_squared
# 
# 
# ANN_mode = resl$model
# plot(ANN_mode, rep = 'best')
```



## Tableau des output
```{r output,echo=FALSE}
# # Pour AB_tot
# AB_tot_RMSE_Results<-matrix(0,ncol=6,nrow=n_sim,dimnames=list(1:n_sim,c("RMSE_GLM", "RMSE_GAM", "RMSE_RF","RMSE_GBM","RMSE_ANN","RMS_BestMod")))
# AB_tot_RMSE_Results<-as.data.frame(AB_tot_RMSE_Results)
# 
# AB_tot_R_Results<-matrix(0,ncol=6,nrow=n_sim,dimnames=list(1:n_sim,c("R_GLM", "R_GAM", "R_RF","R_GBM","R_ANN","R_BestMod")))
# AB_tot_R_Results<-as.data.frame(AB_tot_R_Results)
# 
# 
# 
# # Pour BM_tot
# BM_tot_RMSE_Results<- AB_tot_RMSE_Results
# 
# BM_tot_R_Results<-AB_tot_R_Results
# 
# 
# 
# # Pour Richesse_tot
# Richesse_tot_RMSE_Results<-AB_tot_RMSE_Results
# 
# Richesse_tot_R_Results<-AB_tot_R_Results
# 
# 
# head(AB_tot_RMSE_Results)
# head(AB_tot_R_Results)



```

## Compilation pour chaque algoritme


-   GLM
```{r Loop.GLM}
# # Pour AB_tot ------------------------------------------------------------------
# temps1=Sys.time()
# set.seed(1234)
# 
# # cl <- detectCores() %>% -1 %>% makeCluster
# # registerDoParallel(cl)
# 
# for (i in 1:n_sim){
# lignes <- createDataPartition(data_mod$clcm_lvl3, p = 0.8, list = FALSE)
# 
# # Séparer les données en ensembles d'entraînement et de test
# b_df_train <- data_mod[lignes, ]  # Données d'entraînement
# b_df_test <- data_mod[-lignes, ]  # Données de test
# b_df_train = droplevels(b_df_train)
# b_df_test = droplevels(b_df_test)
# 
# # GLM
# GLM_reslt_AB_tot = GLM(var_rep ="AB_tot",df_app = b_df_train, df_valid = b_df_test,family = 'gaussian')
# 
# AB_tot_RMSE_Results[i,"RMSE_GLM"] <- GLM_reslt_AB_tot$RMSE
# 
# AB_tot_R_Results[i,"R_GLM"] <- GLM_reslt_AB_tot$R_squared
# 
# cat("Itération : ",i, "/",n_sim,"\n")
# }
# temps2=Sys.time()
# duree=difftime(temps2,temps1)
# # stopCluster(cl)
# # head(AB_tot_RMSE_Results)
# # head(AB_tot_R_Results)
# write.csv2(x=AB_tot_RMSE_Results$RMSE_GLM, file = "Results/GLM_AB_tot_RMSE.csv")
# write.csv2(x=AB_tot_R_Results$R_GLM, file = "Results/GLM_AB_tot_R.csv")





# # Pour BM_tot ------------------------------------------------------------------
# temps1=Sys.time()
# set.seed(1234)
# 
# # cl <- detectCores() %>% -1 %>% makeCluster
# # registerDoParallel(cl)
# 
# for (i in 1:n_sim){
# lignes <- createDataPartition(data_mod$clcm_lvl3, p = 0.8, list = FALSE)
# 
# # Séparer les données en ensembles d'entraînement et de test
# b_df_train <- data_mod[lignes, ]  # Données d'entraînement
# b_df_test <- data_mod[-lignes, ]  # Données de test
# b_df_train = droplevels(b_df_train)
# b_df_test = droplevels(b_df_test)
# 
# # GLM
# GLM_reslt_BM_tot = GLM(var_rep ="BM_tot",df_app = drop_na(b_df_train), df_valid = drop_na(b_df_test),family = 'gaussian')
# 
# BM_tot_RMSE_Results[i,"RMSE_GLM"] <- GLM_reslt_BM_tot$RMSE
# 
# BM_tot_R_Results[i,"R_GLM"] <- GLM_reslt_BM_tot$R_squared
# 
# cat("Itération : ",i, "/",n_sim,"\n")
# }
# temps2=Sys.time()
# duree=difftime(temps2,temps1)
# # stopCluster(cl)
# # head(BM_tot_RMSE_Results)
# # head(BM_tot_R_Results)
# write.csv2(x=BM_tot_RMSE_Results$RMSE_GLM, file = "Results/GLM_BM_tot_RMSE.csv")
# write.csv2(x=BM_tot_R_Results$R_GLM, file = "Results/GLM_BM_tot_R.csv")





# # Pour Richesse_tot ------------------------------------------------------------------
# temps1=Sys.time()
# set.seed(1234)
# 
# # cl <- detectCores() %>% -1 %>% makeCluster
# # registerDoParallel(cl)
# 
# for (i in 1:n_sim){
# lignes <- createDataPartition(data_mod$clcm_lvl3, p = 0.8, list = FALSE)
# 
# # Séparer les données en ensembles d'entraînement et de test
# b_df_train <- data_mod[lignes, ]  # Données d'entraînement
# b_df_test <- data_mod[-lignes, ]  # Données de test
# b_df_train = droplevels(b_df_train)
# b_df_test = droplevels(b_df_test)
# 
# # GLM
# GLM_reslt_Richesse_tot = GLM(var_rep ="Richesse_tot",df_app = b_df_train, df_valid = b_df_test,family = 'gaussian')
# 
# Richesse_tot_RMSE_Results[i,"RMSE_GLM"] <- GLM_reslt_Richesse_tot$RMSE
# 
# Richesse_tot_R_Results[i,"R_GLM"] <- GLM_reslt_Richesse_tot$R_squared
# 
# cat("Itération : ",i, "/",n_sim,"\n")
# }
# temps2=Sys.time()
# duree=difftime(temps2,temps1)
# # stopCluster(cl)
# # head(Richesse_tot_RMSE_Results)
# # head(Richesse_tot_R_Results)
# write.csv2(x=Richesse_tot_RMSE_Results$RMSE_GLM, file = "Results/GLM_Richesse_tot_RMSE.csv")
# write.csv2(x=Richesse_tot_R_Results$R_GLM, file = "Results/GLM_Richesse_tot_R.csv")

```




-   GAM
```{r Loop.GAM}
# # Pour AB_tot ------------------------------------------------------------------
# temps1=Sys.time()
# set.seed(1234)
# 
# # cl <- detectCores() %>% -1 %>% makeCluster
# # registerDoParallel(cl)
# 
# for (i in 1:n_sim){
# lignes <- createDataPartition(data_mod$clcm_lvl3, p = 0.8, list = FALSE)
# 
# # Séparer les données en ensembles d'entraînement et de test
# b_df_train <- data_mod[lignes, ]  # Données d'entraînement
# b_df_test <- data_mod[-lignes, ]  # Données de test
# b_df_train = droplevels(b_df_train)
# b_df_test = droplevels(b_df_test)
# 
# # GAM
# GAM_reslt_AB_tot <- GAM(var_rep ="AB_tot",df_app = b_df_train, df_valid = b_df_test,family = 'gaussian',method = "REML")
# 
# AB_tot_RMSE_Results[i,"RMSE_GAM"] = GAM_reslt_AB_tot$RMSE
# 
# AB_tot_R_Results[i,"R_GAM"] = GAM_reslt_AB_tot$R_squared
# 
# cat("Itération : ",i, "/",n_sim,"\n")
# }
# temps2=Sys.time()
# duree=difftime(temps2,temps1)
# # stopCluster(cl)
# # head(AB_tot_RMSE_Results)
# # head(AB_tot_R_Results)
# write.csv2(x=AB_tot_RMSE_Results$RMSE_GAM, file = "Results/GAM_AB_tot_RMSE.csv")
# write.csv2(x=AB_tot_R_Results$R_GAM, file = "Results/GAM_AB_tot_R.csv")




# # Pour BM_tot ------------------------------------------------------------------
# temps1=Sys.time()
# set.seed(1234)
# 
# # cl <- detectCores() %>% -1 %>% makeCluster
# # registerDoParallel(cl)
# 
# for (i in 1:n_sim){
# lignes <- createDataPartition(data_mod$clcm_lvl3, p = 0.8, list = FALSE)
# 
# # Séparer les données en ensembles d'entraînement et de test
# b_df_train <- data_mod[lignes, ]  # Données d'entraînement
# b_df_test <- data_mod[-lignes, ]  # Données de test
# b_df_train = droplevels(b_df_train)
# b_df_test = droplevels(b_df_test)
# 
# # GAM
# GAM_reslt_BM_tot <- GAM(var_rep ="BM_tot",df_app = drop_na(b_df_train), df_valid = drop_na(b_df_test),family = 'gaussian',method = "REML")
# 
# BM_tot_RMSE_Results[i,"RMSE_GAM"] = GAM_reslt_BM_tot$RMSE
# 
# BM_tot_R_Results[i,"R_GAM"] = GAM_reslt_BM_tot$R_squared
# 
# cat("Itération : ",i, "/",n_sim,"\n")
# }
# temps2=Sys.time()
# duree=difftime(temps2,temps1)
# # stopCluster(cl)
# # head(BM_tot_RMSE_Results)
# # head(BM_tot_R_Results)
# write.csv2(x=BM_tot_RMSE_Results$RMSE_GAM, file = "Results/GAM_BM_tot_RMSE.csv")
# write.csv2(x=BM_tot_R_Results$R_GAM, file = "Results/GAM_BM_tot_R.csv")




# # Pour Richesse_tot ------------------------------------------------------------------
# temps1=Sys.time()
# set.seed(1234)
# 
# # cl <- detectCores() %>% -1 %>% makeCluster
# # registerDoParallel(cl)
# 
# for (i in 1:n_sim){
# lignes <- createDataPartition(data_mod$clcm_lvl3, p = 0.8, list = FALSE)
# 
# # Séparer les données en ensembles d'entraînement et de test
# b_df_train <- data_mod[lignes, ]  # Données d'entraînement
# b_df_test <- data_mod[-lignes, ]  # Données de test
# b_df_train = droplevels(b_df_train)
# b_df_test = droplevels(b_df_test)
# 
# # GAM
# GAM_reslt_Richesse_tot <- GAM(var_rep ="Richesse_tot",df_app = b_df_train, df_valid = b_df_test,family = 'gaussian',method = "REML")
# 
# Richesse_tot_RMSE_Results[i,"RMSE_GAM"] = GAM_reslt_Richesse_tot$RMSE
# 
# Richesse_tot_R_Results[i,"R_GAM"] = GAM_reslt_Richesse_tot$R_squared
# 
# cat("Itération : ",i, "/",n_sim,"\n")
# }
# temps2=Sys.time()
# duree=difftime(temps2,temps1)
# # stopCluster(cl)
# # head(Richesse_tot_RMSE_Results)
# # head(Richesse_tot_R_Results)
# write.csv2(x=Richesse_tot_RMSE_Results$RMSE_GAM, file = "Results/GAM_Richesse_tot_RMSE.csv")
# write.csv2(x=Richesse_tot_R_Results$R_GAM, file = "Results/GAM_Richesse_tot_R.csv")
```




-   RF
```{r Loop.RF}
# # Pour AB_tot  ------------------------------------------------------------------
# temps1=Sys.time()
# set.seed(1234)
# 
# # cl <- detectCores() %>% -1 %>% makeCluster
# # registerDoParallel(cl)
# 
# for (i in 1:n_sim){
# lignes <- createDataPartition(df_mod_AB_tot$clcm_lvl3, p = 0.8, list = FALSE)
# 
# # Séparer les données en ensembles d'entraînement et de test
# b_df_train <- df_mod_AB_tot[lignes, ]  # Données d'entraînement
# b_df_test <- df_mod_AB_tot[-lignes, ]  # Données de test
# b_df_train = droplevels(b_df_train)
# b_df_test = droplevels(b_df_test)
# 
# # RF
# RF_result = ForetAlea(var_rep ="AB_tot", df_app=b_df_train[,c("AB_tot",best_20_AB_tot)], df_valid = b_df_test[,c("AB_tot",best_20_AB_tot)],mtry =9,ntree= 2000)
# 
# AB_tot_RMSE_Results[i,"RMSE_RF"] <- RF_result$RMSE
# 
# AB_tot_R_Results[i,"R_RF"] <- RF_result$R_squared
# 
# 
# cat("Itération : ",i, "/",n_sim,"\n")
# }
# temps2=Sys.time()
# duree=difftime(temps2,temps1)
# # stopCluster(cl)
# # head(AB_tot_RMSE_Results)
# # head(AB_tot_R_Results)
# # mean(AB_tot_RMSE_Results$RMSE_RF)
# # mean(AB_tot_R_Results$R_RF)
# 
# write.csv2(x=AB_tot_RMSE_Results$RMSE_RF, file = "Results/RF_AB_tot_RMSE.csv")
# write.csv2(x=AB_tot_R_Results$R_RF, file = "Results/RF_AB_tot_R.csv")


# # Pour BM_tot  ------------------------------------------------------------------
# temps1=Sys.time()
# set.seed(1234)
# 
# # cl <- detectCores() %>% -1 %>% makeCluster
# # registerDoParallel(cl)
# 
# for (i in 1:n_sim){
# lignes <- createDataPartition(df_mod_BM_tot$clcm_lvl3, p = 0.8, list = FALSE)
# 
# # Séparer les données en ensembles d'entraînement et de test
# b_df_train <- df_mod_BM_tot[lignes, ]  # Données d'entraînement
# b_df_test <- df_mod_BM_tot[-lignes, ]  # Données de test
# b_df_train = droplevels(b_df_train)
# b_df_test = droplevels(b_df_test)
# 
# # RF
# RF_result = ForetAlea(var_rep ="BM_tot", df_app=b_df_train[,c("BM_tot",best_20_BM_tot)], df_valid = b_df_test[,c("BM_tot",best_20_BM_tot)],mtry =9,ntree= 2000)
# 
# BM_tot_RMSE_Results[i,"RMSE_RF"] <- RF_result$RMSE
# 
# BM_tot_R_Results[i,"R_RF"] <- RF_result$R_squared
# 
# 
# cat("Itération : ",i, "/",n_sim,"\n")
# }
# temps2=Sys.time()
# duree=difftime(temps2,temps1)
# # stopCluster(cl)
# # head(BM_tot_RMSE_Results)
# # head(BM_tot_R_Results)
# # mean(BM_tot_RMSE_Results$RMSE_RF)
# # mean(BM_tot_R_Results$R_RF)
# 
# write.csv2(x=BM_tot_RMSE_Results$RMSE_RF, file = "Results/RF_BM_tot_RMSE.csv")
# write.csv2(x=BM_tot_RMSE_Results$R_RF, file = "Results/RF_BM_tot_R.csv")




# # Pour Richesse_tot  ------------------------------------------------------------------
# temps1=Sys.time()
# set.seed(1234)
# 
# # cl <- detectCores() %>% -1 %>% makeCluster
# # registerDoParallel(cl)
# 
# for (i in 1:n_sim){
# lignes <- createDataPartition(df_mod_Richesse_tot$clcm_lvl3, p = 0.8, list = FALSE)
# 
# # Séparer les données en ensembles d'entraînement et de test
# b_df_train <- df_mod_Richesse_tot[lignes, ]  # Données d'entraînement
# b_df_test <- df_mod_Richesse_tot[-lignes, ]  # Données de test
# b_df_train = droplevels(b_df_train)
# b_df_test = droplevels(b_df_test)
# 
# # RF
# RF_result = ForetAlea(var_rep ="Richesse_tot", df_app=b_df_train[,c("Richesse_tot",best_20_Richesse_tot)], df_valid = b_df_test[,c("Richesse_tot",best_20_Richesse_tot)],mtry =9,ntree= 2000)
# 
# Richesse_tot_RMSE_Results[i,"RMSE_RF"] <- RF_result$RMSE
# 
# Richesse_tot_R_Results[i,"R_RF"] <- RF_result$R_squared
# 
# 
# cat("Itération : ",i, "/",n_sim,"\n")
# }
# temps2=Sys.time()
# duree=difftime(temps2,temps1)
# # stopCluster(cl)
# # head(Richesse_tot_RMSE_Results)
# # head(Richesse_tot_R_Results)
# # mean(Richesse_tot_RMSE_Results$RMSE_RF)
# # mean(Richesse_tot_R_Results$R_RF)
# 
# write.csv2(x=Richesse_tot_RMSE_Results$RMSE_RF, file = "Results/RF_Richesse_tot_RMSE.csv")
# write.csv2(x=Richesse_tot_R_Results$R_RF, file = "Results/RF_Richesse_tot_R.csv")
```


-   GBM
```{r Loop.GBM}
# Pour AB_tot  ------------------------------------------------------------------
# temps1=Sys.time()
# set.seed(1234)
# 
# # cl <- detectCores() %>% -1 %>% makeCluster
# # registerDoParallel(cl)
# 
# for (i in 1:n_sim){
# lignes <- createDataPartition(df_mod_AB_tot$clcm_lvl3, p = 0.8, list = FALSE)
# 
# # Séparer les données en ensembles d'entraînement et de test
# b_df_train <- df_mod_AB_tot [lignes, ]  # Données d'entraînement
# b_df_test <- df_mod_AB_tot [-lignes, ]  # Données de test
# b_df_train = droplevels(b_df_train)
# b_df_test = droplevels(b_df_test)
# 
# # GBM
# GBM_result = GBM(var_rep ="AB_tot", df_app=b_df_train[,c("AB_tot",best_20_AB_tot)], df_valid = b_df_test[,c("AB_tot",best_20_AB_tot)],distribution = 'gaussian',n.trees = 6000,shrinkage = 0.01,interaction.depth = 7,n.minobsinnode = 15)
# 
# AB_tot_RMSE_Results[i,"RMSE_GBM"] <- GBM_result$RMSE
# 
# AB_tot_R_Results[i,"R_GBM"] <- GBM_result$R_squared
# 
# 
# cat("Itération : ",i, "/",n_sim,"\n")
# }
# temps2=Sys.time()
# duree=difftime(temps2,temps1)
# # stopCluster(cl)
# # head(AB_tot_RMSE_Results)
# # head(AB_tot_R_Results)
# write.csv2(x=AB_tot_RMSE_Results$RMSE_GBM, file = "Results/GBM_AB_tot_RMSE.csv")
# write.csv2(x=AB_tot_R_Results$R_GBM, file = "Results/GBM_AB_tot_R.csv")




# Pour BM_tot ------------------------------------------------------------------
# temps1=Sys.time()
# set.seed(1234)
# 
# # cl <- detectCores() %>% -1 %>% makeCluster
# # registerDoParallel(cl)
# 
# for (i in 1:n_sim){
# lignes <- createDataPartition(df_mod_BM_tot$clcm_lvl3, p = 0.8, list = FALSE)
# 
# # Séparer les données en ensembles d'entraînement et de test
# b_df_train <- df_mod_BM_tot [lignes, ]  # Données d'entraînement
# b_df_test <- df_mod_BM_tot [-lignes, ]  # Données de test
# b_df_train = droplevels(b_df_train)
# b_df_test = droplevels(b_df_test)
# 
# # GBM
# GBM_result = GBM(var_rep ="BM_tot", df_app=b_df_train[,c("BM_tot",best_20_BM_tot)], df_valid = b_df_test[,c("BM_tot",best_20_BM_tot)],distribution = 'gaussian',n.trees = 6000,shrinkage = 0.01,interaction.depth = 7,n.minobsinnode = 15)
# 
# BM_tot_RMSE_Results[i,"RMSE_GBM"] <- GBM_result$RMSE
# 
# BM_tot_R_Results[i,"R_GBM"] <- GBM_result$R_squared
# 
# 
# cat("Itération : ",i, "/",n_sim,"\n")
# }
# temps2=Sys.time()
# duree=difftime(temps2,temps1)
# stopCluster(cl)
# head(BM_tot_RMSE_Results)
# head(BM_tot_R_Results)
# write.csv2(x=BM_tot_RMSE_Results$RMSE_GBM, file = "Results/GBM_BM_tot_RMSE.csv")
# write.csv2(x=BM_tot_R_Results$R_GBM, file = "Results/GBM_BM_tot_R.csv")





# # Pour Richesse_tot ------------------------------------------------------------------
# temps1=Sys.time()
# set.seed(1234)
# 
# # cl <- detectCores() %>% -1 %>% makeCluster
# # registerDoParallel(cl)
# 
# for (i in 1:n_sim){
# lignes <- createDataPartition(df_mod_Richesse_tot$clcm_lvl3, p = 0.8, list = FALSE)
# 
# # Séparer les données en ensembles d'entraînement et de test
# b_df_train <- df_mod_Richesse_tot[lignes, ]  # Données d'entraînement
# b_df_test <- df_mod_Richesse_tot[-lignes, ]  # Données de test
# b_df_train = droplevels(b_df_train)
# b_df_test = droplevels(b_df_test)
# 
# # GBM
# GBM_result = GBM(var_rep ="Richesse_tot", df_app=b_df_train[,c("Richesse_tot",best_20_Richesse_tot)], df_valid = b_df_test[,c("Richesse_tot",best_20_Richesse_tot)],distribution = 'gaussian',n.trees = 6000,shrinkage = 0.01,interaction.depth = 7,n.minobsinnode = 15)
# 
# Richesse_tot_RMSE_Results[i,"RMSE_GBM"] <- GBM_result$RMSE
# 
# Richesse_tot_R_Results[i,"R_GBM"] <- GBM_result$R_squared
# 
# 
# cat("Itération : ",i, "/",n_sim,"\n")
# }
# temps2=Sys.time()
# duree=difftime(temps2,temps1)
# # stopCluster(cl)
# # head(Richesse_tot_RMSE_Results)
# # head(Richesse_tot_R_Results)
# write.csv2(x=Richesse_tot_RMSE_Results$RMSE_GBM, file = "Results/GBM_Richesse_tot_RMSE.csv")
# write.csv2(x=Richesse_tot_R_Results$R_GBM, file = "Results/GBM_Richesse_tot_R.csv")

```


-   ANN
```{r Loop.ANN}
# Pour AB_tot  ------------------------------------------------------------------
# temps1=Sys.time()
# set.seed(1234)
# 
# # cl <- detectCores() %>% -1 %>% makeCluster
# # registerDoParallel(cl)
# 
# for (i in 1:n_sim){
# lignes <- createDataPartition(data_mod$clcm_lvl3, p = 0.8, list = FALSE)
# 
# # Séparer les données en ensembles d'entraînement et de test
# b_df_train <- data_mod[lignes, ]  # Données d'entraînement
# b_df_test <- data_mod[-lignes, ]  # Données de test
# b_df_train = droplevels(b_df_train)
# b_df_test = droplevels(b_df_test)
# 
# # GAM
# AB_tot_RMSE_Results[i,"RMSE_ANN"] <- ANN(var_rep ="AB_tot" , df_app=b_df_train, df_valid = b_df_test)$RMSE
# 
# AB_tot_R_Results[i,"R_ANN"] <- ANN(var_rep ="AB_tot" , df_app=b_df_train, df_valid = b_df_test)$R_squared
# 
# 
# cat("Itération : ",i, "/",n_sim,"\n")
# }
# temps2=Sys.time()
# duree=difftime(temps2,temps1)
# # stopCluster(cl)
# # head(AB_tot_RMSE_Results)
# # head(AB_tot_R_Results)
```




# Resultats

## Sansibilité au nombre d'observation

```{r sensbilite au nbr}

df_reslt=data.frame(obs=numeric(),RMSE=numeric(),R=numeric())
n_linges=seq(0.4,1,0.1)
set.seed(1234)  # Pour rendre les résultats reproductibles

for (i in n_linges){

res = data.frame(obs=numeric(),RMSE=numeric(),R=numeric())
for (j in 1:5){ 

index <- createDataPartition(df_explo_AB_tot$clcm_lvl3, p = i, list = FALSE)

df = df_explo_AB_tot[index, ]
index <- createDataPartition(df$clcm_lvl3, p = 0.8, list = FALSE)
# Séparer les données en ensembles d'entraînement et de test
df_train <- df[index, ]  # Données d'entraînement
df_test <- df[-index, ]  # Données de test
df_train = droplevels(df_train)
df_test = droplevels(df_test)



res=ForetAlea(var_rep ="AB_tot" , df_app=df_train [,c("AB_tot",best_20_AB_tot)], df_valid = df_test [,c("AB_tot",best_20_AB_tot)],mtry =9,ntree= 2000)


df_reslt=rbind(df_reslt,data.frame(obs=i,RMSE=res$RMSE,R=res$R_squared))
}

}


df_reslt$obs_ligne = round(df_reslt$obs*nrow(df_explo_AB_tot))
#df_reslt[nrow(df_reslt)+1,] = c("1",res$RMSE,res$R_squared,nrow(df_explo_AB_tot))



df_reslt$obs=as.factor(df_reslt$obs)
df_reslt$RMSE = as.numeric(df_reslt$RMSE)
df_reslt$R = as.numeric(df_reslt$R)
df_reslt$obs_ligne = as.numeric(df_reslt$obs_ligne)


res_f = df_reslt %>% group_by(obs) %>% summarise_at(vars(RMSE,R,obs_ligne),funs(mean(.,na.rm=TRUE)))

plot(res_f$R ~ res_f$obs_ligne, xlab="Numbers of observations", ylab="R squared (%)", main="Evolution of RF R² (%)",type='p')

```






## RMSE sur le JDD test
```{r save result}
# AB_tot
AB_tot_RMSE_Results = read.csv2(file = "Results/AB_tot_RMSE_Results.csv")
AB_tot_RMSE_Results = AB_tot_RMSE_Results [,-1]
AB_tot_R_Results = read.csv2(file = "Results/AB_tot_R_Results.csv")
AB_tot_R_Results = AB_tot_R_Results[,-1]
# write.csv2(x=AB_tot_RMSE_Results, file = "Results/AB_tot_RMSE_Results.csv")
# write.csv2(x=AB_tot_R_Results, file = "Results/AB_tot_R_Results.csv")

# BM_tot
BM_tot_RMSE_Results = read.csv2(file = "Results/BM_tot_RMSE_Results.csv")
BM_tot_RMSE_Results = BM_tot_RMSE_Results[,-1]
BM_tot_R_Results = read.csv2(file = "Results/BM_tot_R_Results.csv")
BM_tot_R_Results=BM_tot_R_Results[,-1]
# write.csv2(x=BM_tot_RMSE_Results, file = "Results/BM_tot_RMSE_Results.csv")
# write.csv2(x=BM_tot_R_Results, file = "Results/BM_tot_R_Results.csv")

# Richesse_tot
Richesse_tot_RMSE_Results = read.csv2(file = "Results/Richesse_tot_RMSE_Results.csv")
Richesse_tot_RMSE_Results= Richesse_tot_RMSE_Results[,-1]
Richesse_tot_R_Results = read.csv2(file = "Results/Richesse_tot_R_Results.csv")
Richesse_tot_R_Results = Richesse_tot_R_Results[,-1]
# write.csv2(x=Richesse_tot_RMSE_Results, file = "Results/Richesse_tot_RMSE_Results.csv")
# write.csv2(x=Richesse_tot_R_Results, file = "Results/Richesse_tot_R_Results.csv")
```

-   AB_tot
```{r RMSE.plot AB_tot, fig.align='center', fig.height=5, fig.dpi=300}
Results_long = AB_tot_RMSE_Results%>%
  select(starts_with("RMSE"))%>%
  pivot_longer(everything())

Results_long$name = as.factor(Results_long$name)
levels(Results_long$name) = c("ANN","GAM", "GBM", "GLM", "RF")
ordonee = c("GLM","GAM","RF","GBM","ANN")
Results_long$name <- factor(Results_long$name, levels = ordonee)
Results_long = as.data.frame(Results_long)
Results_long = Results_long[!Results_long$name=="ANN",]
Results_long$value = Results_long$value^2 # Pour retrouver les données de base

recap = Results_long%>%
  group_by(name)%>%
  summarise(mean.rmse = mean(value),
            sd.rmse = sd(value))


means <- colMeans(AB_tot_R_Results[,-c(5:6)], na.rm = TRUE)
means = round(means,2)
recap$R_squared = means

subtitle <- sprintf("Abundance : %.2f ± %.2f ind/m²", mean(bdd_explo$AB_tot, na.rm = TRUE), sd(bdd_explo$AB_tot, na.rm = TRUE))
result_AB_tot = ggplot()+
  geom_sina(data=Results_long, aes(name, value), alpha = 0.2,shape=19, cex = 2,
            method = "density")+
  geom_point(data = recap, aes(name, mean.rmse), cex =2, col = "orange")+
  geom_errorbar(data = recap, 
                aes(x= name, ymin=mean.rmse-sd.rmse, ymax=mean.rmse+sd.rmse),
                cex =1.2, width=.1, position = position_dodge(0.5), col = "orange")+
  geom_text(data = recap, aes(x = name, y = mean.rmse, label = paste0 ("R² = ",round(R_squared, 2))),
            vjust = -7, size = 6)+   # Ajoute le texte avec les valeurs R²
  labs(y= "RMSE",x ="Models",title = subtitle)+ 
  scale_y_continuous(limits = c(25, 50), breaks = seq(25, 50, by = 5))+
  theme_bw()+
  theme(line = element_blank(), 
        axis.line = element_line(colour = "black"),
        panel.border = element_blank(),
        axis.ticks =  element_line(colour = "black"),
        axis.text.x = element_text(colour = "black", size=20,
                                   angle = 45, hjust = 1),
        axis.text.y = element_text(colour = "black", size=20),
        legend.title = element_text(colour = "black", size=20),
        legend.title.align=0.5,
        legend.text = element_text(colour = "black", size=18),
        axis.title=element_text(size=28),
        strip.background = element_rect(fill="white"))

ggsave("Results/Result_abundance.png", plot = result_AB_tot, dpi = 300)
result_AB_tot

#combined_plot <- grid.arrange(result_AB_tot, R_result_AB_tot, ncol = 2)
```

-   BM_tot
```{r RMSE.plot BM_tot, fig.align='center', fig.height=5, fig.dpi=300}
Results_long = BM_tot_RMSE_Results%>%
  select(starts_with("RMSE"))%>%
  pivot_longer(everything())

Results_long$name = as.factor(Results_long$name)
levels(Results_long$name) = c("ANN","GAM", "GBM", "GLM", "RF")
ordonee = c("GLM","GAM","RF","GBM","ANN")
Results_long$name <- factor(Results_long$name, levels = ordonee)
Results_long = as.data.frame(Results_long)
Results_long = Results_long[!Results_long$name=="ANN",]
Results_long$value = Results_long$value^2 # Pour retrouver les données de base

recap = Results_long%>%
  group_by(name)%>%
  summarise(mean.rmse = mean(value),
            sd.rmse = sd(value),
            max.rmse = max(value))

means <- colMeans(BM_tot_R_Results[,-c(5:6)], na.rm = TRUE)
means = round(means,2)
recap$R_squared = means

subtitle <- sprintf("Biomass : %.2f ± %.2f g/m²", mean(bdd_explo$BM_tot, na.rm = TRUE), sd(bdd_explo$BM_tot, na.rm = TRUE))
result_BM_tot = ggplot()+
  geom_sina(data=Results_long, aes(name, value), alpha = 0.2,shape=19, cex = 2,
            method = "density")+
  geom_point(data = recap, aes(name, mean.rmse), cex =2, col = "orange")+
  geom_errorbar(data = recap, 
                aes(x= name, ymin=mean.rmse-sd.rmse, ymax=mean.rmse+sd.rmse),
                cex =1.2, width=.1, position = position_dodge(0.5), col = "orange")+
    geom_text(data = recap, aes(x = name, y = max.rmse, label = paste0 ("R² = ",round(R_squared, 2))),
            vjust = -1, size = 6)+   # Ajoute le texte avec les valeurs R²
  labs(y= "RMSE",x ="Models",title = subtitle)+ 
  scale_y_continuous(limits = c(6, 18), breaks = seq(6, 18, by = 4))+
  theme_bw()+
  theme(line = element_blank(), 
        axis.line = element_line(colour = "black"),
        panel.border = element_blank(),
        axis.ticks =  element_line(colour = "black"),
        axis.text.x = element_text(colour = "black", size=20,
                                   angle = 45, hjust = 1),
        axis.text.y = element_text(colour = "black", size=20),
        legend.title = element_text(colour = "black", size=20),
        legend.title.align=0.5,
        legend.text = element_text(colour = "black", size=18),
        axis.title=element_text(size=28),
        strip.background = element_rect(fill="white"))

ggsave("Results/Result_biomass.png", plot = result_BM_tot, dpi = 300)
result_BM_tot
```

-   Richesse_tot
```{r RMSE.plot Richesse_tot, fig.align='center', fig.height=5, fig.dpi=300}
Results_long = Richesse_tot_RMSE_Results%>%
  select(starts_with("RMSE"))%>%
  pivot_longer(everything())

Results_long$name = as.factor(Results_long$name)
levels(Results_long$name) = c("ANN","GAM", "GBM", "GLM", "RF")
ordonee = c("GLM","GAM","RF","GBM","ANN")
Results_long$name <- factor(Results_long$name, levels = ordonee)
Results_long = as.data.frame(Results_long)
Results_long = Results_long[!Results_long$name=="ANN",]
#Results_long$value = Results_long$value^2 # Pour retrouver les données de base

recap = Results_long%>%
  group_by(name)%>%
  summarise(mean.rmse = mean(value),
            sd.rmse = sd(value),
            max.rmse = max(value))

means <- colMeans(Richesse_tot_R_Results[,-c(5:6)], na.rm = TRUE)
means = round(means,2)
recap$R_squared = means

subtitle <- sprintf("Richness : %.2f ± %.2f", mean(bdd_explo$Richesse_tot, na.rm = TRUE), sd(bdd_explo$Richesse_tot, na.rm = TRUE))

result_Richesse_tot = ggplot()+
  geom_sina(data=Results_long, aes(name, value), alpha = 0.2,shape=19, cex = 2,
            method = "density")+
  geom_point(data = recap, aes(name, mean.rmse), cex =2, col = "orange")+
  geom_errorbar(data = recap, 
                aes(x= name, ymin=mean.rmse-sd.rmse, ymax=mean.rmse+sd.rmse),
                cex =1.2, width=.1, position = position_dodge(0.5), col = "orange")+
  geom_text(data = recap, aes(x = name, y = max.rmse, label = paste0 ("R² = ",round(R_squared, 2))),
            vjust = -1, size = 6)+   # Ajoute le texte avec les valeurs R²
  labs(y= "RMSE",x ="Models",title = subtitle)+ 
  scale_y_continuous(limits = c(1.6, 2.4), breaks = seq(1.6, 2.4, by = 0.2))+
  theme_bw()+
  theme(line = element_blank(), 
        axis.line = element_line(colour = "black"),
        panel.border = element_blank(),
        axis.ticks =  element_line(colour = "black"),
        axis.text.x = element_text(colour = "black", size=20,
                                   angle = 45, hjust = 1),
        axis.text.y = element_text(colour = "black", size=20),
        legend.title = element_text(colour = "black", size=20),
        legend.title.align=0.5,
        legend.text = element_text(colour = "black", size=18),
        axis.title=element_text(size=28),
        strip.background = element_rect(fill="white"))

ggsave("Results/Result_richness.png", plot = result_Richesse_tot, dpi = 300)
result_Richesse_tot
```


<!--
## R² sur le JDD test

-   AB_tot
```{r R.plot AB_tot}
Results_long = AB_tot_R_Results [,-c(5:6)]%>%
  select(starts_with("R"))%>%
  pivot_longer(everything())

Results_long$name = as.factor(Results_long$name)
levels(Results_long$name) = c("GAM", "GBM", "GLM", "RF")
ordonee = c("GLM","GAM","RF","GBM")
Results_long$name <- factor(Results_long$name, levels = ordonee)
Results_long = as.data.frame(Results_long)
#Results_long = Results_long[!Results_long$name=="ANN",]
#Results_long$value = Results_long$value^2 # Pour retrouver les données de base

recap = Results_long%>%
  group_by(name)%>%
  summarise(mean.R = mean(value),
            sd.R = sd(value))
subtitle <- sprintf("Abundance : %.2f ± %.2f ind/m²", mean(bdd_explo$AB_tot, na.rm = TRUE), sd(bdd_explo$AB_tot, na.rm = TRUE))
R_result_AB_tot = ggplot()+
  geom_sina(data=Results_long, aes(name, value), alpha = 0.2,shape=19, cex = 2,
            method = "density")+
  geom_point(data = recap, aes(name, mean.R), cex =2, col = "orange")+
  geom_errorbar(data = recap, 
                aes(x= name, ymin=mean.R-sd.R, ymax=mean.R+sd.R),
                cex =1.2, width=.1, position = position_dodge(0.5), col = "orange")+
  labs(y= "R²",x ="Models",title = subtitle)+ 
  scale_y_continuous(limits = c(0.2, 0.5), breaks = seq(0.2, 0.5, by = 0.1))+
  theme_bw()+
  theme(line = element_blank(), 
        axis.line = element_line(colour = "black"),
        panel.border = element_blank(),
        axis.ticks =  element_line(colour = "black"),
        axis.text.x = element_text(colour = "black", size=20,
                                   angle = 45, hjust = 1),
        axis.text.y = element_text(colour = "black", size=20),
        legend.title = element_text(colour = "black", size=20),
        legend.title.align=0.5,
        legend.text = element_text(colour = "black", size=18),
        axis.title=element_text(size=28),
        strip.background = element_rect(fill="white"))

ggsave("Results/R_abundance.png", plot = R_result_AB_tot, dpi = 300)
R_result_AB_tot
```

-   BM_tot
```{r R.plot BM_tot}
Results_long = BM_tot_R_Results [,-c(5:6)]%>%
  select(starts_with("R"))%>%
  pivot_longer(everything())

Results_long$name = as.factor(Results_long$name)
levels(Results_long$name) = c("GAM", "GBM", "GLM", "RF")
ordonee = c("GLM","GAM","RF","GBM")
Results_long$name <- factor(Results_long$name, levels = ordonee)
Results_long = as.data.frame(Results_long)
#Results_long = Results_long[!Results_long$name=="ANN",]
#Results_long$value = Results_long$value^2 # Pour retrouver les données de base

recap = Results_long%>%
  group_by(name)%>%
  summarise(mean.R = mean(value),
            sd.R = sd(value))
subtitle <- sprintf("Biomass : %.2f ± %.2f g/m²", mean(bdd_explo$BM_tot, na.rm = TRUE), sd(bdd_explo$BM_tot, na.rm = TRUE))
R_result_BM_tot = ggplot()+
  geom_sina(data=Results_long, aes(name, value), alpha = 0.2,shape=19, cex = 2,
            method = "density")+
  geom_point(data = recap, aes(name, mean.R), cex =2, col = "orange")+
  geom_errorbar(data = recap, 
                aes(x= name, ymin=mean.R-sd.R, ymax=mean.R+sd.R),
                cex =1.2, width=.1, position = position_dodge(0.5), col = "orange")+
  labs(y= "R²",x ="Models",title = subtitle)+ 
  scale_y_continuous(limits = c(0, 0.5), breaks = seq(0, 0.5, by = 0.1))+
  theme_bw()+
  theme(line = element_blank(), 
        axis.line = element_line(colour = "black"),
        panel.border = element_blank(),
        axis.ticks =  element_line(colour = "black"),
        axis.text.x = element_text(colour = "black", size=20,
                                   angle = 45, hjust = 1),
        axis.text.y = element_text(colour = "black", size=20),
        legend.title = element_text(colour = "black", size=20),
        legend.title.align=0.5,
        legend.text = element_text(colour = "black", size=18),
        axis.title=element_text(size=28),
        strip.background = element_rect(fill="white"))

ggsave("Results/R_biomass.png", plot = R_result_BM_tot, dpi = 300)
R_result_BM_tot
```

-   Richesse_tot
```{r R.plot Richesse_tot}
Results_long = Richesse_tot_R_Results [,-c(5:6)] %>%
  select(starts_with("R"))%>%
  pivot_longer(everything())

Results_long$name = as.factor(Results_long$name)
levels(Results_long$name) = c("GAM", "GBM", "GLM", "RF")
ordonee = c("GLM","GAM","RF","GBM")
Results_long$name <- factor(Results_long$name, levels = ordonee)
Results_long = as.data.frame(Results_long)
#Results_long = Results_long[!Results_long$name=="ANN",]
#Results_long$value = Results_long$value^2 # Pour retrouver les données de base

recap = Results_long%>%
  group_by(name)%>%
  summarise(mean.R = mean(value),
            sd.R = sd(value))
subtitle <- sprintf("Richness : %.2f ± %.2f", mean(bdd_explo$Richesse_tot, na.rm = TRUE), sd(bdd_explo$Richesse_tot, na.rm = TRUE))

R_result_Richesse_tot = ggplot()+
  geom_sina(data=Results_long, aes(name, value), alpha = 0.2,shape=19, cex = 2,
            method = "density")+
  geom_point(data = recap, aes(name, mean.R), cex =2, col = "orange")+
  geom_errorbar(data = recap, 
                aes(x= name, ymin=mean.R-sd.R, ymax=mean.R+sd.R),
                cex =1.2, width=.1, position = position_dodge(0.5), col = "orange")+
  labs(y= "R²",x ="Models",title = subtitle)+ 
  scale_y_continuous(limits = c(0.2, 0.6), breaks = seq(0.2, 0.6, by = 0.1))+
  theme_bw()+
  theme(line = element_blank(), 
        axis.line = element_line(colour = "black"),
        panel.border = element_blank(),
        axis.ticks =  element_line(colour = "black"),
        axis.text.x = element_text(colour = "black", size=20,
                                   angle = 45, hjust = 1),
        axis.text.y = element_text(colour = "black", size=20),
        legend.title = element_text(colour = "black", size=20),
        legend.title.align=0.5,
        legend.text = element_text(colour = "black", size=18),
        axis.title=element_text(size=28),
        strip.background = element_rect(fill="white"))

ggsave("Results/R_richness.png", plot = R_result_Richesse_tot, dpi = 300)
R_result_Richesse_tot
```
-->

## Prediction
-   AB_tot

```{r predit AB_tot, fig.align='center'}
# Prediction avec RF
prediction <-predict(best20_var_AB_tot$model, df_test_AB_tot[,-1])

RF_df_AB_tot = data.frame(Observed=df_test_AB_tot[,1],Predicted = prediction)

  cor_RF_AB_tot <- cor(RF_df_AB_tot$Observed, RF_df_AB_tot$Predicted)
# graphique avec ggplot
    RF_AB_tot <- ggplot(RF_df_AB_tot, aes(x = Observed, y = Predicted)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = TRUE, color = "red") + 
      labs(subtitle =paste("RF: R² = ", round(cor_RF_AB_tot^2,2), "; RMSE = ", best20_var_AB_tot$RMSE),x = "Real values", y = "Predicted values") + 
      theme_classic() 
RF_AB_tot





# Prediction avec GBM
GBM_best20_var_AB_tot = GBM(var_rep ="AB_tot", df_app=df_train_AB_tot[,c("AB_tot",best_20_AB_tot)], df_valid = df_test_AB_tot[,c("AB_tot",best_20_AB_tot)],distribution = 'gaussian',n.trees = 6000,
shrinkage = 0.01,interaction.depth = 7,n.minobsinnode = 15)

prediction <-predict(GBM_best20_var_AB_tot$model, df_test_AB_tot[,-1])

GBM_df_AB_tot = data.frame(Observed=df_test_AB_tot[,1],Predicted = prediction)

   cor_GBM_AB_tot<- cor(GBM_df_AB_tot$Observed, GBM_df_AB_tot$Predicted)
# graphique avec ggplot
    GBM_AB_tot <- ggplot(GBM_df_AB_tot, aes(x = Observed, y = Predicted)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = TRUE, color = "red") + 
      labs(subtitle =paste("GBM: R² = ", round(cor_GBM_AB_tot^2,2) , "; RMSE = ", GBM_best20_var_AB_tot$RMSE),x = "Real values", y = "Predicted values") + 
      theme_classic() 
GBM_AB_tot





# ANN_best20_var_AB_tot = ANN(var_rep ="AB_tot" , df_app= ANN_df_train_AB_tot, df_valid = ANN_df_test_AB_tot)
# 
# # ANN_best20_var_AB_tot$RMSE
# # ANN_best20_var_AB_tot$R_squared
# 
# 
# ANN_model = ANN_best20_var_AB_tot$model
# plot(ANN_model, rep = 'best')
# 
# nn.results <- compute(ANN_model, ANN_df_test_AB_tot[,-1])
# ANN_df_AB_tot <- data.frame(Observed = ANN_df_test_AB_tot[,1], Predicted = nn.results$net.result)
# 
# 
#   correlation_AB_tot <- cor(ANN_df_AB_tot$Observed, ANN_df_AB_tot$Predicted)
# # graphique avec ggplot
#     ANN_AB_tot <- ggplot(ANN_df_AB_tot, aes(x = Observed, y = Predicted)) +
#       geom_point() + # Ajout des points
#       geom_smooth(method = "lm", se = TRUE, color = "red") + 
#       labs(subtitle =paste("ANN: R² = ", round(correlation_AB_tot^2,2) ),x = "Real values", y = "Predicted values") + 
#       theme_classic() 
# # ANN_AB_tot
# 
# 
#   
# # Predict on test data 
# ANN_data = data_mod
# ANN_data = ANN_data[,c("AB_tot",best_20_AB_tot)]
# colnames(ANN_data)[colnames(ANN_data) == "clcm_lvl3"] <- "clcm_lvl3"
# 
# dummy_vars <- model.matrix(~ clcm_lvl3 - 1, data = ANN_data)
# ANN_data <- cbind(ANN_data, dummy_vars)
# ANN_data <- ANN_data[, -which(names(ANN_data) == "clcm_lvl3")]
# 
# 
#  
#   
# # Compute mean squared error 
# predit <- nn.results$net.result * (max(ANN_df_train_AB_tot$AB_tot) - min(ANN_df_train_AB_tot$AB_tot))+ min(ANN_df_train_AB_tot$AB_tot) 

```


-   BM_tot
```{r predit BM_tot , fig.align='center'}
# Prediction avec RF
prediction <-predict(best20_var_BM_tot$model, df_test_BM_tot[,-1])

RF_df_BM_tot = data.frame(Observed=df_test_BM_tot[,1],Predicted = prediction)


  cor_RF_BM_tot <- cor(RF_df_BM_tot$Observed, RF_df_BM_tot$Predicted)
# graphique avec ggplot
    p_BM_tot <- ggplot(RF_df_BM_tot, aes(x = Observed, y = Predicted)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = TRUE, color = "red") + 
      labs(subtitle =paste("R² = ", round(cor_RF_BM_tot^2,2), "; RMSE = ", best20_var_BM_tot$RMSE),x = "Real values", y = "Predicted values") + 
      theme_classic() 



# Prediction avec GBM
GBM_best20_var_BM_tot = GBM(var_rep ="BM_tot", df_app=df_train_BM_tot[,c("BM_tot",best_20_BM_tot)], df_valid = df_test_BM_tot[,c("BM_tot",best_20_BM_tot)],distribution = 'gaussian',n.trees = 6000,
shrinkage = 0.01,interaction.depth = 7,n.minobsinnode = 15)


prediction <-predict(GBM_best20_var_BM_tot$model, df_test_BM_tot[,-1])

GBM_df_BM_tot = data.frame(Observed=df_test_BM_tot[,1],Predicted = prediction)

  cor_GBM_BM_tot <- cor(GBM_df_BM_tot$Observed, GBM_df_BM_tot$Predicted)
# graphique avec ggplot
    GBM_BM_tot <- ggplot(GBM_df_BM_tot, aes(x = Observed, y = Predicted)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = TRUE, color = "red") + 
      labs(subtitle =paste("GBM: R² = ", round(cor_GBM_BM_tot^2,2), "; RMSE = ", GBM_best20_var_BM_tot$RMSE),x = "Real values", y = "Predicted values") + 
      theme_classic()
    
p_BM_tot
GBM_BM_tot

```

-   Richesse_tot
```{r predit Richesse_tot , fig.align='center'}
# Étape 6) Évaluation du model modèle
prediction <-predict(best20_var_Richesse_tot$model, df_test_Richesse_tot[,-1])

RF_df_Richesse_tot = data.frame(Observed=df_test_Richesse_tot[,1],Predicted = prediction)


  cor_RF_Richesse_tot <- cor(RF_df_Richesse_tot$Observed, RF_df_Richesse_tot$Predicted)
# graphique avec ggplot
    p_Richesse_tot <- ggplot(RF_df_Richesse_tot, aes(x = Observed, y = Predicted)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = TRUE, color = "red") + 
      labs(subtitle =paste("RF: R² = ", round(cor_RF_Richesse_tot^2,2), "; RMSE = ", best20_var_Richesse_tot$RMSE),x = "Real values", y = "Predicted values") + 
      theme_classic() 




# Prediction avec GBM
GBM_best20_var_Richesse_tot = GBM(var_rep ="Richesse_tot", df_app=df_train_Richesse_tot[,c("Richesse_tot",best_20_Richesse_tot)], df_valid = df_test_Richesse_tot[,c("Richesse_tot",best_20_Richesse_tot)],distribution = 'gaussian',n.trees = 6000,
shrinkage = 0.01,interaction.depth = 7,n.minobsinnode = 15)


prediction <-predict(GBM_best20_var_Richesse_tot$model, df_test_Richesse_tot[,-1])

GBM_df_Richesse_tot = data.frame(Observed=df_test_Richesse_tot[,1],Predicted = prediction)

  cor_GBM_Richesse_tot <- cor(GBM_df_Richesse_tot$Observed, GBM_df_Richesse_tot$Predicted)
# graphique avec ggplot
    GBM_Richesse_tot <- ggplot(GBM_df_Richesse_tot, aes(x = Observed, y = Predicted)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = TRUE, color = "red") + 
      labs(subtitle =paste("GBM: R² = ", round(cor_GBM_Richesse_tot^2,2), "; RMSE = ", GBM_best20_var_Richesse_tot$RMSE),x = "Real values", y = "Predicted values") + 
      theme_classic() 
    
    
    
p_Richesse_tot   
GBM_Richesse_tot

```











# Questions





