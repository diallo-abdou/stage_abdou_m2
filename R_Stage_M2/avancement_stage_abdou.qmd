---
title: "Internship progress"
author: "Abdourahmane Diallo"
date: "2024-02-20"
format: 
  revealjs
#smaller: true
scrollable: true
#theme: sky
editor: visual
number-sections: true
toc: FALSE
#toc-expand: false
#toc_float: 'yes'
code_download: 'yes'
slide-number: true
margin: 0.1
#center: true
code-fold: true
width: 1300
height: 700
toc_depth: 1
execute: 
  cache: true
---

# Setting

```{r setup, include=FALSE,fig.align='center',message=FALSE,warning=FALSE,message=FALSE,echo=TRUE}
# rm(list=ls()) # Properly clear workspace
# source("function_abdou.R")

```

## Packages

```{r,echo=TRUE}
  library(tidyverse)
  library(glme)
  library(lsmeans)
  library(agricolae)
  library(RVAideMemoire)
  library(corrplot)
  library(emmeans)
  library(lme4)
  library(multcomp)
  library(MASS)
  library(R2WinBUGS)
  library(arm)
  library(performance)
  library(AER)
  library(AICcmodavg)
  library(MuMIn)
  library(ade4)
  library(Hmisc)
  library(labdsv)
  library(vegan)
  library(cowplot)
  library(ggpubr)
  library(rstatix)
  library(patchwork)
  library(multcompView)
  library(ggsignif)
  library(grid)
  library(FactoMineR)
  library(factoextra)
  library(explore)
  library(ggrepel)
  library(naniar)
  library(outliers)
  library(leaps)
  library(fastDummies)
  library(caret)
  library(mgcv)
  library(ggeffects)
  library(gratia)
  library(GGally)
  library(caTools)
  library(rpart)
  library(rpart.plot)
  library(openxlsx)
  library(readxl)
  library(leaflet)
  library(quarto)
  library(raster)
  library(knitr)
  library(kableExtra)
  library(stringr)
  library(plotly)
  library(PerformanceAnalytics)
  library(usdm)
  library(vcd) # pour la distribution des var reponse
  library(prospectr)# pour split data avec kenSton()
  library(glmnet)
  library(randomForest)
  library(doParallel)
  library(gbm)
  library(kernlab)
  library(e1071)
  library(ggforce)
  #library(keras)
  #library(tensorflow)
  library(neuralnet)
  library(parallel)



```

## Functions

```{r, echo=TRUE}

## Identification des NA dans un df -----------------------------------------------
taux_completion<-
  function(df, afficher_zero_percent = FALSE, seuil, trie=FALSE) {
    # Calcule du pourcentage de NA dans le dataframe
    pourcentage_total <-
      round(sum(is.na(df)) / (nrow(df) * ncol(df)) * 100, 1)
    
    # Calcule du pourcentage de NA par colonne
    pourcentage_colonnes <- round(colMeans(is.na(df)) * 100, 1)
    
    # Creation d'un dataframe résultat avec deux colonnes
    result <-
      data.frame(
        Variables = names(df),
        CR = pourcentage_colonnes,
        row.names = NULL
      )
    
    if (afficher_zero_percent) {
      result <- result[result$CR == 0, ]
      result$CR = 100 -result$CR
    } else {
      result <- result[result$CR > 0, ]
      result$CR = 100 -result$CR
      
    }
    
    result <- rbind(result, c("Total", pourcentage_total))
    #result <- rbind(result, c("Total", paste0(pourcentage_total, "")))
    
    result <- result[, c("Variables", "CR")]
    result$CR = as.numeric(result$CR)
    result$CR = round(result$CR,1)
    if (trie){
      result = result %>% arrange(desc(CR))
    }
    result$CR = paste0(result$CR,"%")
    
    return(result)
  }
# Converssion des colonne en num ou factor-----------------------------------------------
conv_col <- function (data, columns_to_convert, to_types) {
  if (to_types == "numeric") {
    # Conversion des colonnes en numeric
    for (col in columns_to_convert) {
      data[, col] <- as.numeric(data[, col])
    }
  } else {
    # Conversion des colonnes en facteurs
    for (col in columns_to_convert) {
      data[, col] <- as.factor(data[, col])
    }
  }
  return(data)
}
#data_converted <- conv_col(data, names(data [, c(1, 3)]), "factor")

# exploration graphiques des variables numeriques -----------------------------------------------
explo_num <- function(nom_col, titre, df = bdd, ligne_col = c(2, 2),mini = min(df[[nom_col]]), maxi=max(df[[nom_col]]) ) {
  par(mfrow = ligne_col)
  
  df[complete.cases(df[[nom_col]]), ]
  df <- df %>%filter(!is.na(df[[nom_col]]))
  df[[nom_col]] = as.numeric(df[[nom_col]])
  # Boxplot
  boxplot(df[[nom_col]], col = 'blue', ylab = titre, ylim = c(mini, maxi))
  # Cleveland plot
  dotchart(df[[nom_col]], pch = 16, col = 'blue', xlab = titre)
  # Histogram
  hist(df[[nom_col]], col = 'blue', xlab = titre, main = "")
  # Quantile-Quantile plot
  qqnorm(df[[nom_col]], pch = 16, col = 'blue', xlab = '')
  qqline(df[[nom_col]], col = 'red') 
}

# Extraction des predictors + moyennes -----------------------------------------------

extraction <- function(nom_col, tif_file_path, df = bdd, conv = 1) {
  #df <- df %>%filter(!is.na(gps_x) & !is.na(gps_y))
  raster_data <- raster(tif_file_path)
  
  # Création d'un dataframe pour stocker les valeurs extraites
  df_interne <- data.frame(gps_x = df$gps_x, gps_y = df$gps_y)
  proj4Str <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"
  # Transformer les coordonnées GPS en système de coordonnées du raster
  gps_coords_sp <- SpatialPoints(df_interne, proj4string = CRS(proj4Str))
  gps_coords_proj <- spTransform(gps_coords_sp, crs(raster_data))
  
  # Extraction des valeurs du raster 
  values <- raster::extract(raster_data, gps_coords_proj)
  
  # Ajout des valeurs extraites comme nouvelles colonnes a df
  #df_save = data.frame()
  #df_save[[nom_col]] <- values / conv
  
  df[[nom_col]] <- values / conv
  
  return(df)
}

# la moyenne des predictores -----------------------------------------------
moyenne_val_extrct <- function(nom_col, vec_col, df=bdd) {
  df[[nom_col]] <- rowMeans(as.matrix(df[, vec_col, drop = FALSE]), na.rm = TRUE)
  df[[nom_col]] = round(df[[nom_col]],1)
  return(as.data.frame(df))
}


# tests de corrélation avec un seuil -----------------------------------------------
cor_function_seuil <- function(data, seuil,affiche=FALSE) {
  # Création d'un vecteur pour stocker les paires de variables corrélées
  variables_corr <- c()
  
  # Boucle pour tester la corrélation entre chaque paire de variables
  for (i in 1:(ncol(data) - 1)) {
    for (j in (i + 1):ncol(data)) {
      # Calcul de la corrélation entre les variables i et j
      cor_value <- stats::cor(data[, i], data[, j], use = "na.or.complete")
      
      # Stockage du résultat dans le vecteur si supérieur au seuil
      if (cor_value >= seuil | cor_value <= -seuil) {
        if(affiche){
        cat(
          "***",
          colnames(data)[i],
          "  __est correlee a__  ",
          colnames(data)[j],
          "avec un R =",
          cor_value,
          "\n \n \n"
        )
      }
        
        variables_corr <-
          c(variables_corr, colnames(data)[i], colnames(data)[j])
      }
    }
  }
  
  return(variables_corr)
}
```

# Plan

-   Setting

-   Database import

-   Database exploration

-   Earthworms data

-   Soil data extraction

-   Climate data extraction

-   To do next

# Database import

-   Import of database **LandWorm_dataset_site_V1.9.xlsx** (february 22, 2024)

```{r,echo=FALSE}
chemin_fichier_excel = "C:/Users/diall/Downloads/datas/LandWorm_dataset_site_V1.9.xlsx"
bdd <- read.xlsx(chemin_fichier_excel, sheet = "Sheet1")
```

-   The database contains **`r nrow(bdd)`** rows and **`r ncol(bdd)`** columns

```{r,echo=FALSE}
col_en_factor = c("Programme","Annee","ID_Site","Code_Parcelle","postal_code","clcm_lvl1",
                  "clcm_lvl2","clcm_lvl3","Modalite","Bloc","Protocole","land_cover_detail","type_tillage","fertilisation","ferti_min_product","ferti_orga_product")
bdd = conv_col(bdd, col_en_factor, "factor")
```

## Data selection: EcoBioSoil

```{r,echo=FALSE}
n_line=nrow(bdd)
bdd$owner=as.factor(bdd$owner)
summary_df <- as.data.frame(summary(bdd$owner))
colnames(summary_df) <- c("Numbers")
kable(summary_df)
```

```{r,echo=FALSE}
bdd <- subset(bdd, owner == "dc")
bdd$owner=droplevels(bdd$owner)

```

-   The database therefore changes from **`r n_line`** to **`r nrow(bdd)`** observations.

# Database exploration

-   CR = Completion rate

## Complete columns

```{r, echo=TRUE}
df_col=taux_completion(bdd,TRUE,trie=FALSE)
df_col = df_col[df_col$Variables != "Total",]
#print("table")
kable(df_col, caption = "", col.width = c("75%", "25%"))
# cat(                                                    )
# head(bdd[, "ID"])
```

## Non-complete columns

```{r, scrollable = TRUE}
df_col= taux_completion(bdd,FALSE,trie = TRUE)
df_col = df_col[df_col$Variables != "Total",]
kable(df_col, caption = " ", col.width = c("75%", "25%"))
```

## Focus on GPS coordinates

-   There is **`r sum(is.na(bdd$gps_x))`** NA (CR = `r df_col[df_col$Variable=="gps_x", "CR"]`) in **GPS_X**
-   There is **`r sum(is.na(bdd$gps_y))`** NA (CR = `r df_col[df_col$Variable=="gps_y", "CR"]`) in **GPS_Y**

```{r,echo=TRUE}
n_line= nrow(bdd)
bdd$gps_x <- as.numeric(gsub("[^0-9.-]", "", bdd$gps_x))
bdd$gps_y <- as.numeric(gsub("[^0-9.-]", "", bdd$gps_y))
bdd <- bdd[complete.cases(bdd$gps_x, bdd$gps_y), ]
bdd <- bdd %>%filter(!is.na(gps_x) & !is.na(gps_y))
#sum(is.na(bdd$gps_x))
#sum(is.na(bdd$gps_y))
```

-   We delete the *NA* lines in the GPS coordinates
-   The database therefore changes from **`r n_line`** to **`r nrow(bdd)`** observations.
-   Merging database and climat database

```{r, echo=TRUE}
# Ajout variables climatiques (voir chunk extraction données climatiques)
chemin_fichier <- "C:/Users/diall/OneDrive/Bureau/M2_MODE/stage_abdou_m2/datas/bdd_climat_ok.rds"
# saveRDS(bdd_climat_ok, chemin_fichier)
bdd_climat_ok <- readRDS(chemin_fichier)
df_fusion <- subset(bdd_climat_ok, select = -c(gps_x, gps_y))

rows_not_in_df_fusion <- anti_join(bdd, df_fusion, by = "ID")
merged_df <- merge(bdd, df_fusion, by = "ID")

ids_not_matching <- anti_join( merged_df,bdd, by = "ID")

bdd = merged_df

#bdd <- cbind(bdd, df_fusion) # all = TRUE pour garder toutes les lignes
```

## Cartography

```{r,echo=TRUE}
n_ligne= nrow(bdd)
df_coord <- bdd[, c("gps_x", "gps_y")] %>% mutate(gps_x = as.numeric(gps_x),gps_y = as.numeric(gps_y))

df_coord$num_ligne <- seq(nrow(df_coord))
carte <- leaflet(df_coord) %>%
  addTiles() %>%
  addCircleMarkers(lng = ~gps_x, lat = ~gps_y, radius = 0.8, fillOpacity = 0.8, fillColor = "blue")
carte
```

```{r}
hors_france= read.csv(file = "C:/Users/diall/Downloads/datas/hors_france.csv", header = TRUE)

bdd <- bdd[!(bdd$gps_x %in% hors_france$gps_x & bdd$gps_y %in% hors_france$gps_y), ]
bdd <- droplevels(bdd)
```

-   We delete points outside France (**`r nrow(hors_france)`**)
-   The database therefore changes from **`r n_ligne`** to **`r nrow(bdd)`** observations.

## Focus on years

-   Cleaning the Annee column 
<br/> 
<!--
```{r , echo=TRUE}
# levels(bdd$Annee) # parfois années et jours et ou mois
# bdd$Annee= as.factor(bdd$Annee)
# bdd$Annee <- gsub("^(\\d{4}).*$", "\\1", bdd$Annee) # on prend uniquement les 04 premier chiffre
# bdd$Annee= as.factor(bdd$Annee)

```
-->

-   CR of Annee = **`r df_col[df_col$Variable=="Annee", "CR"]`** (`r length(levels(bdd$Annee))` levels)

```{r, echo=TRUE, scrollable = TRUE}
bdd$Annee= as.factor(bdd$Annee)
summary_df <- as.data.frame(summary(bdd$Annee))
colnames(summary_df) <- c("Numbers")
kable(summary_df)
```


<!-- 
-   We remove all the years before **1990** and the NA 

```{r, echo=TRUE, scrollable = TRUE}
n_ligne =nrow(bdd)
bdd <- bdd %>%filter(!is.na(Annee))# on enleve les NA
annes_omit= c("1821", "1960", "1978", "1982", "1983", "1984", "1986", "1988", "1989") # annee sup
bdd <- bdd[!bdd$Annee %in% annes_omit, ]
bdd=droplevels(bdd)
#levels (bdd$Annee)
#summary (bdd$Annee)
summary_df <- as.data.frame(summary(bdd$Annee))
colnames(summary_df) <- c("Numbers")
kable(summary_df,padding = 5)
```

-   The database therefore changes from **`r n_ligne`** to **`r nrow(bdd)`** observations. 
-->



<!--
## Table land use & protocol ( start )

-   clcm_lvl1 & protocol

```{r, echo=TRUE}
kable(table(bdd$clcm_lvl1, bdd$Protocole,exclude = NULL),padding = 10,align = "c")

```

\n\n\n

-   clcm_lvl2 & protocol

```{r, echo=TRUE}
kable(table(bdd$clcm_lvl2, bdd$Protocole,exclude = NULL),padding = 10,align = "c")
```

\n\n\n

-   clcm_lvl3 & protocol

```{r, echo=TRUE}
kable(table(bdd$clcm_lvl3, bdd$Protocole,exclude = NULL),padding = 0,align = "c")
```
-->


## Focus on protocols

-   List of protocols available on the database ( `r length(levels(bdd$Protocole))` levels)

```{r,echo=TRUE}
bdd$Protocole = as.factor(bdd$Protocole)
summary_df <- as.data.frame(summary(bdd$Protocole))
colnames(summary_df) <- c("Numbers")
kable(summary_df,padding = 5)
```


<!--
-   Selection of protocols: **F_HS, FHS, hand sorting, HS**
-->

-   Selection of protocols: **F_HS, HS**

```{r,echo=TRUE}
n_ligne = nrow(bdd)
#select_protocole =c("F_HS", "FHS", "hand sorting" ,"HS")
select_protocole =c("F_HS", "HS")
bdd <- bdd[bdd$Protocole %in% select_protocole, ]
bdd=droplevels(bdd)
bdd$Protocole = as.factor(bdd$Protocole)
summary_df <- as.data.frame(summary(bdd$Protocole))
colnames(summary_df) <- c("Numbers")
kable(summary_df,padding = 5)
```

-   The database therefore changes from **`r n_ligne`** to **`r nrow(bdd)`** observations.


<!--
-   Merging levels :

    -   F_HS $=$ F_HS $+$ FHS
    -   HS $=$ HS $+$ hand sorting

    ```{r,echo=TRUE}
    levels(bdd$Protocole)[levels(bdd$Protocole) == "FHS"] <- "F_HS"
    levels(bdd$Protocole)[levels(bdd$Protocole) == "hand sorting"] <- "HS"
    bdd$Protocole = as.factor(bdd$Protocole)
    summary_df <- as.data.frame(summary(bdd$Protocole))
    colnames(summary_df) <- c("Numbers")
    kable(summary_df,padding = 5)
    ```
-->

## Focus on clcm_lvl1

-   CR of clcm_lvl1 = **`r df_col[df_col$Variable=="clcm_lvl1","CR"]`** (`r length(levels(bdd$clcm_lvl1))` levels)

```{r, echo=TRUE}
bdd$clcm_lvl1= as.factor(bdd$clcm_lvl1)
summary_df <- as.data.frame(summary(bdd$clcm_lvl1))
colnames(summary_df) <- c("Numbers")
# kable(summary_df,padding = 5)
```

-   Merging levels

```{r, echo=TRUE}
levels(bdd$clcm_lvl1)[levels(bdd$clcm_lvl1) == "1_Naturel"] <- "Forest and semi natural areas"
levels(bdd$clcm_lvl1)[levels(bdd$clcm_lvl1) == "2_Agricole"] <- "Agricultural areas"

bdd$clcm_lvl1= as.factor(bdd$clcm_lvl1)
summary_df <- as.data.frame(summary(bdd$clcm_lvl1))
colnames(summary_df) <- c("Numbers")
kable(summary_df,padding = 5)
```

-   Update **code_clcm_lvl1**

```{r, echo=TRUE}
#bdd$code_clcm_lvl1 = as.factor(bdd$code_clcm_lvl1)

bdd$code_clcm_lvl1 <- ifelse(bdd$clcm_lvl1 == "Forest and semi natural areas", 3, bdd$code_clcm_lvl1)

bdd$code_clcm_lvl1 <- ifelse(bdd$clcm_lvl1 == "Agricultural areas", 2, bdd$code_clcm_lvl1)
```

-   For the moment, we will keep the NA of **clcm_lvl1**

## Focus on clcm_lvl2

-   CR of clcm_lvl2 = **`r df_col[df_col$Variable=="clcm_lvl2","CR"]`** (`r length(levels(bdd$clcm_lvl2))` levels)

```{r, echo=TRUE}
bdd$clcm_lvl2= as.factor(bdd$clcm_lvl2)
summary_df <- as.data.frame(summary(bdd$clcm_lvl2))
colnames(summary_df) <- c("Numbers")
# kable(summary_df,padding = 8)
```

-   Merging levels

```{r, echo=TRUE}
levels(bdd$clcm_lvl2)[levels(bdd$clcm_lvl2) == "21_Agricole ouvert"] <- "Arable land"

bdd$clcm_lvl2= as.factor(bdd$clcm_lvl2)
summary_df <- as.data.frame(summary(bdd$clcm_lvl2))
colnames(summary_df) <- c("Numbers")
kable(summary_df,padding = 5)
```

-   Update **code_clcm_lvl2**

```{r, echo=TRUE}

bdd$code_clcm_lvl2 <- ifelse(bdd$clcm_lvl2 == "Arable land", 21, bdd$code_clcm_lvl2)

```

## Focus on clcm_lvl3

-   CR of clcm_lvl3 = **`r df_col[df_col$Variable=="clcm_lvl3","CR"]`** (`r length(levels(bdd$clcm_lvl3))` levels)

```{r, echo=TRUE, scrollable = TRUE}
bdd$clcm_lvl3= as.factor(bdd$clcm_lvl3)
summary_df <- as.data.frame(summary(bdd$clcm_lvl3))
colnames(summary_df) <- c("Numbers")
kable(summary_df,padding = 5)

```

## Land use selection (clcm_lvl3)


<!--
-   **Broad-leaved forest** 
-   **Coniferous forest** 
-   **Mixed forest** 

-   **Pastures, meadows and other permanent grasslands under agricultural use** 

-   **Non-irrigated arable land** 

-   **Vineyards**

-   **Green urban areas**

-   **Natural grasslands**
-->

```{r, echo=TRUE}
select_os= c("Broad-leaved forest", "Coniferous forest", "Mixed forest", 
"Pastures, meadows and other permanent grasslands under agricultural use", "Non-irrigated arable land", 
"Vineyards","Green urban areas","Natural grasslands")

bdd <- bdd[bdd$clcm_lvl3 %in% select_os, ]
bdd=droplevels(bdd)
bdd$clcm_lvl3 = as.factor(bdd$clcm_lvl3)
summary_df <- as.data.frame(summary(bdd$clcm_lvl3))
colnames(summary_df) <- c("Numbers")
kable(summary_df)

```

-   **Maybe, we can merge the three types of forest ?**

## Land use & protocol overview

```{r, echo=TRUE}
# kable (table(bdd$clcm_lvl1, bdd$Protocole,exclude = NULL), align = "c", format = "pipe", padding = 10)
# kable (table(bdd$clcm_lvl2, bdd$Protocole,exclude = NULL), align = "c", format = "pipe", padding = 10)
kable (table(bdd$clcm_lvl3, bdd$Protocole,exclude = NULL), align = "c", format = "pipe", padding = 10)
```


<!--
# Earthworms data

## Total abundance (CR = 100 % )

```{r,fig.align='center',fig.height=10}
summary(bdd$AB_tot) 
#bdd <- subset(bdd, AB_tot <= 800)
explo_num(nom_col = "AB_tot", titre = "Total abundance")
```

## Total biomass (CR = `r df_col[df_col$Variable=="BM_tot","CR"]`)

```{r,fig.align='center',fig.height=10}
summary(bdd$BM_tot) 
#bdd <- subset(bdd, BM_tot <= 3000)
explo_num(nom_col = "BM_tot", titre = "Total biomass",)
```


-->



<!--
## Total richness calculation method

-   Removal of columns with only NA (**`r length(colnames(bdd)[colSums(is.na(bdd)) == nrow(bdd)])`**) and/or only 0
-   Identify columns beginning with **AB\_**
-   Deletion of **AB\_** columns that are not species
-   Calculate richness by assigning **1** to each column if the value is different from 0 and NA
-   Total richness = **1** if the plot has a value in AB and/or BM
-->

```{r, echo=FALSE,"Total richness calculation method"}
# on supprime tout les colonnes ayant que des NA
colonnes_na <- colnames(bdd)[colSums(is.na(bdd)) == nrow(bdd)]
# summary(bdd[, colonnes_na])
bdd <- bdd[, !colnames(bdd) %in% colonnes_na]



# On supprimme toutes les colonnes ayant que des NA et des 0
colonnes_numeriques <- sapply(bdd, is.numeric)
somme_colonnes_numeriques <- colSums(bdd[, colonnes_numeriques],na.rm=TRUE)
colonnes_zeros <- names(somme_colonnes_numeriques[somme_colonnes_numeriques == 0])
#summary(bdd[, colonnes_zeros])
bdd <- bdd[, !colnames(bdd) %in% colonnes_zeros]



# On récupère toutes les colonnes qui commencent par **AB_**
colonnes_AB <- grep("^AB_", names(bdd), value = TRUE)



# On supprimme les colonnes AB_ qui ne sont pas des espèces dans le calcule
ab_supprimee =  c("AB_AD","AB_JV","AB_SA","AB_STAD_X","AB_indéterminable","AB_Indéterminable","AB_indéterminable_endogeic","AB_tot","AB_Indéterminable_epigeic","AB_indéterminable_endogeic","AB_Ep.X","AB_vide", "AB_Ep.X1","AB_Ep.X2","AB_A.X","AB_Adult","AB_cocon","AB_indéterminé","AB_Juvenile","AB_Sub.adult","AB_Indéterminé","AB_Lumbricidae")
colonnes_AB <- colonnes_AB[!colonnes_AB %in% ab_supprimee]



# On calcule la richesse en attribiant 1 à chaque colonne si la valeur est différent de 0 et de NA
bdd$Richesse_tot <- 0
bdd$Richesse_tot <- rowSums(!is.na(bdd[colonnes_AB]) & bdd[colonnes_AB] != 0)
#sum (is.na(bdd$Richesse_tot) )
#summary(bdd$Richesse_tot)



# Check des lignes ayant des 0 richesse et X AB ou BM : 0 lignes
# vdt_a_checker = bdd[bdd$Richesse_tot == 0 & (bdd$Total_AB !=0 | bdd$BM_to !=0), c("ID_Site","AB_tot","BM_tot","Richesse_tot")]
# vdt_a_checker = subset(vdt_a_checker, Richesse_tot==0)
# View(vdt_a_checker)
# vdt_a_checker$Richesse_tot <- 1
# Mettre à jour les ligne correspondant dans la bdd 
# bdd[rownames(bdd) %in% rownames(vdt_a_checker), "Richesse_tot"] <- 1



# Check si y a des ligne ayant que des NA dans AB, BM et Richesse : nop
resultat <- subset(bdd, is.na(AB_tot) & is.na(BM_tot) & is.na(Richesse_tot))
# View(resultat[, c("AB_tot","BM_tot", "Richesse_tot")])



# Check si y a des ligne ayant que des zéros ou des NA dans AB, BM et Richesse_tot: 66 ligne
vdt <- c("AB_tot", "BM_tot", "Richesse_tot")
lignes_zero <- which(rowSums(bdd[vdt] != 0, na.rm = TRUE) == 0)
# View(bdd[lignes_zero,c("ID_Site","AB_tot", "BM_tot", "Richesse_tot")])



# Check des lignes ayant de BM mais pas de AB
bm_sans_ab <- subset(bdd, AB_tot == 0 & BM_tot != 0)
# bm_sans_ab[, c("ID","ID_Site", "Programme", "Protocole", "AB_tot", "BM_tot")]

ab_sans_bm <- subset(bdd, BM_tot == 0 & AB_tot != 0) # 1 parcelles
# ab_sans_bm[, c("ID","ID_Site", "Programme", "Protocole", "AB_tot", "BM_tot")]


# Check des doublons

#duplicated_rows <- subset(bdd, duplicated(bdd[, c("ID", "AB_tot", "BM_tot")]) | #duplicated(bdd[, c("ID", "AB_tot", "BM_tot")], fromLast = TRUE))

```


<!--
## Total richness (CR = 100 % )

```{r, fig.align='center',fig.height=10}
summary(bdd$Richesse_tot)
#bdd <- subset(bdd, Richesse_tot <= 2000)
explo_num(nom_col = "Richesse_tot", titre = "Total richness",)
```
-->



<!--
# Synthèse du taux de remplissage

## Complete columns

```{r, echo=TRUE}
df_col=taux_completion(bdd,TRUE,trie=FALSE)
df_col = df_col[df_col$Variables != "Total",]
kable(df_col, caption = " ", col.width = c("75%", "25%"))
```

## Non-complete columns

```{r, scrollable = TRUE}
df_col= taux_completion(bdd,FALSE,trie = TRUE)
df_col = df_col[df_col$Variables != "Total",]
kable(df_col, caption = " ", col.width = c("75%", "25%"))
```

-->



<!--
# Climate data extraction
## The source database ([CHELSA V2](https://chelsa-climate.org/bioclim/){target="_blank"})

```{r,echo=TRUE}

# Lire le fichier Excel
chemin_fichier_excel <- "C:/Users/diall/Downloads/datas/ODMAP.xlsx"
climat <- read.xlsx(chemin_fichier_excel, sheet = "climat")

# Fusions des cellules des colonnes avec des éléments dupliqués
for (col in names(climat)) {
  climat[[col]] <- ifelse(duplicated(climat[[col]]), "", climat[[col]])
}

# Affichage du tableau avec kableExtra et centrage du contenu des cellules
kableExtra::kable(climat) %>%
  kableExtra::kable_styling() %>%
  kableExtra::column_spec(1:ncol(climat)) 

```

## Extraction method

-   Link recovery ( see file [link .tif](https://1drv.ms/t/s!Avfm81EzNGBHjIZWw8YePljXaGSpCQ?e=qIPeWR){target="_blank"} )

-   Extracting variable names

-   Uses of the **extraction()** function

-   Convert columns to correct format and unit

-   Adding variables to the LANDWORM database

```{r,echo=TRUE}
liens_tif = read.table(file = "C:/Users/diall/Downloads/datas/envidatS3paths.txt")
liens_tif$shortname <- str_extract(liens_tif$V1, "(?<=CHELSA_).*?(?=_1981)")
liens_tif[liens_tif$shortname=="rsds","shortname"]=c("rsds_max","rsds_mean","rsds_min","rsds_range")

#all(is.na(bdd$gps_x))
#all(is.na(bdd$gps_y))

bdd_climat= bdd[, c("ID","gps_x","gps_y")]

temp_1=Sys.time()
#for( i in 1:nrow(liens_tif)){
  #nom=liens_tif[i,c("shortname")]
  #df_ext <- extraction(nom_col = nom,df = bdd_climat,conv = 1, 
                  #tif_file_path = liens_tif[i,c("V1")] ) 
  #bdd_climat[[nom]] <- df_ext [,nom]
  #rm("df_ext","nom")
  #cat("Extraction: ",i,"/",nrow(liens_tif), "\n")
#}
temp_2=Sys.time()
duree= difftime(temp_2,temp_1)

chemin_fichier <- "C:/Users/diall/OneDrive/Bureau/M2_MODE/stage_abdou_m2/datas/bdd_climat.rds"
# saveRDS(bdd_climat, chemin_fichier)
#bdd_climat <- readRDS(chemin_fichier)

# debut cnversion ------------------------------------------------------------
conv_df_climat= data.frame(shortname =liens_tif$shortname )

# unit = 1
conv_df_climat$unit = rep(1)
# unit = 100
unit_100=c("bio4")
conv_df_climat$unit <- ifelse(conv_df_climat$shortname %in% unit_100, 100, 1)


# scale = 0.1
conv_df_climat$scale = rep(0.1)
# scale = 1
scale_1=c("fcf","fgd","gddlgd0","gddlgd5","gddlgd10","gdgfgd0","gdgfgd5","gdgfgd10","gsl","kg0","kg1" ,"kg2" ,"kg3" ,"kg4" ,"kg5","lgd","ngd0","ngd5","ngd10","scd")

# scale = 0.01
scale_01=c("hurs_max","hurs_mean","hurs_min","hurs_range","pet_penman_max",
       "pet_penman_mean","pet_penman_min","pet_penman_range")

# scale = 0.001
scale_001=c("rsds","sfcWind_max","sfcWind_mean","sfcWind_min","sfcWind_range","pet_penman_max","pet_penman_mean","pet_penman_min","pet_penman_range","rsds_max","rsds_mean","rsds_min","rsds_range")

# Remplacement des valeurs de l'échelle en fonction des conditions
conv_df_climat$scale <- ifelse(conv_df_climat$shortname %in% scale_1, 1,
              ifelse(conv_df_climat$shortname %in% scale_01, 0.01,
                    ifelse(conv_df_climat$shortname %in% scale_001,0.001, 0.1)))

# offset = 0
conv_df_climat$offset = rep(0)
# offset = - 273.15
offset_273=c("bio1","bio5","bio6","bio8","bio9","bio10","bio11","gdgfgd10","gsl","gst")
conv_df_climat$offset = ifelse(conv_df_climat$shortname %in% offset_273, -273.15, 0)

# Pas present dans dans le pdf explicative donc pas de conversion
pas_pdf=c( "ai","swb", "clt_max","clt_mean","clt_min","clt_range")
verif=c(unit_100,scale_1,scale_01,scale_001,offset_273)
pas_pdf_2=setdiff(conv_df_climat$shortname, verif)
conv_df_climat[conv_df_climat$shortname %in% pas_pdf,"scale"] = 1

#bdd_climat_ok=bdd_climat[,c("ID","gps_x","gps_y")]

#for ( i in conv_df_climat$shortname){
  #if (i %in% names(bdd_climat)){
  #unitee= conv_df_climat[conv_df_climat$shortname ==i,"unit"]
  #echelle = conv_df_climat[conv_df_climat$shortname ==i,"scale"]
  #decalage = conv_df_climat[conv_df_climat$shortname ==i,"offset"]
  #bdd_climat_ok[[i]] = ((bdd_climat[[i]] / unitee)* echelle) + decalage
  #}else {
    #cat("Attention ",i, "n'exite pas dans la bdd_climat","\n")
  #}
#}


# chemin_fichier <- "C:/Users/diall/OneDrive/Bureau/M2_MODE/stage_abdou_m2/datas/bdd_climat_ok.rds"
# saveRDS(bdd_climat_ok, chemin_fichier)
# bdd_climat_ok <- readRDS(chemin_fichier)
# fin conversion

#df_fusion <- subset(bdd_climat_ok, select = -c(ID,gps_x, gps_y))
#bdd <- cbind(bdd, df_fusion) # all = TRUE pour garder toutes les lignes
```

## List of variables

[Variable description](https://chelsa-climate.org/wp-admin/download-page/CHELSA_tech_specification_V2.pdf){target="_blank"}

```{r}
summary(bdd_climat_ok)
```

## Temperature

-   Average annual air temperature (°C) = bio1

```{r,fig.align='center',fig.height=8}
summary(bdd$bio1)
explo_num(nom_col = "bio1", titre = "temp°.")
```

## Precipitation

-   Annual precipitation (kg/m²) = bio12

```{r,fig.align='center',fig.height=8}
summary(bdd$bio12)
explo_num(nom_col = "bio12", titre = "Précipitat°.")
```




-->



<!--
# Questions

```{r}
ID_Site_dupliques <- bdd$ID_Site[duplicated(bdd$ID_Site)]
#length(ID_Site_dupliques)

lignes_dupliquees <- subset(bdd, duplicated(ID_Site))

lignes_unique <- unique(lignes_dupliquees$ID_Site)
#length(lignes_unique)

# nrow(bdd) - length(ID_Site_dupliques) + length(lignes_unique)
```

-   Comment gérer la répétition temporelle des parcelles ?
    -   Avec répétition : **`r nrow(bdd)`** observations
    -   Sans répétition : **`r nrow(bdd) - length(ID_Site_dupliques) + length(lignes_unique)`** observations
-   Liens des données du sol (sable, argile et limon) de data.gouv.fr ?


# To do next

-   

    1.  Extraction des prédicteurs : CEC, Limon, argile, évapotranspiration, paysage,...

-   

    2.  Analyse exploratoire : test de corrélation, VIF, ACP

-   

    3.  Sélection des variables

-   

    4.  Modélisation (GLM, GAM, RF, ANN)

-   

    5.  Validation croisée

-   

    6.  Rédaction, protocol ODMAP



-->


# Soil data extraction

```{r}
# Calcul des distances euclidiennes entre les sites
distances <- dist(cbind(bdd$gps_x, bdd$gps_y))
distance_moyenne <- mean(distances)
# distance_moyenne

df_col= taux_completion(bdd,FALSE,trie = TRUE)
df_col = df_col[df_col$Variables != "Total",]
```

<!--
## The source database ([openlandmap](https://openlandmap.org/?center=25,39&zoom=4&opacity=72&base=OpenStreetMap&layer=lc_glc.fcs30d&time=2022){target="_blank"})

```{r,echo=TRUE}
chemin_fichier_excel <- "C:/Users/diall/Downloads/datas/ODMAP.xlsx"
pedo <- read.xlsx(chemin_fichier_excel, sheet = "pedo")

# Fusion des cellules des colonnes avec des éléments dupliqués
for (col in names(pedo)) {
  pedo[[col]] <- ifelse(duplicated(pedo[[col]]), "", pedo[[col]])
}

#tableau avec kableExtra et centrage du contenu des cellules
kableExtra::kable(pedo) %>%
  kableExtra::kable_styling() %>%
  kableExtra::column_spec(1:ncol(pedo))  # Centrer le contenu de toutes les colonnes
```

\n
-   Average values between surface (0 cm) and 30 cm depth




## Changing the resolution ![](https://logowik.com/content/uploads/images/python.jpg){width="200"}

-   Long compilation time in R

-   GDAL module with the resampleAlg = bilinear method

-   Resolution = 0.0083 = 30 arc-second \~ 1km

    ```{r}
    test_resolution = bdd
    tif_file_path_origine = "C:/Users/diall/Downloads/datas/raster_origine/sol_ph.h2o_usda.4c1a2a_m_250m_b10..10cm_1950..2017_v0.2.tif"
    raster_ph_origine <- raster(tif_file_path_origine)
    test_resolution <- extraction(nom_col = "ph_10_origine",df = test_resolution,conv = 10, 
                      tif_file_path = tif_file_path_origine)


    tif_file_path_rech = "C:/Users/diall/Downloads/datas/raster_modif/sol_ph.h2o_usda.4c1a2a_m_250m_b10..10cm_1950..2017_v0.2.tif"
    raster_ph_rech <- raster(tif_file_path_rech)
    test_resolution <- extraction(nom_col = "ph_10_rech",df = test_resolution,conv = 10, 
                      tif_file_path = tif_file_path_rech)

    par(mforw=c(1,2))
    image(raster_ph_origine,main="pH at 10cm: original raster (0.002)")
    image(raster_ph_rech, main = "pH at 10cm: raster modify (0.008)")


    bdd_echan = test_resolution
    bdd_echan <- bdd_echan %>%filter(!is.na(ph_10_origine) & !is.na(ph_10_rech))

    # graphique avec ggplot
        # coefficient de corrélation
    correlation <- cor(as.numeric(bdd_echan$ph_10_origine), bdd_echan$ph_10_rech)
    p <- ggplot(bdd_echan, aes(x = ph_10_origine, y = ph_10_rech)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = FALSE, color = "red") + 
      labs(subtitle = paste("R = ", round(correlation, 2)),
           x = "Original pH", y = "Resampled pH") + 
      theme_classic() 

    p

    ```



-->

## Soil organic carbone (g/kg)

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = "c_orga_0",df = bdd,conv = 5, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_organic.carbon_usda.6a1c_m_250m_b0..0cm_1950..2017_v0.2.tif")
bdd <- extraction(nom_col = "c_orga_10",df = bdd,conv = 5, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_organic.carbon_usda.6a1c_m_250m_b10..10cm_1950..2017_v0.2.tif")
bdd <- extraction(nom_col = "c_orga_30",df = bdd,conv = 5, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_organic.carbon_usda.6a1c_m_250m_b30..30cm_1950..2017_v0.2.tif")
bdd = moyenne_val_extrct(nom_col = "c_orga_0_a_30", vec_col = c("c_orga_0","c_orga_10","c_orga_30"),df=bdd)


indice_max <- which.max(bdd$c_orga_0_a_30)
bdd <- bdd[-indice_max, ]


summary(bdd$c_orga_0_a_30)
explo_num(nom_col = "c_orga_0_a_30", titre = "C. organic")
```

## pH

**Extracted values**

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = "ph_0",df = bdd,conv = 10, 
                  tif_file_path ="C:/Users/diall/Downloads/datas/raster_modif/sol_ph.h2o_usda.4c1a2a_m_250m_b10..10cm_1950..2017_v0.2.tif")

bdd <- extraction(nom_col = "ph_10" ,df = bdd,conv = 10, 
                  tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_ph.h2o_usda.4c1a2a_m_250m_b10..10cm_1950..2017_v0.2.tif")
bdd <- extraction(nom_col = "ph_30" ,df = bdd,conv = 10, 
                  tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_ph.h2o_usda.4c1a2a_m_250m_b30..30cm_1950..2017_v0.2.tif")
bdd = moyenne_val_extrct(nom_col = "ph_0_a_30", vec_col = c("ph_0","ph_10","ph_30"),df=bdd)
# summary(bdd$ph_0_a_30)

explo_num(nom_col = "ph_0_a_30", titre = "pH (0 - 30 cm)")
```

**Measured values & extracted values**

-   Clean pH column

```{r,echo=TRUE}
# On recupere les deux colonnes du pH
df_comp=bdd[, c("ID", "ID_Site","ph_eau","ph_0_a_30" )]
df_comp =df_comp[complete.cases(df_comp$ph_eau),] 
df_comp =df_comp[complete.cases(df_comp$ph_0_a_30),] 
df_comp <- df_comp[!grepl("[^0-9.]", df_comp$ph_eau), ]
df_comp$ph_eau <- as.numeric(df_comp$ph_eau)
df_comp$ph_0_a_30 <- as.numeric(df_comp$ph_0_a_30)


df_comp = df_comp[!df_comp$ph_eau== 44140.00,]
df_comp = df_comp[!df_comp$ph_eau== "NA",]
df_comp = df_comp[!df_comp$ph_0_a_30== "NA",]
df_comp = droplevels(df_comp)
```

```{r,echo=TRUE}
ID_Site_dupliques <- df_comp$ID_Site[duplicated(df_comp$ID_Site)]
#length(ID_Site_dupliques)

lignes_dupliquees <- subset(df_comp, duplicated(ID_Site) & duplicated(ph_eau))

lignes_unique <- unique(lignes_dupliquees$ID_Site )
#length(lignes_unique)

# nrow(df_comp) - length(ID_Site_dupliques) + length(lignes_unique)


dupliquees <- duplicated(df_comp$ID_Site)
df_comp <- df_comp[!dupliquees, ]
df_comp=droplevels(df_comp)

# correlation <- cor.test(df_comp$ph_eau, df_comp$ph_0_a_30,method = "pearson")
#resultat_test <- t.test(df_comp$ph_eau, df_comp$ph_0_a_30)

df_comp$ph_eau <- as.numeric(df_comp$ph_eau)
df_comp$ph_0_a_30 <- as.numeric(df_comp$ph_0_a_30)

```

<!-- <br/>  -->

::: columns
::: {.column width="60%"}
-   Method ?

-   Depth ?

-   Measured values (CR = `r df_col[df_col$Variable=="ph_eau","CR"]`)

```{r}
  summary(df_comp$ph_eau)
```

-   Extracted values

```{r}
  summary(df_comp$ph_0_a_30)
```
:::

::: {.column width="40%"}
```{r,fig.align='center',fig.height=5,fig.width=4}
    correlation <- cor(as.numeric(df_comp$ph_eau), df_comp$ph_0_a_30)
# graphique avec ggplot
    p <- ggplot(df_comp, aes(x = ph_eau, y = ph_0_a_30)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = FALSE, color = "red") + 
      labs(subtitle =paste("R = ", round(correlation, 2)),x = "pH measured values", y = "pH extracted values") + 
      theme_classic() 
p
```
:::
:::

<!--
## Bulk density (kg / m-cube)

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = "d_ap_0",df = bdd,conv = 10, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_bulkdens.fineearth_usda.4a1h_m_250m_b0..0cm_1950..2017_v0.2.tif")
bdd <- extraction(nom_col = "d_ap_10",df = bdd,conv = 10, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_bulkdens.fineearth_usda.4a1h_m_250m_b10..10cm_1950..2017_v0.2.tif")
bdd <- extraction(nom_col = "d_ap_30",df = bdd,conv = 10, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_bulkdens.fineearth_usda.4a1h_m_250m_b30..30cm_1950..2017_v0.2.tif")
bdd = moyenne_val_extrct(nom_col = "d_ap_0_a_30", vec_col = c("d_ap_0","d_ap_10","d_ap_30"),bdd)
summary(bdd$d_ap_0_a_30)
explo_num(nom_col = "d_ap_0_a_30", titre = "Bulk density (0 - 30 cm)")
```


## Sand content (% kg/kg)

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = "sable_0",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_sand.wfraction_usda.3a1a1a_m_250m_b0..0cm_1950..2017_v0.2.tif")
bdd <- extraction(nom_col = "sable_10",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_sand.wfraction_usda.3a1a1a_m_250m_b10..10cm_1950..2017_v0.2.tif")
bdd <- extraction(nom_col = "sable_30",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sol_sand.wfraction_usda.3a1a1a_m_250m_b30..30cm_1950..2017_v0.2.tif")
bdd = moyenne_val_extrct(nom_col = "sable_0_a_30", vec_col = c("sable_0","sable_10","sable_30"),df=bdd)
summary(bdd$sable_0_a_30)
explo_num(nom_col = "sable_0_a_30", titre = "Sand (0 - 30 cm)")
```



-->

## Sand

**Extracted values (g/kg, 0 - 30 cm)**

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = "sable.0_5",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sable.0_5.tif")

bdd <- extraction(nom_col = "sable.5_15",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sable.5_15.tif")

bdd <- extraction(nom_col = "sable.15_30",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/sable.15_30.tif")

bdd = moyenne_val_extrct(nom_col = "sable.0_30", vec_col = c("sable.0_5","sable.5_15","sable.15_30"),df=bdd)

# summary(bdd$sable.0_30)

explo_num(nom_col = "sable.0_30", titre = "Sand extracted values")
```

**Measured values & extracted values**

-   Clean sand column

```{r,echo=TRUE}
# On recupere les deux colonnes du pH
df_comp=bdd[, c("ID", "ID_Site","sand","sable.0_30" )]
df_comp =df_comp[complete.cases(df_comp$sand),] 
df_comp =df_comp[complete.cases(df_comp$sable.0_30),] 
df_comp <- df_comp[!grepl("[^0-9.]", df_comp$sand), ]
df_comp$sand <- as.numeric(df_comp$sand)
df_comp$sable.0_30 <- as.numeric(df_comp$sable.0_30)
# colSums(is.na(df_comp))

df_comp = df_comp[!df_comp$sand== "NA",]
df_comp = df_comp[!df_comp$sable.0_30== "NaN",]
df_comp = droplevels(df_comp)
```

```{r,echo=TRUE}
# -   Deleting duplicate measured values

ID_Site_dupliques <- df_comp$ID_Site[duplicated(df_comp$ID_Site)]
#length(ID_Site_dupliques)

lignes_dupliquees <- subset(df_comp, duplicated(ID_Site) & duplicated(sand))

lignes_unique <- unique(lignes_dupliquees$ID_Site )
#length(lignes_unique)
# nrow(df_comp) - length(ID_Site_dupliques) + length(lignes_unique)

dupliquees <- duplicated(df_comp$ID_Site)
df_comp <- df_comp[!dupliquees, ]
df_comp=droplevels(df_comp)
df_comp$sand <- as.numeric(df_comp$sand)
df_comp$sable.0_30 <- as.numeric(df_comp$sable.0_30)

```

::: columns
::: {.column width="60%"}
-   Method ?

-   Depth ?

-   Measured values (CR = `r df_col[df_col$Variable=="sand","CR"]`)

```{r}
  summary(df_comp$sand)
```

-   Extracted values

```{r}
  summary(df_comp$sable.0_30)
```
:::

::: {.column width="40%"}
\n\n\n

```{r,fig.align='center',fig.height=5,fig.width=4}
# graphique avec ggplot
correlation <- cor(as.numeric(df_comp$sand), df_comp$sable.0_30)
    p <- ggplot(df_comp, aes(x = sand, y = sable.0_30)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = FALSE, color = "red") + 
      labs(subtitle =paste("R = ", round(correlation, 2)) ,x = "Sand measured values", y = "Sand extracted values") + 
      theme_classic() 
p
```
:::
:::

## Silt

**Extracted values (g/kg, 0 - 30 cm)**

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = "limon.0_5",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/limon.0_5.tif")

bdd <- extraction(nom_col = "limon.5_15",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/limon.5_15.tif")

bdd <- extraction(nom_col = "limon.15_30",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/limon.15_30.tif")

bdd = moyenne_val_extrct(nom_col = "limon.0_30", vec_col = c("limon.0_5","limon.5_15","limon.15_30"),df=bdd)

# summary(bdd$limon.0_30)

explo_num(nom_col = "limon.0_30", titre = "Silt extracted values")
```

**Measured values & extracted values**

-   Clean silt column

```{r,echo=TRUE}
# On recupere les deux colonnes du pH
df_comp=bdd[, c("ID", "ID_Site","silt","limon.0_30" )]
df_comp =df_comp[complete.cases(df_comp$silt),] 
df_comp =df_comp[complete.cases(df_comp$limon.0_30),] 
df_comp <- df_comp[!grepl("[^0-9.]", df_comp$silt), ]
df_comp$silt <- as.numeric(df_comp$silt)
df_comp$limon.0_30 <- as.numeric(df_comp$limon.0_30)
# colSums(is.na(df_comp))


df_comp = df_comp[!df_comp$silt== "NA",]
df_comp = df_comp[!df_comp$limon.0_30== "NaN",]
df_comp = droplevels(df_comp)
```

```{r,echo=TRUE}
# -   Deleting duplicate measured values

ID_Site_dupliques <- df_comp$ID_Site[duplicated(df_comp$ID_Site)]
#length(ID_Site_dupliques)

lignes_dupliquees <- subset(df_comp, duplicated(ID_Site) & duplicated(silt))

lignes_unique <- unique(lignes_dupliquees$ID_Site )
#length(lignes_unique)
# nrow(df_comp) - length(ID_Site_dupliques) + length(lignes_unique)


dupliquees <- duplicated(df_comp$ID_Site)
df_comp <- df_comp[!dupliquees, ]
df_comp=droplevels(df_comp)
df_comp$silt <- as.numeric(df_comp$silt)
df_comp$limon.0_30 <- as.numeric(df_comp$limon.0_30)

```

::: columns
::: {.column width="60%"}
-   Method ?

-   Depth ?

-   Measured values (CR = `r df_col[df_col$Variable=="silt","CR"]`)

```{r}
  summary(df_comp$silt)
```

-   Extracted values

```{r}
  summary(df_comp$limon.0_30)
```
:::

::: {.column width="40%"}
\n\n\n

```{r,fig.align='center',fig.height=5,fig.width=4}
# graphique avec ggplot
correlation <- cor(as.numeric(df_comp$silt), df_comp$limon.0_30)
    p <- ggplot(df_comp, aes(x = silt, y = limon.0_30)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = FALSE, color = "red") + 
      labs(subtitle =paste("R = ", round(correlation, 2)) ,x = "Silt measured values", y = "Silt extracted values") + 
      theme_classic() 
p
```
:::
:::

## Clay

**Extracted values (g/kg, 0 - 30 cm)**

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = "argile.0_5",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/argile.0_5.tif")

bdd <- extraction(nom_col = "argile.5_15",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/argile.5_15.tif")

bdd <- extraction(nom_col = "argile.15_30",df = bdd,conv = 1, 
      tif_file_path = "C:/Users/diall/Downloads/datas/raster_modif/argile.15_30.tif")

bdd = moyenne_val_extrct(nom_col = "argile.0_30", vec_col = c("argile.0_5","argile.5_15","argile.15_30"),df=bdd)

summary(bdd$argile.0_30)

explo_num(nom_col = "argile.0_30", titre = "Clay extracted values")
```

**Measured values & extracted values** - Clean clay column

```{r,echo=TRUE}
# On recupere les deux colonnes du pH
df_comp=bdd[, c("ID", "ID_Site","clay","argile.0_30" )]
df_comp =df_comp[complete.cases(df_comp$clay),] 
df_comp =df_comp[complete.cases(df_comp$argile.0_30),] 
df_comp <- df_comp[!grepl("[^0-9.]", df_comp$clay), ]
df_comp$clay <- as.numeric(df_comp$clay)
df_comp$argile.0_30 <- as.numeric(df_comp$argile.0_30)
# colSums(is.na(df_comp))

df_comp = df_comp[!df_comp$clay== "NA",]
df_comp = df_comp[!df_comp$argile.0_30== "NaN",]
df_comp = droplevels(df_comp)
```

```{r,echo=TRUE}
# -   Deleting duplicate measured values

ID_Site_dupliques <- df_comp$ID_Site[duplicated(df_comp$ID_Site)]
#length(ID_Site_dupliques)

lignes_dupliquees <- subset(df_comp, duplicated(ID_Site) & duplicated(clay))

lignes_unique <- unique(lignes_dupliquees$ID_Site )
#length(lignes_unique)
# nrow(df_comp) - length(ID_Site_dupliques) + length(lignes_unique)

dupliquees <- duplicated(df_comp$ID_Site)
df_comp <- df_comp[!dupliquees, ]
df_comp=droplevels(df_comp)
df_comp$clay <- as.numeric(df_comp$clay)
df_comp$argile.0_30 <- as.numeric(df_comp$argile.0_30)

```

::: columns
::: {.column width="60%"}
-   Method ?

-   Depth ?

-   Measured values (CR = `r df_col[df_col$Variable=="clay","CR"]`)

```{r}
  summary(df_comp$clay)
```

-   Extracted values

```{r}
  summary(df_comp$argile.0_30)
```
:::

::: {.column width="40%"}
\n\n\n

```{r,fig.align='center',fig.height=5,fig.width=4}
# graphique avec ggplot
correlation <- cor(as.numeric(df_comp$clay), df_comp$argile.0_30)
    p <- ggplot(df_comp, aes(x = clay, y = argile.0_30)) +
      geom_point() + # Ajout des points
      geom_smooth(method = "lm", se = FALSE, color = "red") + 
      labs(subtitle =paste("R = ", round(correlation, 2)) ,x = "Clay measured values", y = "Clay extracted values") + 
      theme_classic() 
p
```
:::
:::

## Elevation

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = "elevation",df = bdd,conv = 1, 
                  tif_file_path ="C:/Users/diall/Downloads/datas/raster_modif/GMTED2010_Spatial.tif")

summary(bdd$elevation)

explo_num(nom_col = "elevation", titre = "elevation")
```


<!--
## pH_H2O_CaCl 

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = 'jrc_pH_H2O_CaCl', df = bdd, conv = 1, tif_file_path = 'C:/Users/diall/Downloads/datas/raster_modif/pH_H2O_CaCl.tif')

summary(bdd$jrc_pH_H2O_CaCl)

explo_num(nom_col = 'jrc_pH_H2O_CaCl', titre = 'jrc_pH_H2O_CaCl')
```

## pH_H2O 

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = 'jrc_pH_H2O', df = bdd, conv = 1, tif_file_path = 'C:/Users/diall/Downloads/datas/raster_modif/pH_H2O.tif')

summary(bdd$jrc_pH_H2O)

explo_num(nom_col = 'jrc_pH_H2O', titre = 'jrc_pH_H2O')

df_comp=bdd[, c("ID", "ID_Site","ph_eau","jrc_pH_H2O" )]
df_comp =df_comp[complete.cases(df_comp$ph_eau),] 
df_comp =df_comp[complete.cases(df_comp$jrc_pH_H2O),] 
df_comp <- df_comp[!grepl("[^0-9.]", df_comp$ph_eau), ]
df_comp$ph_eau <- as.numeric(df_comp$ph_eau)
df_comp$jrc_pH_H2O <- as.numeric(df_comp$jrc_pH_H2O)


df_comp = df_comp[!df_comp$ph_eau== 44140.00,]
df_comp = df_comp[!df_comp$ph_eau== "NA",]
df_comp = df_comp[!df_comp$jrc_pH_H2O== "NA",]
df_comp = droplevels(df_comp)

correlation <- cor(as.numeric(df_comp$ph_eau), df_comp$jrc_pH_H2O)
```

## pH_CaCl 

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = 'jrc_pH_CaCl', df = bdd, conv = 1, tif_file_path = 'C:/Users/diall/Downloads/datas/raster_modif/pH_CaCl.tif')

summary(bdd$jrc_pH_CaCl)

explo_num(nom_col = 'jrc_pH_CaCl', titre = 'jrc_pH_CaCl')
```

-->


## Phosphore (P, mg/kg)

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = 'P', df = bdd, conv = 1, tif_file_path = 'C:/Users/diall/Downloads/datas/raster_modif/P.tif')

summary(bdd$P)

explo_num(nom_col = 'P', titre = 'P')
```

## Azote (N, g/kg)

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = 'N', df = bdd, conv = 1, tif_file_path = 'C:/Users/diall/Downloads/datas/raster_modif/N.tif')

summary(bdd$N)

explo_num(nom_col = 'N', titre = 'N')
```

## Potassium (K, mg/kg)

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = 'K', df = bdd, conv = 1, tif_file_path = 'C:/Users/diall/Downloads/datas/raster_modif/K.tif')

summary(bdd$K)

explo_num(nom_col = 'K', titre = 'K')
```

## C/N

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = 'CN', df = bdd, conv = 1, tif_file_path = 'C:/Users/diall/Downloads/datas/raster_modif/CN.tif')

summary(bdd$CN)

explo_num(nom_col = 'CN', titre = 'CN')
```

## Capacité d'échange de cations (CEC, cmol/kg)

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = 'CEC', df = bdd, conv = 1, tif_file_path = 'C:/Users/diall/Downloads/datas/raster_modif/CEC.tif')

summary(bdd$CEC)

explo_num(nom_col = 'CEC', titre = 'CEC')
```

## Carbonates de calcium (CaCO3, g/kg)

```{r,fig.align='center',fig.height=8}
bdd <- extraction(nom_col = 'CaCO3', df = bdd, conv = 1, tif_file_path = 'C:/Users/diall/Downloads/datas/raster_modif/CaCO3.tif')

summary(bdd$CaCO3)

explo_num(nom_col = 'CaCO3', titre = 'CaCO3')
```

# Analyses explorations

## Réduction du jeu de donnée

```{r,echo=TRUE,fig.height=8,fig.show='animate',fig.align='center'}
id_col=c("ID","Programme","Annee","ID_Site","Protocole")

vdt_col=c("AB_tot", "BM_tot", "Richesse_tot")

land_cover_col=c("clcm_lvl3")

topo_col=c("elevation","gps_x","gps_y")


soil_col=c("ph_0_a_30","sable.0_30","limon.0_30","argile.0_30","c_orga_0_a_30","P","N","K","CN","CEC","CaCO3")


climate_col=c()
for (i in 1:19){
  climate_col=c(climate_col, paste0("bio",i) )
}
climate_col=c(climate_col,"cmi_mean","gdd0","gdd10","hurs_mean","pet_penman_mean")

bdd_explo= bdd[,c(id_col,vdt_col,land_cover_col,topo_col,soil_col,climate_col)]
# str(bdd_explo)
bdd_explo$ID = as.factor(bdd_explo$ID)


# Renome
new_soil_col=c("pH","sand","silt","clay","C","P","N","K","CN","CEC","CaCO3")
bdd_explo <- rename(bdd_explo, !!setNames(soil_col, new_soil_col))

bdd_explo <- bdd_explo %>% 
  rename(PET = pet_penman_mean)
climate_col=c()
for (i in 1:19){
  climate_col=c(climate_col, paste0("bio",i) )
}
climate_col=c(climate_col,"cmi_mean","gdd0","gdd10","hurs_mean","PET")


col_graph=c(vdt_col,land_cover_col,topo_col,new_soil_col,climate_col)
# for (i in names(bdd_explo[,col_graph])){
#   par(mfrow=c(2,2))
#   plot(bdd_explo[[i]], main=i)
# }


# STANDARIZATION
# names(bdd_explo)
var_quanti = c(topo_col,new_soil_col,climate_col)
for (col in var_quanti) {
  bdd_explo[[col]] <- scale(bdd_explo[[col]])
}
# summary(bdd_explo[, var_quanti])
# str(bdd_explo[, var_quanti])
```

## Test de correlation : intra catégories

-   **Topographie**

Colonnes supprimée : *gps_x*

```{r,echo=FALSE,fig.align='center'}
# ggpairs(bdd_explo[,topo_col])
correlation_matrix <- cor(bdd_explo[,topo_col],use = "na.or.complete")
# 
corrplot::corrplot(correlation_matrix, method = "color", type = "lower", order = "hclust", addCoef.col = "black",diag = FALSE,cl.cex = 1,number.cex = 1)
topo_sup=c("gps_x")
bdd_explo <- bdd_explo[, setdiff(names(bdd_explo), topo_sup)]

```


<br/>
-   **Soil data**

```{r,echo=FALSE}
# ggpairs(bdd_explo[,new_soil_col])
soil_sup=c("sand")
```

::: columns
::: {.column width="25%"}
<br/>
Colonnes supprimée : *`r soil_sup`*
:::

::: {.column width="75%"}

```{r,echo=FALSE,fig.height=6,fig.width=9,fig.align='right'}
correlation_matrix <- cor(bdd_explo[,new_soil_col],use = "na.or.complete")
corrplot::corrplot(correlation_matrix, method = "color", type = "lower", order = "hclust", addCoef.col = "black",diag = FALSE,cl.cex = 1,number.cex = 0.7)

bdd_explo <- bdd_explo[, setdiff(names(bdd_explo), soil_sup)]
```
:::
:::



<br/>
-   **Climat data**

```{r,echo=FALSE,out.height="100%",out.width="100%"}
# correlation_matrix <- cor(bdd_explo[,climate_col],use = "na.or.complete")
# 
# chemin="C:/Users/diall/OneDrive/Bureau/M2_MODE/stage_abdou_m2/R_Stage_M2/graphes/climat_corrplot.png"
# png(chemin, width = 2000, height = 1000,res = 110)
# 
# 
# corrplot::corrplot(correlation_matrix, method = "color", type = "lower", order = "hclust", addCoef.col = "black",diag = FALSE,cl.cex = 1,number.cex = 0.7,mar = c(0,0,0,0))
# 
# dev.off()
# ![](graphes/climat_corrplot.png)


# cor_function_seuil(data = bdd_explo[,climate_col], seuil = 0.7)
climat_sup=c("bio2","bio4","bio5","bio6","bio7", "bio9", "bio10","bio11","bio13","bio16","bio17","bio18","bio19","gdd0","gdd10", "cmi_mean","PET")
```

::: columns
::: {.column width="25%"}
<br/>
Colonnes supprimmées :*`r climat_sup`*
:::

::: {.column width="75%"}
```{r,echo=FALSE,fig.height=7,fig.width=9,fig.align='right'}
climat_selec = climate_col[!climate_col %in% climat_sup]
# ggpairs(bdd_explo[,climat_selec])
correlation_matrix <- cor(bdd_explo[,climat_selec],use = "na.or.complete")
corrplot::corrplot(correlation_matrix, method = "color", type = "lower", order = "hclust", addCoef.col = "black",diag = FALSE,cl.cex = 1,number.cex = 0.9,mar = c(0,0,0,0))

bdd_explo <- bdd_explo[, setdiff(names(bdd_explo), climat_sup)]
```
:::
:::



## Test de correlation : inter catégories

Colonnes supprimée : *gps_y*
```{r,echo=FALSE}
# correlation_matrix <- cor(bdd_explo[,10:31],use = "na.or.complete")
# corrplot::corrplot(correlation_matrix, method = "color", type = "lower", order = "hclust", addCoef.col = "black",diag = FALSE,cl.cex = 0.5,number.cex = 0.5,mar = c(0,0,0,0))

# var=c("elevation","pH","silt","clay","C","P","N","K","CN","CEC","CaCO3","bio1","bio3","bio8","bio12","bio14","bio15","hurs_mean","gps_y")
# 
# correlation_matrix <- cor(bdd_explo[,var],use = "na.or.complete")
# 
# chemin="C:/Users/diall/OneDrive/Bureau/M2_MODE/stage_abdou_m2/R_Stage_M2/graphes/interC_corrplot.png"
# png(chemin, width = 1200, height = 1000,res = 110)
# 
# 
# corrplot::corrplot(correlation_matrix, method = "color", type = "lower", order = "hclust", addCoef.col = "black",diag = FALSE,cl.cex = 1,number.cex = 0.7,mar = c(0,0,0,0))
# 
# dev.off()


bdd_explo <- bdd_explo[, setdiff(names(bdd_explo), c("gps_y"))]
```

![](graphes/interC_corrplot.png) {width="1200",aligne="center"}



## VIF
```{r}
var_avant=c("elevation","pH","silt","clay","C","P","N","K","CN","CEC","CaCO3","bio1","bio3",
      "bio8","bio12","bio14","bio15","hurs_mean")
#usdm::vif(bdd_explo[,var_avant])
#usdm::vifcor(bdd_explo[,var_avant], th = 0.9, keep = NULL, method = 'pearson')
usdm::vifstep(bdd_explo[,var_avant], th = 10, keep = NULL, method = 'pearson')
# -   On enleve "bio14" car la plus forte VIF
bdd_explo <- bdd_explo[, setdiff(names(bdd_explo), c("bio14"))]

var=c("elevation","pH","silt","clay","C","P","N","K","CN","CEC","CaCO3","bio1","bio3","bio8","bio12","bio15","hurs_mean")
```

## Création des variables factices

```{r,echo=TRUE}
bdd_explo$clcm_lvl3= as.factor(bdd_explo$clcm_lvl3)
summary_df <- as.data.frame(summary(bdd_explo$clcm_lvl3))
colnames(summary_df) <- c("Numbers")
kable(summary_df,padding = 5)
```

-   Merging:
    *Mixed forest = Broad\_leaved forest +* 
                    *Coniferous forest +* 
                    *Mixed forest*

```{r, echo=TRUE}
levels(bdd_explo$clcm_lvl3)[levels(bdd_explo$clcm_lvl3) == "Broad-leaved forest"] <- "Mixed forest"
levels(bdd_explo$clcm_lvl3)[levels(bdd_explo$clcm_lvl3) == "Coniferous forest"] <- "Mixed forest"
bdd_explo$clcm_lvl3= as.factor(bdd_explo$clcm_lvl3)
# summary_df <- as.data.frame(summary(bdd_explo$clcm_lvl3))
# colnames(summary_df) <- c("Numbers")
# kable(summary_df,padding = 5)
```

-   Abréviation des levels

```{r, echo=TRUE}
cl_original <- levels(bdd_explo$clcm_lvl3)
new_cl <- c("mf","gua", "ng", "nial", "p", "v")
bdd_explo$clcm_lvl3 <- factor(bdd_explo$clcm_lvl3, levels = cl_original, labels = new_cl)

bdd_explo <- bdd_explo %>% 
  rename(clc3 = clcm_lvl3)
```

-   Variables

```{r,echo=TRUE}
bdd_explo <- dummy_cols(bdd_explo, select_columns = c("clc3"))
clc3_col= c("clc3_mf","clc3_gua", "clc3_ng", "clc3_nial", "clc3_p", "clc3_v")
head(bdd_explo[,clc3_col])
# str(bdd_explo[,clc3_col])
```


```{r}
# usdm::vif(bdd_explo[,c(var,clc3_col)])
# usdm::vifcor(bdd_explo[,c(var,clc3_col)], th = 0.9, keep = NULL, method = 'pearson')
# usdm::vifstep(bdd_explo[,c(var,clc3_col)], th = 10, keep = NULL, method = 'pearson')
# summary(bdd_explo$clc3)
```


## ACP
```{r, echo=TRUE, fig.align='center'}
# On supprime les lignes avec des NA dans toutes les colonnes sauf BM_tot
# colSums(is.na(bdd_explo))
bdd_explo <- bdd_explo[apply(bdd_explo[, !colnames(bdd_explo) %in% "BM_tot"], 1, function(x) all(!is.na(x))), ]
# colSums(is.na(bdd_explo))


var=c("elevation","pH","silt","clay","C","P","N","K","CN","CEC","CaCO3","bio1","bio3",
      "bio8","bio12","bio15","hurs_mean")

data_acp = bdd_explo[, var]
acp0 <- PCA(data_acp, graph = FALSE)


## Choix du nombre d'axes
# acp0$eig
fviz_eig(acp0, addlabels = TRUE) # on prend les trois premiers axes
contrib_axes <- acp0$var$contrib[, 1:3]  # 3 premiers axes
contrib_axes <- round(contrib_axes, 3)   # Plus facile a lire
fviz_contrib(acp0, choice = "var", axes = 1)
fviz_contrib(acp0, choice = "var", axes = 2)
fviz_contrib(acp0, choice = "var", axes = 3)

seuil <- 1 / ncol(data_acp) * 100
lignes_superieures <- rownames(contrib_axes)[apply(contrib_axes, 1,
                                                   function(x)
                                                     any(x >= seuil))]
var [! var %in% lignes_superieures]

#  **** En consderant les deux premiers axes:
#le Phosphore_.P._g.kg et le C_N et le pH ne sont pas trés influant





# coloree les variables selon leurs contributions aux axes
fviz_pca_var(
  axes = c(1, 2),
  acp0,
  col.var = "contrib",
  gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
  repel = TRUE
) # evite le chevauchement de texte


# Suppression de Limon fin

#Suppression de Argiles


coul = c("yellow", "green", "violet", "blue", "black", "red")
fviz_pca_ind(
  axes = c(1, 2),
  acp0,
  geom.ind = "text",
  pointshape = 21,
  pointsize = 2,
  #palette = coul,
  palette = "viridis",
  addEllipses = TRUE,
  legend.title = "CLC",
  fill.ind = bdd_explo$clc3
)
fviz_pca_ind(
  axes = c(1, 2),
  acp0,
  geom.ind = "point",
  pointshape = 21,
  pointsize = 2,
  palette = "viridis",
  addEllipses = TRUE,
  legend.title = "CLC",
  fill.ind = bdd_explo$clc3
)
```


```{r}
# Classification
# class1 = HCPC(acp0)
# plot(class1$call$t$inert.gain, type = "s")
# #Je choisis finalement 4 groupes
# class1 = HCPC(acp0, nb.clust = 4)
# fviz_dend(class1)
# fviz_cluster(class1)
# fviz_cluster(class1, ellipse.type = "norm", ellipse.level = 0.8)    #Groupes
```


# Selecting variables with regsubsets()
```{r, echo=TRUE}
# colSums(is.na(bdd_explo))
bdd_explo <- bdd_explo[apply(bdd_explo[, !colnames(bdd_explo) %in% "BM_tot"], 1, function(x) all(!is.na(x))), ]
# colSums(is.na(bdd_explo))
dim(bdd_explo)
```

## AB_tot
```{r, echo=TRUE, fig.align='center'}
# names(bdd_explo)
supp = c("ID","Programme","Annee","ID_Site", "Protocole","BM_tot", "Richesse_tot","clc3")
df_AB_tot= bdd_explo[, setdiff(names(bdd_explo), supp)]
df_AB_tot= df_AB_tot[, setdiff(names(df_AB_tot), clc3_col)]
# str(df_AB_tot)
# colSums(is.na(df_AB_tot))
results_AB_tot <- regsubsets(AB_tot ~ ., data = df_AB_tot,method = "exhaustive",nvmax =17 )
# summary(results_AB_tot)
rsq_AB_tot= round (summary(results_AB_tot)$rsq,2)
adjr2_AB_tot= round(summary(results_AB_tot)$adjr2,2)
cp_AB_tot=round (summary(results_AB_tot)$cp,2)
bic_AB_tot=round(summary(results_AB_tot)$bic,2)
```


-   Selection by R² : stable from `r which.max(rsq_AB_tot)` variables
```{r, echo=FALSE, fig.align='center'}
plot(results_AB_tot, scale = "r2", main='R² criteria')
```

-   Selection by R² adj : stable from `r which.max(adjr2_AB_tot)` variables
```{r, echo=FALSE, fig.align='center'}
plot(results_AB_tot, scale = "adjr2",main='R² adj criteria')
```

-   Selection by Cp : stable from `r which.min(cp_AB_tot)` variables
```{r, echo=FALSE, fig.align='center'}
plot(results_AB_tot, scale = "Cp",main='Mallows Cp criteria')
```

-   Selection by BIC : stable from `r which.min(bic_AB_tot)` variables
```{r, echo=FALSE, fig.align='center'}
plot(results_AB_tot, scale = "bic",main="BIC criteria")
```

```{r}
nbr_AB_tot= max(which.max(rsq_AB_tot),which.max(adjr2_AB_tot),which.min(cp_AB_tot),which.min(bic_AB_tot))
best_var_AB_tot=coefficients(results_AB_tot, id = nbr_AB_tot)
best_var_AB_tot=names(best_var_AB_tot)[-1]
```
-   Les `r length(best_var_AB_tot)` meilleurs variables sont: **`r best_var_AB_tot`**



## BM_tot

-   Suppression de `r sum(is.na(bdd_explo$BM_tot))` lignes de NA de BM_tot
```{r, echo=TRUE, fig.align='center'}
# names(bdd_explo)
supp = c("ID","Programme","Annee","ID_Site", "Protocole","AB_tot", "Richesse_tot","clc3")
df_BM_tot= bdd_explo[, setdiff(names(bdd_explo), supp)]
df_BM_tot= df_BM_tot[, setdiff(names(df_BM_tot), clc3_col)]
df_BM_tot=drop_na(df_BM_tot)
# str(df_BM_tot)
# colSums(is.na(df_BM_tot))
results_BM_tot <- regsubsets(BM_tot ~ ., data = df_BM_tot,method = "exhaustive",nvmax =17 )
# summary(results_BM_tot)
rsq_BM_tot= round (summary(results_BM_tot)$rsq,2)
adjr2_BM_tot= round(summary(results_BM_tot)$adjr2,2)
cp_BM_tot=round (summary(results_BM_tot)$cp,2)
bic_BM_tot=round(summary(results_BM_tot)$bic,2)
```


-   Selection by R² : stable from `r which.max(rsq_BM_tot)` variables
```{r, echo=FALSE, fig.align='center'}
plot(results_BM_tot, scale = "r2", main='R² criteria')
```

-   Selection by R² adj : stable from `r which.max(adjr2_BM_tot)` variables
```{r, echo=FALSE, fig.align='center'}
plot(results_BM_tot, scale = "adjr2",main='R² adj criteria')
```

-   Selection by Cp : stable from `r which.min(cp_BM_tot)` variables
```{r, echo=FALSE, fig.align='center'}
plot(results_BM_tot, scale = "Cp",main='Mallows Cp criteria')
```

-   Selection by BIC : stable from `r which.min(bic_BM_tot)` variables
```{r, echo=FALSE, fig.align='center'}
plot(results_BM_tot, scale = "bic",main="BIC criteria")
```

```{r}
nbr_BM_tot= max(which.max(rsq_BM_tot),which.max(adjr2_BM_tot),which.min(cp_BM_tot),which.min(bic_BM_tot))
best_var_BM_tot=coefficients(results_BM_tot, id = nbr_BM_tot)
best_var_BM_tot=names(best_var_BM_tot)[-1]
```
-   Les `r length(best_var_BM_tot)` meilleurs variables sont: **`r best_var_BM_tot`**



## Richesse_tot
```{r, echo=TRUE, fig.align='center'}
# names(bdd_explo)
supp = c("ID","Programme","Annee","ID_Site", "Protocole","BM_tot", "AB_tot","clc3")
df_Richesse_tot= bdd_explo[, setdiff(names(bdd_explo), supp)]
df_Richesse_tot= df_Richesse_tot[, setdiff(names(df_Richesse_tot), clc3_col)]
# str(df_Richesse_tot)
# colSums(is.na(df_Richesse_tot))
results_Richesse_tot <- regsubsets(Richesse_tot ~ ., data = df_Richesse_tot,method = "exhaustive",nvmax =17 )
# summary(results_Richesse_tot)
rsq_Richesse_tot= round (summary(results_Richesse_tot)$rsq,2)
adjr2_Richesse_tot= round(summary(results_Richesse_tot)$adjr2,2)
cp_Richesse_tot=round (summary(results_Richesse_tot)$cp,2)
bic_Richesse_tot=round(summary(results_Richesse_tot)$bic,2)
```


-   Selection by R² : stable from `r which.max(rsq_Richesse_tot)` variables
```{r, echo=FALSE, fig.align='center'}
plot(results_Richesse_tot, scale = "r2", main='R² criteria')
```

-   Selection by R² adj : stable from `r which.max(adjr2_Richesse_tot)` variables
```{r, echo=FALSE, fig.align='center'}
plot(results_Richesse_tot, scale = "adjr2",main='R² adj criteria')
```

-   Selection by Cp : stable from `r which.min(cp_Richesse_tot)` variables
```{r, echo=FALSE, fig.align='center'}
plot(results_Richesse_tot, scale = "Cp",main='Mallows Cp criteria')
```

-   Selection by BIC : stable from `r which.min(bic_Richesse_tot)` variables
```{r, echo=FALSE, fig.align='center'}
plot(results_Richesse_tot, scale = "bic",main="BIC criteria")
```

```{r}
nbr_Richesse_tot= max(which.max(rsq_Richesse_tot),which.max(adjr2_Richesse_tot),which.min(cp_Richesse_tot),which.min(bic_Richesse_tot))
best_var_Richesse_tot=coefficients(results_Richesse_tot, id = nbr_Richesse_tot)
best_var_Richesse_tot=names(best_var_Richesse_tot)[-1]
```
-   Les `r length(best_var_Richesse_tot)` meilleurs variables sont: **`r best_var_Richesse_tot`**


## Synthèses
```{r}
all_var=unique(c(best_var_AB_tot,best_var_BM_tot,best_var_Richesse_tot))
col_max=max(c(length(best_var_AB_tot),length(best_var_BM_tot),length(best_var_Richesse_tot)))


# Colonnes communes
colonnes_communes <- intersect(intersect(best_var_AB_tot, best_var_BM_tot), best_var_Richesse_tot)

resultat_tableau <- data.frame(
  Var_AB_tot =c(best_var_AB_tot, rep(" ", col_max - length(best_var_AB_tot))),
  
  Var_BM_tot =  c(best_var_BM_tot, rep(" ", col_max - length(best_var_BM_tot))),
  
  Var_Richesse_tot =  c(best_var_Richesse_tot, rep(" ", col_max -    length(best_var_Richesse_tot))),
  
  Communes =c(colonnes_communes, rep(" ", col_max - length(colonnes_communes)))
)
kable(resultat_tableau,padding=10)


```
**bio1** = mean annual air temperature

**bio3** = isothermality

**bio8** = mean air temperatures of the wettest quarter

**bio8** = mean air temperatures of the wettest quarter

**bio12** = annual precipitation amount

**bio15** = precipitation seasonality

**hurs_mean** = Mean monthly near-surface relative humidity



# Modélisation
```{r}
supp = c("ID","Programme","Annee","ID_Site", "Protocole","AB_tot","BM_tot","Richesse_tot", "AB_tot","clc3")
predicteurs=c(names(df_mod)[! names(df_mod) %in% supp])
# names(bdd_explo[,10:26])
df_mod=bdd_explo


set.seed(42)  # Pour rendre les résultats reproductibles
index <- createDataPartition(df_mod$clc3, p = 0.8, list = FALSE)

# Séparer les données en ensembles d'entraînement et de test
df_train <- df_mod[index, ]  # Données d'entraînement
df_test <- df_mod[-index, ]  # Données de test
df_train = droplevels(df_train)
df_test = droplevels(df_test)
# summary(df_train$clc3)
# summary(df_test$clc3)
# summary(df_mod$clc3)
# 80*summary(df_mod$clc3)/100
```

## Models

```{r}
GLM <- function(var_rep, df_app, df_valid,family = 'gaussian'){
  var_mod=c(var_rep,predicteurs)
  df_app = df_app[,var_mod]
  df_valid = df_valid[,var_mod]
  formula <- substitute(var_rep ~ ., list(var_rep = as.name(var_rep)))
  # entrainement du modele sur le jeu d'entrainement
  modelglm<-glm(formula,family = family ,data = df_app)
  
  # Prediction sur le jeu de validation
  prev.glm<-predict(modelglm,newdata=as.data.frame(df_valid[,predicteurs]))
  
  # Calcul du RMSE pour évaluer la qualite du modele
  rmse <- round (sqrt(mean((df_valid[,var_rep] - prev.glm)^2)),2)
  
  
 
  formula <- substitute(var_rep ~ 1, list(var_rep = as.name(var_rep)))
  null.deviance <- deviance(lm(formula, data = df_app))
  deviance<- deviance(tests)
  r_squared = with(summary(tests), 1 - deviance/null.deviance) # McFadden's R-squared
  
  r_adj <- round (100*r_squared,2)
  
  
  results <- list(RMSE = rmse, R_squared= r_adj)
  return(results)
}

# GLM(var_rep ="AB_tot" , df_app=df_train, df_valid = df_test,family = 'gaussian')



GAM <- function(var_rep, df_app, df_valid, family = 'gaussian',method = "REML"){
  # var_mod=c(var_rep,predicteurs)
  # df_app = df_app[,var_mod]
  # df_valid = df_valid[,var_mod]
  # formula <- substitute(var_rep ~ ,  list(var_rep = as.name(var_rep)))
  
  # entrainement du modele sur le jeu d'entrainement
  modelgam<-gam(AB_tot ~ clc3 + s(elevation) + s(pH) + s(silt) + s(clay) + s(C) + s(N) + s(K) + s(CN) + s(CEC) + s(CaCO3) + s(bio1) + s(bio3) + s(bio8) + s(bio12) + s(bio15) + s(hurs_mean) ,
        family=family,method = method,data = df_app)
  
  # Prediction sur le jeu de validation
  prev.gam<-predict(modelgam,newdata=as.data.frame(df_valid[,9:26]))
  
  # Calcul du RMSE pour évaluer la qualite du modele
  rmse <- round (sqrt(mean((df_valid[,var_rep] - prev.gam)^2)),2)

  
  null.deviance <- modelgam$null.deviance
  deviance<- modelgam$deviance
  r_squared = with(summary(tests), 1 - deviance/null.deviance) # McFadden's R-squared
  
  r_adj <- round (100*r_squared,2)
  
  
  results <- list(RMSE = rmse, R_squared= r_adj)
  return(results)
}

# GAM(var_rep ="AB_tot" , df_app=df_train, df_valid = df_test,family = 'gaussian')




ForetAlea <- function(var_rep, df_app, df_valid){
  
  # Nombre de variables tirées aléatoirement pour la construction des arbre : mtry =    1/nombre de variable explicatives totales
  # nombre d'arbres : ntree = 250
  
  var_mod=c(var_rep,predicteurs)
  df_app = df_app[,var_mod]
  df_valid = df_valid[,var_mod]
  
  
  col_posi <- which(names(df_app) == var_rep)
  ForeVDT <- randomForest(df_app[-col_posi], df_app[[col_posi]], mtry=(ncol(df_app)-1)/3,
                          ntree=1000, keep.forest=TRUE)
  
  # Prediction sur le jeu de validation
  prev<-predict(ForeVDT,newdata=as.data.frame(df_valid[,predicteurs]))
  
  # Calcul du RMSE pour évaluer la qualité du modele
  rmse <- round (sqrt(mean((df_valid[,var_rep] - prev)^2)),2)
  
  r_adj = round (mean(100*ForeVDT$rsq),2)
  
    results <- list(RMSE = rmse, R_squared= r_adj)
  return(results)
}
# ForetAlea(var_rep ="AB_tot" , df_app=df_train, df_valid = df_test)

```


```{r}
n_sim=5 # Nombre de similation

# Pour AB_tot
AB_tot_RMSE_Results<-matrix(0,ncol=6,nrow=n_sim,dimnames=list(1:n_sim,c("RMSEGLM", "RMSEGAM", "RMSERandomF","RMSEGB","RMSEResNeu","RMS_BestMod")))
AB_tot_RMSE_Results<-as.data.frame(AB_tot_RMSE_Results)

AB_tot_R_Results<-matrix(0,ncol=6,nrow=n_sim,dimnames=list(1:n_sim,c("R_GLM", "R_GAM", "R_RandomF","R_GB","R_ResNeu","R_BestMod")))
AB_tot_R_Results<-as.data.frame(AB_tot_R_Results)



# Pour BM_tot
BM_tot_RMSE_Results<-matrix(0,ncol=6,nrow=n_sim,dimnames=list(1:n_sim,c("RMSEGLM", "RMSEGAM", "RMSERandomF","RMSEGB","RMSEResNeu","RMS_BestMod")))
BM_tot_RMSE_Results<-as.data.frame(BM_tot_RMSE_Results)

BM_tot_R_Results<-matrix(0,ncol=6,nrow=n_sim,dimnames=list(1:n_sim,c("R_GLM", "R_GAM", "R_RandomF","R_GB","R_ResNeu","R_BestMod")))
BM_tot_R_Results<-as.data.frame(BM_tot_R_Results)



# Pour AB_tot
Richesse_tot_RMSE_Results<-matrix(0,ncol=6,nrow=n_sim,dimnames=list(1:n_sim,c("RMSEGLM", "RMSEGAM", "RMSERandomF","RMSEGB","RMSEResNeu","RMS_BestMod")))
Richesse_tot_RMSE_Results<-as.data.frame(Richesse_tot_RMSE_Results)

Richesse_tot_R_Results<-matrix(0,ncol=6,nrow=n_sim,dimnames=list(1:n_sim,c("R_GLM", "R_GAM", "R_RandomF","R_GB","R_ResNeu","R_BestMod")))
Richesse_tot_R_Results<-as.data.frame(Richesse_tot_R_Results)
```

## Distributions: AB_tot

```{r}
# Noraml
set.seed(123)
donnees_normales <- rnorm(nrow(df_mod), mean = mean(df_mod$AB_tot), sd = 2)
hist(donnees_normales, freq = FALSE, main = "Distribution Normale", xlab = "Valeurs", ylab = "Densité")
curve(dnorm(x, mean = mean(donnees_normales), sd = sd(donnees_normales)), add = TRUE, col = "blue", lwd = 2)


# Poisson
set.seed(123)
donnees_poisson <- rpois(nrow(df_mod), lambda = mean(df_mod$AB_tot))
hist(donnees_poisson, freq = FALSE, main = "Distribution de Poisson", xlab = "Valeurs", ylab = "Densité")
x <- 0:max(donnees_poisson)
lines(x, dpois(x, lambda = mean(df_mod$AB_tot)), col = "blue", lwd = 2)


hist(df_mod$AB_tot,xlab = "AB_tot",main = "Distribution des données")

par(mfrow = c(1,1))
qqnorm(df_mod$AB_tot, pch= 16, col = 'blue',xlab='')
qqline(df_mod$AB_tot, col= 'red')
```

## Distributions: BM_tot

```{r}
# Noraml
df_bm=drop_na(df_mod)
set.seed(123)
donnees_normales <- rnorm(nrow(df_bm), mean = mean(df_bm$BM_tot), sd = 2)
hist(donnees_normales, freq = FALSE, main = "Distribution Normale", xlab = "Valeurs", ylab = "Densité")
curve(dnorm(x, mean = mean(donnees_normales), sd = sd(donnees_normales)), add = TRUE, col = "blue", lwd = 2)


# Poisson
set.seed(123)
donnees_poisson <- rpois(nrow(df_bm), lambda = mean(df_bm$BM_tot))
hist(donnees_poisson, freq = FALSE, main = "Distribution de Poisson", xlab = "Valeurs", ylab = "Densité")
x <- 0:max(donnees_poisson)
lines(x, dpois(x, lambda = mean(df_bm$BM_tot)), col = "blue", lwd = 2)


hist(df_bm$BM_tot,xlab = "BM_tot",main = "Distribution des données")

par(mfrow = c(1,1))
qqnorm(df_bm$BM_tot, pch= 16, col = 'blue',xlab='')
qqline(df_bm$BM_tot, col= 'red')
```

## Distributions: Richesse_tot

```{r}
# Noraml
set.seed(123)
donnees_normales <- rnorm(nrow(df_mod), mean = mean(df_mod$Richesse_tot), sd = 2)
hist(donnees_normales, freq = FALSE, main = "Distribution Normale", xlab = "Valeurs", ylab = "Densité")
curve(dnorm(x, mean = mean(donnees_normales), sd = sd(donnees_normales)), add = TRUE, col = "blue", lwd = 2)


# Poisson
set.seed(123)
donnees_poisson <- rpois(nrow(df_mod), lambda = mean(df_mod$Richesse_tot))
hist(donnees_poisson, freq = FALSE, main = "Distribution de Poisson", xlab = "Valeurs", ylab = "Densité")
x <- 0:max(donnees_poisson)
lines(x, dpois(x, lambda = mean(df_mod$Richesse_tot)), col = "blue", lwd = 2)


hist(df_mod$Richesse_tot,xlab = "Richesse_tot",main = "Distribution des données")

par(mfrow = c(1,1))
qqnorm(df_mod$Richesse_tot, pch= 16, col = 'blue',xlab='')
qqline(df_mod$Richesse_tot, col= 'red')
```


## Compilation

-   GLM
```{r}
temps1=Sys.time()
set.seed(1234)

# cl <- detectCores() %>% -1 %>% makeCluster
# registerDoParallel(cl)

for (i in 1:n_sim){
lignes <- createDataPartition(df_mod$clc3, p = 0.8, list = FALSE)

# Séparer les données en ensembles d'entraînement et de test
b_df_train <- df_mod[lignes, ]  # Données d'entraînement
b_df_test <- df_mod[-lignes, ]  # Données de test
b_df_train = droplevels(b_df_train)
b_df_test = droplevels(b_df_test)

# GLM
AB_tot_RMSE_Results[i,"RMSEGLM"] <- GLM(var_rep ="AB_tot",df_app = b_df_train, df_valid = b_df_test,family = 'gaussian')$RMSE

AB_tot_R_Results[i,"R_GLM"] <- GLM(var_rep ="AB_tot",df_app = b_df_train, df_valid = b_df_test,family = 'gaussian')$R_squared

cat("compilation : ",i, "/",n_sim,"\n")
}
temps2=Sys.time()
duree=difftime(temps2,temps1)
# stopCluster(cl)
# head(AB_tot_RMSE_Results)
# head(AB_tot_R_Results)
```


-   GAM
```{r}
temps1=Sys.time()
set.seed(1234)

# cl <- detectCores() %>% -1 %>% makeCluster
# registerDoParallel(cl)

for (i in 1:n_sim){
lignes <- createDataPartition(df_mod$clc3, p = 0.8, list = FALSE)

# Séparer les données en ensembles d'entraînement et de test
b_df_train <- df_mod[lignes, ]  # Données d'entraînement
b_df_test <- df_mod[-lignes, ]  # Données de test
b_df_train = droplevels(b_df_train)
b_df_test = droplevels(b_df_test)

# GAM
AB_tot_RMSE_Results[i,"RMSEGAM"] <- GAM(var_rep ="AB_tot",df_app = b_df_train, df_valid = b_df_test,family = 'gaussian')$RMSE

AB_tot_R_Results[i,"R_GAM"] <- GAM(var_rep ="AB_tot",df_app = b_df_train, df_valid = b_df_test,family = 'gaussian')$R_squared

cat("compilation : ",i, "/",n_sim,"\n")
}
temps2=Sys.time()
duree=difftime(temps2,temps1)
# stopCluster(cl)
# head(AB_tot_RMSE_Results)
# head(AB_tot_R_Results)
```



# Questions

